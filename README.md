\# \*\*AIPlatformAutomation â€” Full Solution v76.5.0\*\*

\`\# AIPlatformAutomation\`

\`\#\# Overview\`    
\`AIPlatformAutomation provides a \*\*fully modular, dockerized AI platform\*\* enabling:\`

\`- Internal LLMs (Ollama, LiteLLM, AnythingLLM)\`    
\`- External LLM providers (Google Gemini, OpenRouter, Groq, OpenAI)\`    
\`- AI applications (Dify, ComfyUI, OpenWebUI, Flowise, OpenClaw UI)\`    
\`- Vector databases (Chroma, Qdrant)\`    
\`- Monitoring stack (Grafana, Prometheus, ELK, Portainer)\`    
\`- Secure internal networking via Tailscale\`    
\`- Private embeddings and credentials management\`    
\`- Optional Google Drive sync\`    
\`- Signal integration for messaging\`

\`The platform is designed for \*\*fully autonomous deployment\*\* while offering a \*\*rich interactive UX\*\* with numbered selections, retry loops, icons, and post-step summaries.\`

\`\#\# Key Outcomes\`

\`- Fully autonomous AI platform.\`    
\`- On-premise embeddings for private data and synced data from gdrive in /mnt/data/gdrive\`    
\`- Modular stack for AI apps, internal and external LLMs.\`    
\`- customizable routing strategy internal/external via litellm,\`    
\`- Secure networking via Tailscale and optional public proxy.\`    
\`- Comprehensive logging, health checks, and interactive UX.\`

\#\# \\\#\\\# Expected behavior

\- Clean slate system, reset via script 0    
\- Step 1 to install required dependencies, collect all user variables in a nice UX flow interactively    
\- Step 2 deploys each service, performs health checks, display a summary of all services working    
\- Step 3 allow to re-configure a service (re-pair with signal, enter new auth key for gdrive, add a llm provider, change litellm query routing etc    
\- \\- step 4 allows the user to add a new service to the stack and re-deploy

After step2, most urls must be accessible (unless failed services which can be fixed in step3). From there:

\* A user will use openclaw to interrogate data and perfroam actions via channels    
\* A user will use anything llm to work on the same embeddings    
\* A user will use dify or any programmatic agentic flow thatâ€™s connected to the embeddings

Any queries will be routed locally first or externally (based on complexity and maybe more fine tuning later)

â†’\\\> the stack is DOCKERIZED as much as possible and automated and modular. Whilst scripts runs as sudo, they need to retrieve the $PID of the logged in user, and use this for paths, chmod operations etc. This stack may run tomorrow with a different OS, GPU enabled and potentially different EBS volumes, which is why script 1 should take care of mountain the device into /mnt/data, and script 0 deletes and unmount.

\\\#\\\# KEy architecture principles

\#\#\# \*\*2.2 Logging Standard\*\*

All scripts use a common logging pattern:

RED='\\\\033\\\[0;31m'    
GREEN='\\\\033\\\[0;32m'    
YELLOW='\\\\033\\\[1;33m'    
BLUE='\\\\033\\\[0;34m'    
CYAN='\\\\033\\\[0;36m'    
NC='\\\\033\\\[0m'

LOG\\\_FILE="${ROOT\\\_PATH}/logs/script-N.log"

log\\\_info()    { echo \\-e "${GREEN}\\\[INFO\\\]${NC}  $ 1" | tee \\-a " $ LOG\\\_FILE"; }    
log\\\_warn()    { echo \\-e "${YELLOW}\\\[WARN\\\]${NC}  $ 1" | tee \\-a " $ LOG\\\_FILE"; }    
log\\\_error()   { echo \\-e "${RED}\\\[ERROR\\\]${NC}  $ 1" | tee \\-a " $ LOG\\\_FILE"; }    
log\\\_section() { echo \\-e "\\\\n${CYAN}========================================${NC}" | tee \\-a " $ LOG\\\_FILE"    
                echo \\-e " $ {CYAN}   $ 1 $ {NC}" | tee \\-a " $ LOG\\\_FILE"    
                echo \\-e " $ {CYAN}========================================${NC}\\\\n" | tee \\-a "$LOG\\\_FILE"; }

\#\#\# \*\*2.3 Error Handling Standard\*\*

set \\-euo pipefail

trap 'error\\\_handler $? $LINENO  $ BASH\\\_COMMAND' ERR

error\\\_handler() {    
    local exit\\\_code= $ 1    
    local line\\\_number= $ 2    
    local command= $ 3    
    log\\\_error "Command failed at line ${line\\\_number}: ${command} (exit code: ${exit\\\_code})"    
    log\\\_error "Log file: ${LOG\\\_FILE}"    
    exit "${exit\\\_code}"    
}

\#\#\# \*\*2.4 Idempotency Pattern\*\*

Every function follows this pattern:

install\\\_something() {    
    if something\\\_already\\\_installed; then    
        log\\\_info "Something already installed, skipping"    
        return 0    
    fi    
    \\\# ... perform installation ...    
    log\\\_info "Something installed successfully"    
}

\#\# \*\*Section 4: Script Inventory & Flow\*\*

\#\#\# \*\*4.1 Execution Order\*\*

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
â”‚  0-complete-cleanup.sh     â”‚  Purge & reset (optional, for re-installs)    
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    
         â–¼    
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
â”‚ 1-setup-system.sh   â”‚  Hardware, Docker, NVIDIA, Ollama, validation    
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    
         â–¼    
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
â”‚  2-deploy-services.sh â”‚  Questionnaire â†’ generate configs â†’ deploy containers    
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    
         â–¼    
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
â”‚  3-configure-services.sh â”‚  Wait healthy â†’ configure Dify, n8n,signal WebUI etc via APIs    
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    
         â–¼    
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
â”‚ 4-add-service.sh â”‚  Optional: extra services, models, integrations, remove a service    
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\`\*\*\* Target Folder Structure :\` 

\#\# \*\*MODULAR FILE STRUCTURE (CORRECTED)\*\*

/mnt/data/    
â”œâ”€â”€ compose/                           \\\# Individual service compose files    
â”‚   â”œâ”€â”€ nginx.yml                      \\\# If Nginx selected    
â”‚   â”œâ”€â”€ traefik.yml                    \\\# If Traefik selected    
â”‚   â”œâ”€â”€ caddy.yml                      \\\# If Caddy selected    
â”‚   â”œâ”€â”€ postgres.yml                   \\\# Core infrastructure    
â”‚   â”œâ”€â”€ redis.yml    
â”‚   â”œâ”€â”€ qdrant.yml                     \\\# If Qdrant selected    
â”‚   â”œâ”€â”€ weaviate.yml                   \\\# If Weaviate selected    
â”‚   â”œâ”€â”€ milvus.yml                     \\\# If Milvus selected    
â”‚   â”œâ”€â”€ ollama.yml                     \\\# LLM engines    
â”‚   â”œâ”€â”€ litellm.yml    
â”‚   â”œâ”€â”€ localai.yml                    \\\# If selected    
â”‚   â”œâ”€â”€ openwebui.yml                  \\\# AI platforms (if selected)    
â”‚   â”œâ”€â”€ anythingllm.yml    
â”‚   â”œâ”€â”€ dify-api.yml                   \\\# Dify split into 3 services    
â”‚   â”œâ”€â”€ dify-worker.yml    
â”‚   â”œâ”€â”€ dify-web.yml    
â”‚   â”œâ”€â”€ n8n.yml    
â”‚   â”œâ”€â”€ flowise.yml    
â”‚   â”œâ”€â”€ signal-api.yml                 \\\# Integrations (if selected)    
â”‚   â”œâ”€â”€ gdrive.yml    
â”‚   â”œâ”€â”€ langfuse.yml                   \\\# Monitoring (if selected)    
â”‚   â”œâ”€â”€ prometheus.yml    
â”‚   â”œâ”€â”€ grafana.yml    
â”‚   â”œâ”€â”€ loki.yml    
â”‚   â”œâ”€â”€ promtail.yml    
â”‚   â”œâ”€â”€ cadvisor.yml    
â”‚   â””â”€â”€ node-exporter.yml    
â”‚    
â”œâ”€â”€ env/                               \\\# Individual service environment files    
â”‚   â”œâ”€â”€ global.env                     \\\# Shared variables (domain, IPs, etc.)    
â”‚   â”œâ”€â”€ nginx.env    
â”‚   â”œâ”€â”€ traefik.env    
â”‚   â”œâ”€â”€ caddy.env    
â”‚   â”œâ”€â”€ postgres.env    
â”‚   â”œâ”€â”€ redis.env    
â”‚   â”œâ”€â”€ qdrant.env    
â”‚   â”œâ”€â”€ ollama.env    
â”‚   â”œâ”€â”€ litellm.env                    \\\# Contains all provider API keys    
â”‚   â”œâ”€â”€ openwebui.env    
â”‚   â”œâ”€â”€ anythingllm.env    
â”‚   â”œâ”€â”€ dify.env                       \\\# Shared by all 3 Dify services    
â”‚   â”œâ”€â”€ n8n.env    
â”‚   â”œâ”€â”€ flowise.env    
â”‚   â”œâ”€â”€ signal-api.env    
â”‚   â”œâ”€â”€ gdrive.env    
â”‚   â”œâ”€â”€ langfuse.env    
â”‚   â””â”€â”€ monitoring.env                 \\\# Shared by Prometheus, Grafana, Loki    
â”‚    
â”œâ”€â”€ config/                            \\\# Service-specific configuration files    
â”‚   â”œâ”€â”€ nginx/    
â”‚   â”‚   â”œâ”€â”€ nginx.conf                 \\\# Main config    
â”‚   â”‚   â”œâ”€â”€ ssl/                       \\\# SSL certificates    
â”‚   â”‚   â”‚   â”œâ”€â”€ dhparam.pem    
â”‚   â”‚   â”‚   â””â”€â”€ letsencrypt/    
â”‚   â”‚   â””â”€â”€ sites/                     \\\# Per-service configs    
â”‚   â”‚       â”œâ”€â”€ openwebui.conf    
â”‚   â”‚       â”œâ”€â”€ anythingllm.conf    
â”‚   â”‚       â”œâ”€â”€ dify.conf    
â”‚   â”‚       â”œâ”€â”€ n8n.conf    
â”‚   â”‚       â”œâ”€â”€ flowise.conf    
â”‚   â”‚       â”œâ”€â”€ grafana.conf    
â”‚   â”‚       â””â”€â”€ langfuse.conf    
â”‚   â”‚    
â”‚   â”œâ”€â”€ traefik/    
â”‚   â”‚   â”œâ”€â”€ traefik.yml                \\\# Static config    
â”‚   â”‚   â”œâ”€â”€ acme.json                  \\\# Let's Encrypt certificates    
â”‚   â”‚   â””â”€â”€ dynamic/                   \\\# Dynamic configs    
â”‚   â”‚       â”œâ”€â”€ routers.yml    
â”‚   â”‚       â””â”€â”€ middlewares.yml    
â”‚   â”‚    
â”‚   â”œâ”€â”€ caddy/    
â”‚   â”‚   â”œâ”€â”€ Caddyfile                  \\\# Main config (auto-HTTPS)    
â”‚   â”‚   â””â”€â”€ data/                      \\\# Caddy data dir    
â”‚   â”‚    
â”‚   â”œâ”€â”€ litellm/    
â”‚   â”‚   â””â”€â”€ config.yaml                \\\# Routing strategy \\+ model definitions    
â”‚   â”‚    
â”‚   â”œâ”€â”€ postgres/    
â”‚   â”‚   â””â”€â”€ init.sql                   \\\# Create all databases \\+ users    
â”‚   â”‚    
â”‚   â”œâ”€â”€ redis/    
â”‚   â”‚   â””â”€â”€ redis.conf                 \\\# Redis configuration    
â”‚   â”‚    
â”‚   â”œâ”€â”€ prometheus/    
â”‚   â”‚   â””â”€â”€ prometheus.yml             \\\# Scrape configs for all services    
â”‚   â”‚    
â”‚   â”œâ”€â”€ grafana/    
â”‚   â”‚   â”œâ”€â”€ datasources.yml            \\\# Prometheus, Loki    
â”‚   â”‚   â””â”€â”€ dashboards/                \\\# Pre-configured dashboards    
â”‚   â”‚       â”œâ”€â”€ docker.json    
â”‚   â”‚       â”œâ”€â”€ llm-metrics.json    
â”‚   â”‚       â”œâ”€â”€ n8n.json    
â”‚   â”‚       â”œâ”€â”€ dify.json    
â”‚   â”‚       â””â”€â”€ system.json    
â”‚   â”‚    
â”‚   â”œâ”€â”€ loki/    
â”‚   â”‚   â””â”€â”€ loki-config.yaml    
â”‚   â”‚    
â”‚   â”œâ”€â”€ promtail/    
â”‚   â”‚   â””â”€â”€ promtail-config.yaml       \\\# Log collection from all containers    
â”‚   â”‚    
â”‚   â”œâ”€â”€ gdrive/    
â”‚   â”‚   â””â”€â”€ credentials.json           \\\# Service account key (if selected)    
â”‚   â”‚    
â”‚   â””â”€â”€ signal-api/    
â”‚       â””â”€â”€ signal-config.json    
â”‚    
â”œâ”€â”€ metadata/                          \\\# Script 1 outputs (used by script 2\\)    
â”‚   â”œâ”€â”€ selected\\\_services.json         \\\# List of services user selected    
â”‚   â”œâ”€â”€ configuration.json             \\\# All user inputs & generated secrets    
â”‚   â”œâ”€â”€ deployment\\\_plan.json           \\\# Ordered deployment plan    
â”‚   â”œâ”€â”€ proxy\\\_config.json              \\\# Proxy type & SSL settings    
â”‚   â”œâ”€â”€ network\\\_config.json            \\\# Domain, IPs, DNS resolution    
â”‚   â”œâ”€â”€ directory\\\_structure.json       \\\# Paths, symlinks    
â”‚   â”œâ”€â”€ vectordb\\\_choice.json           \\\# Which vector DB was chosen    
â”‚   â”œâ”€â”€ ollama\\\_models.json             \\\# Models to download    
â”‚   â”œâ”€â”€ providers.json                 \\\# External LLM providers configured    
â”‚   â”œâ”€â”€ routing\\\_strategy.json          \\\# LiteLLM routing logic    
â”‚   â”œâ”€â”€ signal\\\_config.json             \\\# Signal pairing method & number    
â”‚   â”œâ”€â”€ gdrive\\\_config.json             \\\# GDrive auth method & credentials    
â”‚   â”œâ”€â”€ port\\\_check.json                \\\# Port availability results    
â”‚   â””â”€â”€ deployment\\\_summary.json        \\\# Human-readable summary    
â”‚    
â”œâ”€â”€ data/                              \\\# Actual persistent data    
â”‚   â”œâ”€â”€ postgres/                      \\\# Database files    
â”‚   â”œâ”€â”€ redis/                         \\\# Redis persistence    
â”‚   â”œâ”€â”€ qdrant/                        \\\# Vector DB storage    
â”‚   â”œâ”€â”€ ollama/models/                 \\\# Downloaded Ollama models    
â”‚   â”œâ”€â”€ litellm/                       \\\# LiteLLM database    
â”‚   â”œâ”€â”€ n8n/                           \\\# N8N workflows & executions    
â”‚   â”œâ”€â”€ dify/                          \\\# Dify knowledge base & uploads    
â”‚   â”œâ”€â”€ anythingllm/documents/         \\\# AnythingLLM documents    
â”‚   â”œâ”€â”€ flowise/                       \\\# Flowise flows    
â”‚   â”œâ”€â”€ grafana/                       \\\# Grafana dashboards & plugins    
â”‚   â”œâ”€â”€ prometheus/                    \\\# Prometheus TSDB    
â”‚   â”œâ”€â”€ loki/                          \\\# Loki chunks    
â”‚   â””â”€â”€ langfuse/                      \\\# Langfuse traces    
â”‚    
â””â”€â”€ backups/                           \\\# Backup location    
    â””â”€â”€ pre-install-YYYYMMDD-HHMMSS.tar.gz

Script 2 will:    
1\\. Read metadata/\\\*.json files    
2\\. Merge compose/\\\*.yml files into final docker-compose.yml    
3\\. Merge env/\\\*.env files into final .env    
4\\. Copy config/\\\* to appropriate locations    
5\\. Deploy services based on deployment\\\_plan.json    
\`---\`

\`\#\# Network Architecture\`

     \`â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\`    
      \`â”‚ Public IP 80\`      
      \`â”‚ Proxy/SSL 443â”‚\`    
      \`â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\`    
            \`â”‚\`    
      \`â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\`    
      \`â”‚  Tailscale  â”‚\`    
      \`â”‚ IP :8443 )  â”‚\`    
      \`â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\`    
            \`â”‚\`

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
 â”‚ Core LLMs â”‚    
 â”‚ Ollama | LiteLLM | AnythingLLMâ”‚    
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    
 â”‚    
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
 â”‚ Vector DB â”‚    
 â”‚ Chroma | Qdrant â”‚    
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    
 â”‚    
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
 â”‚ AI Applications â”‚    
 â”‚ Dify | ComfyUI | OpenWebUI â”‚    
 â”‚ Flowise | OpenClaw UI â”‚    
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    
 â”‚    
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
 â”‚ Optional Monitoring â”‚    
 â”‚ Grafana | Prometheus | ELK â”‚    
 â”‚ Portainer â”‚    
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\\\#\\\# Section 5: Script 0 â€” Cleanup System

\\\#\\\#\\\# 5.1 Purpose

\\\`0-cleanup.sh\\\` removes all traces of a previous installation so the system can be re-provisioned cleanly. It is \\\*\\\*optional\\\*\\\* â€” only needed when re-installing or resetting (to validate entire script consistency).

\\\#\\\#\\\# 5.2 Safety

The script requires explicit confirmation before proceeding. It distinguishes between:

\\- \\\*\\\*Soft reset\\\*\\\* â€” Stop containers, remove configs, keep data volumes    
\\- \\\*\\\*Hard reset\\\*\\\* â€” Remove everything including data volumes

\\\#\\\#\\\# 5.3 Execution

\\\`\\\`\\\`bash    
sudo bash 0-cleanup.sh          \\\# Interactive â€” asks which mode    
sudo bash 0-cleanup.sh \\--hard   \\\# Non-interactive hard reset    
sudo bash 0-cleanup.sh \\--soft   \\\# Non-interactive soft reset

\#\#\# \*\*5.4 Cleanup Phases\*\*

Phase 1: Stop & Remove Containers    
  â”œâ”€â”€ Find all docker-compose.\\\*.yml files in /mnt/data/ai-platform/docker/    
  â”œâ”€â”€ For each: docker compose \\-f \\\<file\\\> down \\--remove-orphans    
  â”œâ”€â”€ docker container prune \\-f    
  â””â”€â”€ Remove external networks (ai-platform, ai-backend)

Phase 2: Remove Docker Volumes (hard mode only)    
  â”œâ”€â”€ docker volume ls \\--filter label=com.docker.compose.project    
  â”œâ”€â”€ docker volume rm \\\<each volume\\\>    
  â””â”€â”€ docker volume prune \\-f

Phase 3: Stop Ollama (optional)    
  â”œâ”€â”€ systemctl stop ollama    
  â”œâ”€â”€ systemctl disable ollama    
  â””â”€â”€ Note: does NOT uninstall Ollama binary (Script 1 handles install)

Phase 4: Remove Configuration Files    
  â”œâ”€â”€ rm \\-rf /mnt/data/ai-platform/config/\\\*    
  â”œâ”€â”€ rm \\-rf /mnt/data/ai-platform/docker/\\\*    
  â”œâ”€â”€ rm \\-rf /mnt/data/ai-platform/scripts/\\\*    
  â””â”€â”€ rm \\-rf /mnt/data/ai-platform/logs/\\\*

Phase 5: Remove Data Directories (hard mode only)    
  â”œâ”€â”€ rm \\-rf /mnt/data/ai-platform/data/\\\*    
  â””â”€â”€ rm \\-rf /mnt/data/ai-platform/backups/\\\*    
Phase 6 : apt purge, remove cache, docker purge, reboot

Script will handle re-creating all environment and directory    
Phase 1: Recreate Directory Structure    
  â”œâ”€â”€ mkdir \\-p /mnt/data/ai-platform/{config,docker,data,logs,scripts,backups}    
  â””â”€â”€ chown \\-R ${SUDO\\\_USER:- $ USER}: $ {SUDO\\\_USER:-$USER} /mnt/data/ai-platform/

\#\#\# \*\*5.5 What It Does NOT Remove\*\*

\* Docker Engine itself (Script 1 manages this)    
\* NVIDIA drivers or Container Toolkit (Script 1 manages this)    
\* Ollama binary (Script 1 manages this)    
\* System packages    
\* User accounts

\#\# \*\*SCRIPT 1: SETUP SYSTEM\*\*

\#\#\# \*\*Intent\*\*

Prepare the complete foundation for deployment WITHOUT starting any AI services. This script collects all configuration, allocates ports, creates directory structures, and generates the master \`.env\` file.

\#\#\# \*\*Key Responsibilities\*\*

\#\#\#\# \*\*1\\. System Validation\*\*

\* Root privileges check    
\* Docker installation verification    
\* Docker daemon running check    
\* GPU detection (NVIDIA/AMD/None)

\#\#\#\# \*\*2\\. User & Permissions\*\*

\* Add user to \`docker\` group    
\* Automate session refresh (\`newgrp docker\` or logout warning)    
\* Create service user: \`ai-user\` (non-root for containers)    
\* Mount \`/mnt/data\` (persistent storage for large datasets)    
\* Create \`/mnt/data/gdrive/\` for rsync target and all directory structure for all stacks

\#\#\# \*\*Success Definition\*\*

\* âœ… All directories created    
\* âœ… \`/mnt/data\` mounted and accessible    
\* âœ… All ports allocated (no conflicts)    
\* âœ… \`.env\` file generated (pure text, no ANSI codes)    
\* âœ… \`credentials.txt\` prepared (populated in Script 2\\)    
\* âœ… Reverse proxy config files created (not deployed)    
\* âœ… User confirmed configuration summary    
\* âœ… Docker group permissions active    
\* âœ… NO containers running yet

\`UI EXPECTED OUTPUTS\` 

\`Script 1 expected output :\`   

# **SCRIPT 1: COMPLETE UI FLOW \- CORRECTED**

## **System Setup & Configuration Collection**

**Version:** 4.0.0  
 **Purpose:** Collect ALL configuration, generate modular files, prepare metadata  
 **Path:** All files in `/mnt/data/` (NO `/opt`)  
 **Important:** This script does NOT deploy \- only prepares configuration

---

## **ğŸ¯ Complete Variable Collection List (67 Variables)**

### **System Detection (Auto) \- 7 variables**

* OS type and version  
* CPU cores  
* RAM (GB)  
* Disk space (GB)  
* GPU type (nvidia/amd/intel/apple/none)  
* GPU count  
* Hardware mode (gpu/cpu)

### **Network & Domain \- 5 variables**

* Base domain  
* Proxy type (nginx/traefik/caddy/none)  
* SSL type (letsencrypt/self-signed/none)  
* Let's Encrypt email (if applicable)  
* Cloudflare API token (optional, for DNS challenge)

### **Core Infrastructure \- 4 variables**

* Vector DB choice (qdrant/weaviate/milvus)  
* PostgreSQL version  
* Redis version  
* Object storage type (minio/s3)

### **Core AI Services \- 8 variables**

* Ollama enable (Y/n)  
* Ollama models list (comma-separated)  
* Ollama port (default: 11434\)  
* LiteLLM enable (Y/n)  
* LiteLLM port (default: 4000\)  
* LiteLLM routing strategy (cost/latency/simple-shuffle/usage)  
* Open WebUI enable (Y/n)  
* Open WebUI port (default: 3000\)

### **AI Platforms \- 6 variables**

* AnythingLLM enable (y/N)  
* AnythingLLM port (default: 3001\)  
* Dify enable (y/N)  
* Dify API port (default: 5001\)  
* Dify Web port (default: 3002\)  
* Dify sandbox enable (for code execution)

### **Workflow Tools \- 6 variables**

* n8n enable (y/N)  
* n8n port (default: 5678\)  
* Flowise enable (y/N)  
* Flowise port (default: 3003\)  
* Apache Airflow enable (y/N)  
* Airflow webserver port (default: 8080\)

### **Search & Web Scraping \- 7 variables**

* OpenClaw enable (y/N)  
* OpenClaw port (default: 8000\)  
* Brave Search API key (for web search)  
* SerpAPI key (alternative web search)  
* Web search provider (brave/serpapi/none)  
* Firecrawl enable (for web scraping)  
* Firecrawl API key

### **Signal API \- 5 variables**

* Signal API enable (y/N)  
* Signal API port (default: 8080\)  
* Signal phone number (E.164 format)  
* Signal auth method (qr-code/linking-code)  
* Signal webhook URL (for incoming messages)

### **Google Drive Integration \- 7 variables**

* Google Drive enable (y/N)  
* GDrive auth method (oauth/service-account/rclone)  
* GDrive Client ID (if oauth)  
* GDrive Client Secret (if oauth)  
* GDrive Service Account JSON (if service-account)  
* GDrive sync interval (minutes, default: 15\)  
* GDrive target folders (comma-separated)

### **Tailscale VPN \- 3 variables**

* Tailscale enable (y/N)  
* Tailscale auth key  
* Tailscale exit node enable (y/N)

### **LLM Provider API Keys \- 7 variables**

* OpenAI API key  
* Anthropic API key  
* Google Gemini API key  
* Groq API key  
* Mistral API key  
* OpenRouter API key  
* HuggingFace API key

### **Auto-Generated Secrets (Overridable) \- 12 variables**

Each with: auto-generate OR custom value option

* PostgreSQL master password  
* Redis password  
* Qdrant API key  
* Admin password (for UIs)  
* JWT secret (for auth)  
* Encryption key (for data at rest)  
* n8n encryption key  
* Dify secret key  
* MinIO root password  
* Grafana admin password  
* LiteLLM master key  
* Webhook secret (for integrations)

---

## **ğŸ“º COMPLETE UI FLOW**

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘                                                                    â•‘  
â•‘            AI PLATFORM AUTOMATION \- SETUP                          â•‘  
â•‘                      Version 4.0.0                                 â•‘  
â•‘               Configuration Collection Only                        â•‘  
â•‘                  (No Deployment in Script 1\)                       â•‘  
â•‘                                                                    â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

All files will be created in: /mnt/data/  
Deployment will happen in Script 2

Repository root: /home/user/AIPlatformAutomation  
Running as user: john

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 1: System Detection & Hardware Configuration                â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ Detecting system hardware...

System Information:  
  â€¢ OS: ubuntu 22.04  
  â€¢ Architecture: x86\_64  
  â€¢ CPU Cores: 8  
  â€¢ RAM: 32GB  
  â€¢ Available Disk: 250GB

â–¶ GPU Detection...  
  Checking for NVIDIA GPUs... âœ“ Found  
  â€¢ GPU Type: NVIDIA GeForce RTX 3090  
  â€¢ GPU Count: 1  
  â€¢ CUDA Version: 12.1  
  â€¢ Driver Version: 525.147.05

Hardware Mode: GPU-Accelerated âœ“

â–¶ Checking system requirements (guidelines)...  
âœ“ CPU: 8 cores (4+ recommended)  
âœ“ RAM: 32GB (16GB+ recommended)    
âœ“ Disk: 250GB (50GB+ minimum)  
âœ“ GPU: NVIDIA detected (optional but recommended)

âš  Note: Your system exceeds minimum requirements  
  GPU acceleration will be enabled for:  
  \- Ollama (local LLM inference)  
  \- Dify (if using local embeddings)  
  \- Any ML workloads

Continue with GPU-accelerated configuration? (Y/n): y

âœ“ System detection completed  
  Mode: GPU-Accelerated  
  Ollama will use: NVIDIA GPU  
  Recommended models: llama3.1:70b, mixtral:8x7b, codestral

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 2: Package Installation                                     â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ Installing system dependencies...  
  (These were removed by Script 0 cleanup)

â–¶ Essential packages:  
  âœ“ curl  
  âœ“ wget  
  âœ“ git  
  âœ“ jq (for JSON processing)  
  âœ“ openssl (for secret generation)  
  âœ“ ca-certificates  
  âœ“ gnupg

â–¶ Docker prerequisites:  
  âœ“ apt-transport-https  
  âœ“ software-properties-common  
  âœ“ lsb-release

â–¶ Network tools:  
  âœ“ net-tools  
  âœ“ dnsutils  
  âœ“ iputils-ping

â–¶ Monitoring tools:  
  âœ“ htop  
  âœ“ iotop  
  âœ“ ncdu

âœ“ All system packages installed

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 3: Docker Installation & Configuration                      â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ Checking Docker installation...

Docker not found. Installing Docker CE...

â–¶ Adding Docker's official GPG key...  
âœ“ GPG key added

â–¶ Adding Docker repository...  
âœ“ Repository configured for ubuntu jammy

â–¶ Installing Docker packages...  
  âœ“ docker-ce (25.0.3)  
  âœ“ docker-ce-cli  
  âœ“ containerd.io  
  âœ“ docker-buildx-plugin  
  âœ“ docker-compose-plugin

â–¶ Configuring Docker daemon...  
  Log driver: json-file (max-size: 10m, max-file: 3\)  
  Live restore: enabled  
  Storage driver: overlay2  
    
â–¶ GPU Support Configuration...  
  âœ“ NVIDIA Container Toolkit detected  
  âœ“ GPU runtime configured

â–¶ Starting Docker service...  
  âœ“ Docker daemon started  
  âœ“ Docker daemon enabled (auto-start on boot)

â–¶ User configuration...  
  âœ“ User 'john' added to docker group  
  âš  You'll need to log out and back in for group changes to take effect

âœ“ Docker installed successfully  
  Version: Docker version 25.0.3, build 4debf41  
  Compose: Docker Compose version v2.24.5

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 4: Directory Structure Creation                             â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ Creating modular directory structure at /mnt/data/...

â–¶ Creating core directories:  
  âœ“ /mnt/data/compose/           (individual service compose files)  
  âœ“ /mnt/data/env/               (individual service .env files)  
  âœ“ /mnt/data/config/            (service-specific configs)  
  âœ“ /mnt/data/metadata/          (deployment metadata)  
  âœ“ /mnt/data/logs/              (setup logs)  
  âœ“ /mnt/data/secrets/           (encrypted secrets storage)

â–¶ Creating config subdirectories:  
  âœ“ /mnt/data/config/nginx/  
  âœ“ /mnt/data/config/traefik/  
  âœ“ /mnt/data/config/caddy/  
  âœ“ /mnt/data/config/litellm/  
  âœ“ /mnt/data/config/ollama/  
  âœ“ /mnt/data/config/postgres/  
  âœ“ /mnt/data/config/prometheus/  
  âœ“ /mnt/data/config/grafana/  
  âœ“ /mnt/data/config/loki/

â–¶ Setting permissions:  
  âœ“ Owner: john:john (UID:1000, GID:1000)  
  âœ“ Permissions: 755 (directories), 600 (secrets)

âœ“ Directory structure created  
  Base: /mnt/data/

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 5: Docker Networks Creation                                 â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ Creating Docker networks for service isolation...

  âœ“ ai-platform               (bridge, public services)  
  âœ“ ai-platform-internal      (internal, isolated services)  
  âœ“ ai-platform-monitoring    (monitoring stack)

Network architecture:  
  â€¢ Public services (OpenWebUI, Dify) â†’ ai-platform  
  â€¢ Databases, queues â†’ ai-platform-internal  
  â€¢ Prometheus, Grafana â†’ ai-platform-monitoring  
  â€¢ Proxy can access all networks

âœ“ Docker networks configured

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 6: Firewall Configuration                                   â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ Firewall management delegated to EC2 Security Groups

âš  IMPORTANT: Configure your EC2 Security Group to allow:  
  â€¢ SSH (22/tcp) \- from your IP only  
  â€¢ HTTP (80/tcp) \- if using Let's Encrypt  
  â€¢ HTTPS (443/tcp) \- for public access  
  â€¢ Custom ports \- if exposing services directly

This script does NOT configure UFW or iptables.  
All firewall rules should be managed at the EC2 level.

âœ“ Firewall note recorded

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 7: Reverse Proxy Selection                                  â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Select reverse proxy for public access:

  1\) Nginx      \- Traditional, battle-tested, full control  
  2\) Traefik    \- Modern, auto-discovery, Docker labels  
  3\) Caddy      \- Automatic HTTPS, zero config  
  4\) None       \- Direct port access (testing only)

Which proxy? \[1-4\] (default: 3): 3

Selected: Caddy âœ“

â–¶ Caddy configuration:  
  Automatic HTTPS: Yes  
  HTTP/3 support: Yes  
  Auto-reload on config change: Yes

âœ“ Proxy type: caddy

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 8: Domain & SSL Configuration                               â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Domain Configuration  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Enter your base domain (e.g., example.com): ai.mycompany.com

âœ“ Domain: ai.mycompany.com

Services will be accessible at:  
  â€¢ Open WebUI:  https://chat.ai.mycompany.com  
  â€¢ Dify:        https://dify.ai.mycompany.com  
  â€¢ n8n:         https://workflows.ai.mycompany.com  
  â€¢ Grafana:     https://metrics.ai.mycompany.com  
  â€¢ LiteLLM:     https://api.ai.mycompany.com

SSL Certificate Configuration  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  1\) Let's Encrypt \- Free, automatic renewal, requires DNS  
  2\) Self-signed   \- Testing/internal use only  
  3\) None          \- HTTP only (not recommended)

Select SSL type \[1-3\] (default: 1): 1

Selected: Let's Encrypt âœ“

Enter email for Let's Encrypt notifications: admin@mycompany.com

âœ“ SSL Type: letsencrypt  
âœ“ Email: admin@mycompany.com

âš  DNS Configuration Required:  
  Before running Script 2, create DNS A records:  
  â€¢ chat.ai.mycompany.com      â†’ \<your-server-ip\>  
  â€¢ dify.ai.mycompany.com      â†’ \<your-server-ip\>  
  â€¢ workflows.ai.mycompany.com â†’ \<your-server-ip\>  
  â€¢ metrics.ai.mycompany.com   â†’ \<your-server-ip\>  
  â€¢ api.ai.mycompany.com       â†’ \<your-server-ip\>

Optional: Use Cloudflare DNS Challenge?  
(Allows SSL without exposing port 80\) (y/N): y

Enter Cloudflare API Token: \[paste token\]

âœ“ Cloudflare DNS challenge configured

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 9: Core Infrastructure Selection                            â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” PostgreSQL Configuration â”â”â”

PostgreSQL is REQUIRED for:  
  â€¢ LiteLLM (request logging, API keys)  
  â€¢ n8n (workflows, credentials)  
  â€¢ Dify (app configurations)  
  â€¢ Flowise (chatflows)  
  â€¢ Langfuse (observability logs)

PostgreSQL Version:  
  1\) 16 (latest, recommended)  
  2\) 15  
  3\) 14

Select version \[1-3\] (default: 1): 1

âœ“ PostgreSQL: 16-alpine

â”â”â” Redis Configuration â”â”â”

Redis is REQUIRED for:  
  â€¢ Caching (LiteLLM, Dify)  
  â€¢ Queue management (n8n, Airflow)  
  â€¢ Session storage

âœ“ Redis: 7-alpine

â”â”â” Vector Database Selection â”â”â”

A vector database is REQUIRED for RAG capabilities.  
Choose ONE:

  1\) Qdrant     \- Fastest, simplest, recommended  
  2\) Weaviate   \- Advanced, graph capabilities  
  3\) Milvus     \- Enterprise-scale, most complex

Select vector DB \[1-3\] (default: 1): 1

âœ“ Vector DB: Qdrant

â”â”â” Object Storage â”â”â”

For file uploads, backups, artifacts:

  1\) MinIO      \- Self-hosted S3-compatible  
  2\) AWS S3     \- Managed service (requires credentials)  
  3\) None       \- Use local filesystem only

Select storage \[1-3\] (default: 1): 1

âœ“ Object Storage: MinIO

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 10: Core AI Services Configuration                          â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” Ollama (Local LLM Engine) â”â”â”

Install Ollama for local LLM inference? (Y/n): y

âœ“ Ollama: enabled

â–¶ Ollama Configuration:

Port for Ollama API \[default: 11434\]: 11434  
âœ“ Port: 11434

GPU Configuration:  
  Detected: NVIDIA GeForce RTX 3090  
  âœ“ GPU acceleration: ENABLED  
  âœ“ CUDA layers: All layers on GPU

Which models to pre-download? (comma-separated)  
Recommended for your GPU (24GB VRAM):  
  \- llama3.1:8b       (4.7GB)  \- Fast, general purpose  
  \- llama3.1:70b      (40GB)   \- Powerful, needs offloading  
  \- mixtral:8x7b      (26GB)   \- Good quality/speed  
  \- codestral:22b     (12GB)   \- Best for coding  
  \- phi3:mini         (2.3GB)  \- Tiny, fast

Enter model list: llama3.1:8b,codestral:22b,phi3:mini

âœ“ Models to download: llama3.1:8b, codestral:22b, phi3:mini  
  Total size: \~19GB

â”â”â” LiteLLM (AI Gateway & Router) â”â”â”

Install LiteLLM for unified AI API? (Y/n): y

âœ“ LiteLLM: enabled

â–¶ LiteLLM Configuration:

Port for LiteLLM API \[default: 4000\]: 4000  
âœ“ Port: 4000

Routing Strategy:  
  1\) cost           \- Cheapest model first  
  2\) latency        \- Fastest response first  
  3\) simple-shuffle \- Random load balancing  
  4\) usage          \- Least-used model first

Select strategy \[1-4\] (default: 1): 1

âœ“ Routing: cost-based (Ollama free â†’ Groq cheap â†’ OpenAI expensive)

Database for LiteLLM:  
  âœ“ PostgreSQL database: litellm\_db (will be auto-created)  
  âœ“ Logging: All requests logged  
  âœ“ Caching: Redis cache enabled

â”â”â” Open WebUI (Chat Interface) â”â”â”

Install Open WebUI for chat interface? (Y/n): y

âœ“ Open WebUI: enabled

Port for Open WebUI \[default: 3000\]: 3000  
âœ“ Port: 3000

Connect to:  
  âœ“ Ollama: http://ollama:11434  
  âœ“ LiteLLM: http://litellm:4000  
  âœ“ Vector DB: Qdrant

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 11: AI Platform Services                                    â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” AnythingLLM â”â”â”

Install AnythingLLM? (y/N): n

â”â”â” Dify (AI Application Platform) â”â”â”

Install Dify for building AI apps? (y/N): y

âœ“ Dify: enabled

â–¶ Dify Configuration:

API Port \[default: 5001\]: 5001  
Web UI Port \[default: 3002\]: 3002

âœ“ Ports: API 5001, Web 3002

Enable Dify Sandbox (for code execution)? (y/N): y  
âœ“ Sandbox: enabled (isolated Docker-in-Docker)

Dify will use:  
  âœ“ PostgreSQL: dify\_db (auto-created)  
  âœ“ Redis: Caching  
  âœ“ Qdrant: Vector storage  
  âœ“ LiteLLM: LLM routing

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 12: Workflow Automation Tools                               â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” n8n (Workflow Automation) â”â”â”

Install n8n? (y/N): y

âœ“ n8n: enabled

Port \[default: 5678\]: 5678  
âœ“ Port: 5678

n8n will use:  
  âœ“ PostgreSQL: n8n\_db (auto-created)  
  âœ“ Encryption: Auto-generated key

â”â”â” Flowise â”â”â”

Install Flowise (no-code AI workflows)? (y/N): y

âœ“ Flowise: enabled

Port \[default: 3003\]: 3003  
âœ“ Port: 3003

â”â”â” Apache Airflow â”â”â”

Install Apache Airflow (data orchestration)? (y/N): n

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 13: Search & Web Scraping                                   â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” OpenClaw (AI Web Agent) â”â”â”

Install OpenClaw for web browsing/research? (y/N): y

âœ“ OpenClaw: enabled

Port \[default: 8000\]: 8000  
âœ“ Port: 8000

Web Search Provider:  
  1\) Brave Search    \- Fast, privacy-focused (API key required)  
  2\) SerpAPI         \- Google results (API key required)  
  3\) None            \- No web search

Select provider \[1-3\] (default: 1): 1

Enter Brave Search API Key: BSA\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

âœ“ Brave Search API: configured

â”â”â” Firecrawl â”â”â”

Install Firecrawl for web scraping? (y/N): n

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 14: Signal API Configuration                                â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” Signal API (SMS/Messaging) â”â”â”

Install Signal API for messaging? (y/N): y

âœ“ Signal API: enabled

Port \[default: 8080\]: 8080  
âœ“ Port: 8080

Signal Phone Number (E.164 format, e.g., \+14155551234): \+14155551234  
âœ“ Phone: \+14155551234

Registration Method:  
  1\) QR Code        \- Scan with Signal app (easier)  
  2\) Linking Code   \- Enter 6-digit code from app

Select method \[1-2\] (default: 1): 1

âœ“ Method: QR Code (will be shown during Script 2 deployment)

Webhook URL for incoming messages (optional):  
Leave blank to skip: https://api.mycompany.com/webhooks/signal

âœ“ Webhook: https://api.mycompany.com/webhooks/signal

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 15: Google Drive Integration                                â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” Google Drive Sync â”â”â”

Install Google Drive integration? (y/N): y

âœ“ Google Drive: enabled

Authentication Method:  
  1\) OAuth          \- Interactive browser auth (easiest)  
  2\) Service Account \- JSON key file (automated)  
  3\) Rclone Config  \- Pre-configured rclone.conf

Select method \[1-3\] (default: 1): 2

âœ“ Method: Service Account

Upload/paste Service Account JSON:  
(Contents will be securely stored)

\[Paste JSON content here\]

âœ“ Service Account: configured

Folders to sync (comma-separated, or \* for all):  
  e.g., "Documents/AI,Projects/Research"

Enter folders: Documents/AI,Research

âœ“ Folders: Documents/AI, Research

Sync interval (minutes) \[default: 15\]: 15  
âœ“ Interval: 15 minutes

Target directory in AnythingLLM/Dify: /data/gdrive  
âœ“ Auto-ingestion: enabled

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 16: Tailscale VPN (Optional)                                â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” Tailscale â”â”â”

Install Tailscale for secure remote access? (y/N): y

âœ“ Tailscale: enabled

Tailscale Auth Key:  
(Get from: https://login.tailscale.com/admin/settings/keys)

Enter auth key: tskey-auth-\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

âœ“ Auth key: configured

Use this machine as exit node? (y/N): n

âœ“ Exit node: disabled

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 17: LLM Provider API Keys                                   â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” External LLM API Keys â”â”â”

Configure external LLM providers for LiteLLM routing.  
(All optional \- press Enter to skip)

OpenAI API Key: sk-proj-\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*  
âœ“ OpenAI: configured

Anthropic API Key: sk-ant-\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*  
âœ“ Anthropic: configured

Google Gemini API Key: \[Enter to skip\]  
âŠ˜ Gemini: skipped

Groq API Key: gsk\_\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*  
âœ“ Groq: configured

Mistral API Key: \[Enter to skip\]  
âŠ˜ Mistral: skipped

OpenRouter API Key: \[Enter to skip\]  
âŠ˜ OpenRouter: skipped

HuggingFace API Key: \[Enter to skip\]  
âŠ˜ HuggingFace: skipped

Summary:  
  âœ“ Configured: 3 providers (OpenAI, Anthropic, Groq)  
  âŠ˜ Skipped: 4 providers

LiteLLM will route:  
  1\. Ollama models (free, local)  
  2\. Groq models (cheap, fast)  
  3\. OpenAI models (expensive, powerful)  
  4\. Anthropic models (expensive, powerful)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 18: Security & Secrets Configuration                        â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” Auto-Generated Secrets â”â”â”

Generating secure credentials for all services...  
(You can override any of these with custom values)

â–¶ Core Infrastructure:  
  PostgreSQL master password:    \[auto\] âœ“ Generated (32 chars)  
    Override with custom value? (y/N): n  
    
  Redis password:                \[auto\] âœ“ Generated (32 chars)  
    Override? (y/N): n  
    
  Qdrant API key:                \[auto\] âœ“ Generated (32 chars)  
    Override? (y/N): n

â–¶ Platform Security:  
  Admin password (UIs):          \[auto\] âœ“ Generated (24 chars)  
    Override? (y/N): n  
    
  JWT secret:                    \[auto\] âœ“ Generated (64 chars)  
    Override? (y/N): n  
    
  Encryption key:                \[auto\] âœ“ Generated (32 bytes hex)  
    Override? (y/N): n

â–¶ Service-Specific:  
  n8n encryption key:            \[auto\] âœ“ Generated  
  Dify secret key:               \[auto\] âœ“ Generated  
  MinIO root password:           \[auto\] âœ“ Generated  
  Grafana admin password:        \[auto\] âœ“ Generated  
  LiteLLM master key:            \[auto\] âœ“ Generated  
  Webhook secret:                \[auto\] âœ“ Generated

âœ“ All secrets generated  
  Location: /mnt/data/secrets/  
  Permissions: 600 (owner only)

âš  CRITICAL: These secrets will be saved to:  
  /mnt/data/metadata/secrets.json (encrypted)  
    
  BACKUP THIS FILE IMMEDIATELY AFTER SETUP\!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 19: Monitoring Stack (Optional)                             â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”â”â” Monitoring â”â”â”

Install monitoring stack (Prometheus \+ Grafana \+ Loki)? (Y/n): y

âœ“ Monitoring: enabled

Prometheus port \[default: 9090\]: 9090  
Grafana port \[default: 3001\]: 3001  
Loki port \[default: 3100\]: 3100

âœ“ Monitoring configured:  
  â€¢ Prometheus: http://prometheus:9090  
  â€¢ Grafana: http://grafana:3001  
  â€¢ Loki: http://loki:3100

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 20: Custom Port Configuration Summary                       â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Review port assignments:

Service                Internal Port    Public URL  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Ollama                 11434           (internal only)  
LiteLLM                4000            https://api.ai.mycompany.com  
Open WebUI             3000            https://chat.ai.mycompany.com  
Dify API               5001            (internal only)  
Dify Web               3002            https://dify.ai.mycompany.com  
n8n                    5678            https://workflows.ai.mycompany.com  
Flowise                3003            https://flowise.ai.mycompany.com  
OpenClaw               8000            https://browse.ai.mycompany.com  
Signal API             8080            (webhook only)  
Prometheus             9090            (internal only)  
Grafana                3001            https://metrics.ai.mycompany.com

PostgreSQL             5432            (internal only)  
Redis                  6379            (internal only)  
Qdrant                 6333            (internal only)  
MinIO API              9000            (internal only)  
MinIO Console          9001            https://storage.ai.mycompany.com

Modify any ports? (y/N): n

âœ“ Port configuration confirmed

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 21: Generating Metadata Files                               â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ Generating deployment metadata...

â–¶ Creating metadata files:

  âœ“ configuration.json  
    \- System hardware info  
    \- Network configuration  
    \- Domain & SSL settings  
    \- GPU configuration  
    
  âœ“ selected\_services.json  
    \- Enabled services list  
    \- Service dependencies  
    \- Port mappings  
    
  âœ“ deployment\_plan.json  
    \- Deployment order  
    \- Service dependencies graph  
    \- Health check configuration  
    
  âœ“ secrets.json (encrypted)  
    \- Auto-generated secrets  
    \- API keys  
    \- Service credentials  
    
  âœ“ litellm\_config.json  
    \- Model routing configuration  
    \- Provider priorities  
    \- Cost/latency settings

All metadata files created in: /mnt/data/metadata/

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 22: Generating Modular Compose Files                        â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ Generating individual Docker Compose files...

Creating compose files for 15 services:

  âœ“ postgres.yml        (PostgreSQL 16\)  
  âœ“ redis.yml           (Redis 7\)  
  âœ“ qdrant.yml          (Qdrant vector DB)  
  âœ“ minio.yml           (Object storage)  
  âœ“ caddy.yml           (Reverse proxy)  
  âœ“ ollama.yml          (Local LLM \- GPU enabled)  
  âœ“ litellm.yml         (AI gateway)  
  âœ“ openwebui.yml       (Chat UI)  
  âœ“ dify-api.yml        (Dify backend)  
  âœ“ dify-worker.yml     (Dify worker)  
  âœ“ dify-web.yml        (Dify frontend)  
  âœ“ n8n.yml             (Workflows)  
  âœ“ flowise.yml         (No-code AI)  
  âœ“ openclaw.yml        (Web agent)  
  âœ“ signal-api.yml      (Messaging)  
  âœ“ prometheus.yml      (Metrics)  
  âœ“ grafana.yml         (Dashboards)  
  âœ“ loki.yml            (Logs)

All compose files created in: /mnt/data/compose/

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 23: Generating Modular Environment Files                    â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ Generating individual .env files...

Creating environment files for 15 services:

  âœ“ postgres.env        (DB credentials)  
  âœ“ redis.env           (Cache config)  
  âœ“ qdrant.env          (Vector DB config)  
  âœ“ minio.env           (Storage credentials)  
  âœ“ caddy.env           (Proxy \+ SSL config)  
  âœ“ ollama.env          (GPU config \+ models)  
  âœ“ litellm.env         (API keys \+ routing)  
  âœ“ openwebui.env       (UI config)  
  âœ“ dify.env            (Dify stack config)  
  âœ“ n8n.env             (Workflow config)  
  âœ“ flowise.env         (Flowise config)  
  âœ“ openclaw.env        (Search API keys)  
  âœ“ signal.env          (Phone \+ auth config)  
  âœ“ gdrive.env          (Drive sync config)  
  âœ“ monitoring.env      (Prometheus \+ Grafana)

All env files created in: /mnt/data/env/  
Permissions: 600 (owner only \- contains secrets)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 24: Generating Service Configuration Files                  â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ Generating service-specific configuration files...

â–¶ LiteLLM Router Configuration:  
  âœ“ config/litellm/config.yaml  
    
  Models configured:  
    Local (Ollama):  
      â€¢ llama3.1:8b      â†’ ollama/llama3.1:8b  
      â€¢ codestral:22b    â†’ ollama/codestral:22b  
      â€¢ phi3:mini        â†’ ollama/phi3:mini  
      
    Cloud (API):  
      â€¢ gpt-4o-mini      â†’ openai/gpt-4o-mini      (cost: $0.15/1M)  
      â€¢ gpt-4o           â†’ openai/gpt-4o           (cost: $2.50/1M)  
      â€¢ claude-3-5-sonnet â†’ anthropic/claude-3-5-sonnet (cost: $3.00/1M)  
      â€¢ llama-3.1-70b    â†’ groq/llama-3.1-70b-versatile (cost: $0.59/1M)  
      
  Routing: cost-based (Ollama â†’ Groq â†’ OpenAI â†’ Anthropic)

â–¶ Caddy Reverse Proxy:  
  âœ“ config/caddy/Caddyfile  
    
  Routes configured:  
    â€¢ chat.ai.mycompany.com      â†’ openwebui:3000  
    â€¢ api.ai.mycompany.com       â†’ litellm:4000  
    â€¢ dify.ai.mycompany.com      â†’ dify-web:3002  
    â€¢ workflows.ai.mycompany.com â†’ n8n:5678  
    â€¢ flowise.ai.mycompany.com   â†’ flowise:3003  
    â€¢ browse.ai.mycompany.com    â†’ openclaw:8000  
    â€¢ metrics.ai.mycompany.com   â†’ grafana:3001  
    â€¢ storage.ai.mycompany.com   â†’ minio:9001  
    
  SSL: Let's Encrypt (Cloudflare DNS challenge)

â–¶ PostgreSQL Initialization:  
  âœ“ config/postgres/init.sql  
    
  Databases to be created:  
    â€¢ litellm\_db   (for LiteLLM request logging)  
    â€¢ n8n\_db       (for n8n workflows)  
    â€¢ dify\_db      (for Dify apps)  
    â€¢ flowise\_db   (for Flowise chatflows)  
    
  Extensions:  
    â€¢ uuid-ossp (UUID generation)  
    â€¢ pgcrypto (encryption)  
    â€¢ pg\_trgm (fuzzy search)

â–¶ Prometheus Monitoring:  
  âœ“ config/prometheus/prometheus.yml  
    
  Scrape targets:  
    â€¢ postgres:5432/metrics  
    â€¢ redis:6379/metrics  
    â€¢ ollama:11434/metrics  
    â€¢ litellm:4000/metrics  
    â€¢ caddy:2019/metrics

â–¶ Grafana Dashboards:  
  âœ“ config/grafana/dashboards/ai-platform.json  
  âœ“ config/grafana/dashboards/llm-performance.json  
  âœ“ config/grafana/dashboards/infrastructure.json

All config files created in: /mnt/data/config/

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘ PHASE 25: Final Validation                                        â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¶ Validating all generated files...

Metadata files:  
  âœ“ configuration.json (valid JSON, 847 lines)  
  âœ“ selected\_services.json (valid JSON, 134 lines)  
  âœ“ deployment\_plan.json (valid JSON, 89 lines)  
  âœ“ secrets.json (valid JSON, encrypted, 256 lines)  
  âœ“ litellm\_config.json (valid JSON, 178 lines)

Compose files (18 files):  
  âœ“ All YAML syntax valid  
  âœ“ All images specified  
  âœ“ All networks referenced exist  
  âœ“ All volumes defined  
  âœ“ All env\_file paths correct

Environment files (15 files):  
  âœ“ All required variables present  
  âœ“ No syntax errors  
  âœ“ Permissions: 600

Configuration files:  
  âœ“ LiteLLM config valid  
  âœ“ Caddyfile syntax valid  
  âœ“ PostgreSQL init.sql valid  
  âœ“ Prometheus config valid  
  âœ“ Grafana dashboards valid JSON

Dependencies:  
  âœ“ Docker installed  
  âœ“ Docker Compose available  
  âœ“ Networks created  
  âœ“ Directories exist with correct permissions

âœ“ All validation checks passed\!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â•‘                                                                    â•‘  
â•‘          âœ“ CONFIGURATION COLLECTION COMPLETED\!                     â•‘  
â•‘                                                                    â•‘  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ Setup Summary  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

System Configuration:  
  â€¢ Hardware: GPU-Accelerated (NVIDIA RTX 3090\)  
  â€¢ OS: Ubuntu 22.04 x86\_64  
  â€¢ Resources: 8 cores, 32GB RAM, 250GB disk  
  â€¢ Docker: 25.0.3 with GPU support

Network Configuration:  
  â€¢ Domain: ai.mycompany.com  
  â€¢ Proxy: Caddy (automatic HTTPS)  
  â€¢ SSL: Let's Encrypt (Cloudflare DNS)  
  â€¢ Networks: 3 isolated networks

Services Selected (15 total):  
  Core Infrastructure:  
    âœ“ PostgreSQL 16 (multi-database)  
    âœ“ Redis 7 (cache \+ queue)  
    âœ“ Qdrant (vector DB)  
    âœ“ MinIO (object storage)  
    
  AI Services:  
    âœ“ Ollama (3 models, GPU-accelerated)  
    âœ“ LiteLLM (cost-based routing, 3 providers)  
    âœ“ Open WebUI (chat interface)  
    âœ“ Dify (AI app platform)  
    
  Workflows:  
    âœ“ n8n (automation)  
    âœ“ Flowise (no-code AI)  
    
  Integrations:  
    âœ“ OpenClaw (web agent \+ Brave Search)  
    âœ“ Signal API (messaging)  
    âœ“ Google Drive (auto-sync)  
    âœ“ Tailscale (VPN)  
    
  Monitoring:  
    âœ“ Prometheus (metrics)  
    âœ“ Grafana (dashboards)  
    âœ“ Loki (logs)

External Integrations:  
  â€¢ API Keys: 3 providers (OpenAI, Anthropic, Groq)  
  â€¢ Search: Brave Search API  
  â€¢ Storage: Google Drive (service account)  
  â€¢ Network: Tailscale VPN

Generated Files:  
  â€¢ Metadata: 5 files in /mnt/data/metadata/  
  â€¢ Compose: 18 files in /mnt/data/compose/  
  â€¢ Environment: 15 files in /mnt/data/env/  
  â€¢ Configs: 12 files in /mnt/data/config/  
  â€¢ Total: 50 configuration files

Security:  
  âœ“ 12 auto-generated secrets (32-64 chars each)  
  âœ“ All credentials unique per service  
  âœ“ Secrets encrypted at rest  
  âœ“ File permissions: 600 for sensitive files

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸš€ Next Steps:

  1\. BACKUP YOUR SECRETS (CRITICAL):  
     cp /mnt/data/metadata/secrets.json \~/ai-platform-secrets-backup.json  
     chmod 400 \~/ai-platform-secrets-backup.json

  2\. VERIFY DNS CONFIGURATION:  
     Ensure all subdomains point to this server:  
     â€¢ chat.ai.mycompany.com  
     â€¢ api.ai.mycompany.com  
     â€¢ dify.ai.mycompany.com  
     â€¢ workflows.ai.mycompany.com  
     â€¢ flowise.ai.mycompany.com  
     â€¢ browse.ai.mycompany.com  
     â€¢ metrics.ai.mycompany.com  
     â€¢ storage.ai.mycompany.com

  3\. DEPLOY THE PLATFORM:  
     cd \~/AIPlatformAutomation/scripts  
     sudo ./2-deploy-services.sh  
       
     Deployment will:  
     â€¢ Pull all Docker images (\~15GB)  
     â€¢ Download Ollama models (\~19GB)  
     â€¢ Initialize databases  
     â€¢ Start all services  
     â€¢ Run health checks  
     â€¢ Show Signal QR code for registration  
       
     Estimated time: 15-25 minutes

  4\. AFTER DEPLOYMENT:  
     â€¢ Change admin passwords (stored in secrets.json)  
     â€¢ Test all service URLs  
     â€¢ Configure Google Drive sync  
     â€¢ Set up monitoring alerts

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“„ Important Files:  
  â€¢ Setup log: /mnt/data/logs/setup-20260211-143022.log  
  â€¢ Secrets (BACKUP\!): /mnt/data/metadata/secrets.json  
  â€¢ Configuration: /mnt/data/metadata/configuration.json

âš ï¸  CRITICAL REMINDERS:  
  1\. BACKUP secrets.json IMMEDIATELY \- contains all passwords  
  2\. Configure DNS before running Script 2  
  3\. This script did NOT deploy anything \- only prepared configuration  
  4\. All deployment happens in Script 2

âœ“ Setup completed successfully\!  
  You may now run: sudo ./2-deploy-services.sh

\`Script 2 Expected output :\` 

\#\# \*\*SCRIPT 2: DEPLOY SERVICES\*\*

\#\#\# \*\*Intent\*\*

Pull Docker images, deploy containers with proper network configuration, perform health checks, and deliver a working system.

\#\#\# \*\*Key Responsibilities\*\*

\#\#\#\# \*\*1\\. Environment Validation\*\*

\* Source \`.env\` file    
\* Validate NO ANSI codes present    
\* Validate NO shell injection patterns    
\* Confirm all required variables set

\#\#\#\# \*\*2\\. Pre-Deployment Cleanup\*\*

\* Remove any existing containers with same names    
\* Preserve volumes (data persistence)    
\* Clean stale network connections

\#\#\#\# \*\*3\\. Docker Network Creation\*\*

docker network create ai-network 2\\\>/dev/null || true

\#\#\#\# \*\*4\\. Service Deployment (Dependency Order)\*\*

\*\*Phase 1: Core Infrastructure\*\*

\\\# 1\\. Ollama (LLM backend)    
docker run \\-d \\--name ollama \\\\    
  \\--network ai-network \\\\    
  \\-p ${OLLAMA\\\_PORT}:11434 \\\\    
  \\-v ollama-data:/root/.ollama \\\\    
  \\--gpus all \\\\  \\\# If GPU detected    
  \\--restart unless-stopped \\\\    
  ollama/ollama:latest

\\\# 2\\. Qdrant (Vector DB)    
docker run \\-d \\--name qdrant \\\\    
  \\--network ai-network \\\\    
  \\-p ${QDRANT\\\_PORT}:6333 \\\\    
  \\-v qdrant-data:/qdrant/storage \\\\    
  \\-e QDRANT\\\_\\\_SERVICE\\\_\\\_API\\\_KEY=${QDRANT\\\_API\\\_KEY} \\\\    
  \\--restart unless-stopped \\\\    
  qdrant/qdrant:latest

\*\*Phase 2: Search Infrastructure\*\*

\\\# 3\\. SearXNG (if selected)    
docker run \\-d \\--name searxng \\\\    
  \\--network ai-network \\\\    
  \\-p ${SEARXNG\\\_PORT}:8080 \\\\    
  \\-v searxng-data:/etc/searxng \\\\    
  \\-e SEARXNG\\\_SECRET=${SEARXNG\\\_SECRET\\\_KEY} \\\\    
  \\--restart unless-stopped \\\\    
  searxng/searxng:latest

\*\*Phase 3: LLM Gateway\*\*

\\\# 4\\. LiteLLM (Unified API)    
docker run \\-d \\--name litellm \\\\    
  \\--network ai-network \\\\    
  \\-p ${LITELLM\\\_PORT}:4000 \\\\    
  \\-v litellm-data:/app/config \\\\    
  \\-e OLLAMA\\\_BASE\\\_URL=http://ollama:11434 \\\\    
  \\-e LITELLM\\\_MASTER\\\_KEY=${LITELLM\\\_MASTER\\\_KEY} \\\\    
  \\-e OPENAI\\\_API\\\_KEY=${OPENAI\\\_API\\\_KEY:-} \\\\    
  \\-e ANTHROPIC\\\_API\\\_KEY=${ANTHROPIC\\\_API\\\_KEY:-} \\\\    
  \\--restart unless-stopped \\\\    
  ghcr.io/berriai/litellm:main-latest

\*\*Phase 4: AI Platforms\*\*

\\\# 5\\. Ollama WebUI    
docker run \\-d \\--name ollama-webui \\\\    
  \\--network ai-network \\\\    
  \\-p ${OLLAMA\\\_WEBUI\\\_PORT}:8080 \\\\    
  \\-v ollama-webui-data:/app/backend/data \\\\    
  \\-e OLLAMA\\\_BASE\\\_URL=http://ollama:11434 \\\\    
  \\--restart unless-stopped \\\\    
  ghcr.io/open-webui/open-webui:main

\\\# 6\\. AnythingLLM    
docker run \\-d \\--name anythingllm \\\\    
  \\--network ai-network \\\\    
  \\-p ${ANYTHINGLLM\\\_PORT}:3001 \\\\    
  \\-v anythingllm-data:/app/server/storage \\\\    
  \\-v /mnt/data/gdrive:/app/collector/hotdir \\\\    
  \\-e LLM\\\_PROVIDER=ollama \\\\    
  \\-e OLLAMA\\\_BASE\\\_PATH=http://ollama:11434 \\\\    
  \\-e VECTOR\\\_DB=qdrant \\\\    
  \\-e QDRANT\\\_ENDPOINT=http://qdrant:6333 \\\\    
  \\-e QDRANT\\\_API\\\_KEY=${QDRANT\\\_API\\\_KEY} \\\\    
  \\--restart unless-stopped \\\\    
  mintplexlabs/anythingllm:latest

\\\# 7\\. Dify    
docker run \\-d \\--name dify \\\\    
  \\--network ai-network \\\\    
  \\-p ${DIFY\\\_PORT}:3000 \\\\    
  \\-v dify-data:/app/storage \\\\    
  \\-e LLM\\\_PROVIDER=openai \\\\    
  \\-e OPENAI\\\_API\\\_BASE=http://litellm:4000 \\\\    
  \\-e OPENAI\\\_API\\\_KEY=${LITELLM\\\_MASTER\\\_KEY} \\\\    
  \\--restart unless-stopped \\\\    
  langgenius/dify-api:latest

\*\*Phase 5: Automation\*\*

\\\# 8\\. OpenClaw    
docker run \\-d \\--name open-claw \\\\    
  \\--network ai-network \\\\    
  \\-p ${OPENCLAW\\\_PORT}:3000 \\\\    
  \\-v openclaw-data:/app/data \\\\    
  \\-e ANYTHINGLLM\\\_API\\\_BASE=http://anythingllm:3001 \\\\    
  \\--restart unless-stopped \\\\    
  openclawai/openclaw:latest

\\\# 9\\. n8n (if selected)    
docker run \\-d \\--name n8n \\\\    
  \\--network ai-network \\\\    
  \\-p ${N8N\\\_PORT}:5678 \\\\    
  \\-v n8n-data:/home/node/.n8n \\\\    
  \\--restart unless-stopped \\\\    
  n8nio/n8n:latest

\*\*Phase 6: Reverse Proxy\*\*

\\\# 10a. Nginx (if selected)    
docker run \\-d \\--name nginx \\\\    
  \\--network ai-network \\\\    
  \\-p ${NGINX\\\_HTTP\\\_PORT}:80 \\\\    
  \\-p ${NGINX\\\_HTTPS\\\_PORT}:443 \\\\    
  \\-v /mnt/data/ai-services/config/nginx/conf.d:/etc/nginx/conf.d:ro \\\\    
  \\-v /mnt/data/ai-services/config/nginx/ssl:/etc/nginx/ssl:ro \\\\    
  \\-v nginx-cache:/var/cache/nginx \\\\    
  \\--restart unless-stopped \\\\    
  nginx:alpine

\\\# 10b. Caddy (if selected \\- MUTUALLY EXCLUSIVE)    
docker run \\-d \\--name caddy \\\\    
  \\--network ai-network \\\\    
  \\-p ${CADDY\\\_HTTP\\\_PORT}:80 \\\\    
  \\-p ${CADDY\\\_HTTPS\\\_PORT}:443 \\\\    
  \\-v /mnt/data/ai-services/config/caddy/Caddyfile:/etc/caddy/Caddyfile:ro \\\\    
  \\-v caddy-data:/data \\\\    
  \\-v caddy-config:/config \\\\    
  \\--restart unless-stopped \\\\    
  caddy:latest

\*\*Phase 7: VPN\*\*

\\\# 11\\. Tailscale    
docker run \\-d \\--name tailscale \\\\    
  \\--network host \\\\    
  \\--cap-add NET\\\_ADMIN \\\\    
  \\--cap-add SYS\\\_MODULE \\\\    
  \\-v /dev/net/tun:/dev/net/tun \\\\    
  \\-v tailscale-data:/var/lib/tailscale \\\\    
  \\-e TS\\\_AUTHKEY=${TAILSCALE\\\_AUTH\\\_KEY} \\\\    
  \\-e TS\\\_STATE\\\_DIR=/var/lib/tailscale \\\\    
  \\--restart unless-stopped \\\\    
  tailscale/tailscale:latest

\*\*Phase 8: Data Sync\*\*

\\\# 12\\. Rsync (Cron-based Google Drive sync)    
\\\# Create systemd timer or cron job:    
\\\# \\\*/6 \\\* \\\* \\\* \\\* rsync \\-avz /path/to/gdrive/ /mnt/data/gdrive/

\#\#\#\# \*\*5\\. Health Checks (Per Service)\*\*

check\\\_service\\\_health() {    
    local service=$1    
    local port=$2    
    local max\\\_attempts=30    
        
    for i in $(seq 1 $max\\\_attempts); do    
        if curl \\-sf "http://localhost:${port}/health" \\\>/dev/null 2\\\>&1; then    
            log\\\_success "$service healthy on port $port"    
            return 0    
        fi    
        sleep 2    
    done    
        
    log\\\_error "$service failed health check"    
    docker logs "$service" | tail \\-20    
    return 1    
}

\\\# Execute health checks    
check\\\_service\\\_health "ollama" "$OLLAMA\\\_PORT"    
check\\\_service\\\_health "litellm" "$LITELLM\\\_PORT"    
check\\\_service\\\_health "qdrant" "$QDRANT\\\_PORT"    
\\\# ... etc

\#\#\#\# \*\*6\\. Credentials File Update\*\*

cat \\\>\\\> /mnt/data/ai-services/credentials.txt \\\<\\\<EOF    
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—    
â•‘     AI PLATFORM ACCESS CREDENTIALS     â•‘    
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Generated: $(date)

OLLAMA:    
  URL: http://localhost:${OLLAMA\\\_PORT}    
  API: http://localhost:${OLLAMA\\\_PORT}/api

OLLAMA WEBUI:    
  URL: http://localhost:${OLLAMA\\\_WEBUI\\\_PORT}    
  Default Admin: Create on first access

LITELLM:    
  URL: http://localhost:${LITELLM\\\_PORT}    
  API Key: ${LITELLM\\\_MASTER\\\_KEY}    
  Docs: http://localhost:${LITELLM\\\_PORT}/docs

QDRANT:    
  URL: http://localhost:${QDRANT\\\_PORT}    
  API Key: ${QDRANT\\\_API\\\_KEY}    
  Dashboard: http://localhost:${QDRANT\\\_PORT}/dashboard

ANYTHINGLLM:    
  URL: http://localhost:${ANYTHINGLLM\\\_PORT}    
  OR: https://${DOMAIN}/anythingllm

DIFY:    
  URL: http://localhost:${DIFY\\\_PORT}    
  OR: https://${DOMAIN}/dify

OPENCLAW (via Tailscale):    
  URL: https://$(tailscale status \\--json | jq \\-r '.Self.DNSName'):18789

TAILSCALE:    
  Admin: https://login.tailscale.com/admin/machines    
  This Device IP: $(tailscale ip \\-4)

REVERSE PROXY:    
  Public URL: https://${DOMAIN}    
  Backend: ${PROXY\\\_TYPE} (Nginx/Caddy)

EOF

\#\#\#\# \*\*7\\. Deployment Summary\*\*

echo ""    
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"    
echo "â•‘   DEPLOYMENT COMPLETE v68.0.0          â•‘"    
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"    
echo ""    
echo "âœ“ All containers running"    
echo "âœ“ Health checks passed"    
echo "âœ“ Credentials saved to /mnt/data/ai-services/credentials.txt"    
echo ""    
echo "Next steps:"    
echo "  1\\. Run: 3-configure-services.sh"    
echo "  2\\. Access services at: https://${DOMAIN}"    
echo ""

\#\#\# \*\*Success Definition\*\*

\* âœ… All selected containers running (\`docker ps\` shows all)    
\* âœ… All services pass health checks    
\* âœ… Reverse proxy routing works (test \`curl https://${DOMAIN}/anythingllm\`)    
\* âœ… Inter-service communication works (Dify can reach LiteLLM)    
\* âœ… Credentials file populated    
\* âœ… Tailscale connected (if enabled)    
\* âœ… No port conflicts    
\* âœ… All containers have \`restart: unless-stopped\`

\#\# \*\*SCRIPT 3: CONFIGURE SERVICES\*\*

\#\#\# \*\*Intent\*\*

Fine-tune service-specific settings, load initial models, configure routing rules, enable systemd persistence, and link external integrations (Signal, Google Drive sync).

\#\#\# \*\*Key Responsibilities\*\*

\#\#\#\# \*\*1\\. Service Status Check\*\*

display\\\_service\\\_status() {    
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"    
    echo "â•‘        SERVICE STATUS OVERVIEW         â•‘"    
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"    
        
    for service in ollama litellm qdrant anythingllm dify openclaw; do    
        if docker ps \\--format '{{.Names}}' | grep \\-q "^${service}$"; then    
            status="âœ“ Running"    
            color="${GREEN}"    
        else    
            status="âœ— Stopped"    
            color="${RED}"    
        fi    
        printf "%-20s %s\\\\n" "$service" "${color}${status}${NC}"    
    done    
}

\#\#\#\# \*\*2\\. Ollama Model Management\*\*

configure\\\_ollama\\\_models() {    
    echo ""    
    log "Ollama Model Configuration"    
    echo "  Current models:"    
    docker exec ollama ollama list    
        
    echo ""    
    echo "Available models:"    
    echo "  1\\) llama3.2:1b (Fast, minimal)"    
    echo "  2\\) llama3.2:3b (Balanced \\- RECOMMENDED)"    
    echo "  3\\) mistral:7b (High quality)"    
    echo "  4\\) deepseek-coder:6.7b (Code-focused)"    
    echo "  5\\) Custom model name"    
        
    read \\-p "Select model to pull \\\[2\\\]: " model\\\_choice    
        
    case "${model\\\_choice:-2}" in    
        1\\) MODEL="llama3.2:1b" ;;    
        2\\) MODEL="llama3.2:3b" ;;    
        3\\) MODEL="mistral:7b" ;;    
        4\\) MODEL="deepseek-coder:6.7b" ;;    
        5\\) read \\-p "Enter model name: " MODEL ;;    
    esac    
        
    log "Pulling model: $MODEL (this may take several minutes)"    
    docker exec ollama ollama pull "$MODEL"    
        
    \\\# Set as default in LiteLLM routing    
    update\\\_litellm\\\_config "default\\\_model" "$MODEL"    
}

\#\#\#\# \*\*3\\. LiteLLM Routing Configuration\*\*

configure\\\_litellm\\\_routing() {    
    echo ""    
    log "LiteLLM Routing Rules Configuration"    
        
    cat \\\> /mnt/data/ai-services/config/litellm/config.yaml \\\<\\\<EOF    
model\\\_list:    
  \\\# Local Ollama models    
  \\- model\\\_name: local-llm    
    litellm\\\_params:    
      model: ollama/${OLLAMA\\\_MODEL:-llama3.2:3b}    
      api\\\_base: http://ollama:11434    
          
  \\\# Cloud fallback models    
  \\- model\\\_name: gpt-4-turbo    
    litellm\\\_params:    
      model: gpt-4-turbo-preview    
      api\\\_key: ${OPENAI\\\_API\\\_KEY:-}    
          
  \\- model\\\_name: claude-3-opus    
    litellm\\\_params:    
      model: claude-3-opus-20240229    
      api\\\_key: ${ANTHROPIC\\\_API\\\_KEY:-}

router\\\_settings:    
  routing\\\_strategy: usage-based-routing    
      
  \\\# Route simple queries to local, complex to cloud    
  model\\\_routing:    
    \\- pattern: ".\\\*simple.\\\*|.\\\*quick.\\\*|.\\\*basic.\\\*"    
      target: local-llm    
          
    \\- pattern: ".\\\*complex.\\\*|.\\\*analysis.\\\*|.\\\*research.\\\*"    
      target: gpt-4-turbo    
      fallback: local-llm    
          
    \\- pattern: ".\\\*code.\\\*|.\\\*programming.\\\*"    
      target: ${CODE\\\_MODEL:-local-llm}    
          
  \\\# Cost limits    
  max\\\_budget: 100  \\\# USD per month    
  budget\\\_duration: 30d    
EOF

    docker restart litellm    
    log\\\_success "LiteLLM routing configured"    
}

\#\#\#\# \*\*4\\. LiteLLM Web UI (Expose Configuration Interface)\*\*

\\\# Deploy optional LiteLLM admin UI    
deploy\\\_litellm\\\_ui() {    
    read \\-p "Deploy LiteLLM Admin UI? \\\[Y/n\\\]: " \\-n 1 \\-r    
    echo    
        
    if \\\[\\\[ \\\! $REPLY \\=\\\~ ^\\\[Nn\\\]$ \\\]\\\]; then    
        LITELLM\\\_UI\\\_PORT=$(find\\\_available\\\_port "litellm-ui" 4001\\)    
            
        docker run \\-d \\--name litellm-ui \\\\    
          \\--network ai-network \\\\    
          \\-p ${LITELLM\\\_UI\\\_PORT}:3000 \\\\    
          \\-e LITELLM\\\_API\\\_BASE=http://litellm:4000 \\\\    
          \\-e LITELLM\\\_API\\\_KEY=${LITELLM\\\_MASTER\\\_KEY} \\\\    
          \\--restart unless-stopped \\\\    
          ghcr.io/berriai/litellm-ui:latest    
              
        log\\\_success "LiteLLM UI: http://localhost:${LITELLM\\\_UI\\\_PORT}"    
    fi    
}

\#\#\#\# \*\*5\\. AnythingLLM Configuration\*\*

configure\\\_anythingllm() {    
    log "Configuring AnythingLLM..."    
        
    \\\# Connect to Qdrant vector DB    
    docker exec anythingllm curl \\-X POST http://localhost:3001/api/system/vector-db \\\\    
      \\-H "Content-Type: application/json" \\\\    
      \\-d '{    
        "provider": "qdrant",    
        "config": {    
          "url": "http://qdrant:6333",    
          "apiKey": "'"${QDRANT\\\_API\\\_KEY}"'"    
        }    
      }'    
        
    \\\# Set Ollama as LLM provider    
    docker exec anythingllm curl \\-X POST http://localhost:3001/api/system/llm \\\\    
      \\-H "Content-Type: application/json" \\\\    
      \\-d '{    
        "provider": "ollama",    
        "config": {    
          "baseUrl": "http://ollama:11434",    
          "model": "'"${OLLAMA\\\_MODEL}"'"    
        }    
      }'    
        
    \\\# Configure document ingestion from /mnt/data/gdrive    
    docker exec anythingllm curl \\-X POST http://localhost:3001/api/system/data-connectors \\\\    
      \\-H "Content-Type: application/json" \\\\    
      \\-d '{    
        "type": "local\\\_files",    
        "path": "/app/collector/hotdir"    
      }'    
        
    log\\\_success "AnythingLLM configured with Qdrant \\+ Ollama"    
}

\#\#\#\# \*\*6\\. OpenClaw Configuration\*\*

configure\\\_openclaw() {    
    log "Configuring OpenClaw..."    
        
    \\\# Link to AnythingLLM for knowledge retrieval    
    docker exec open-claw sh \\-c 'cat \\\> /app/config.json' \\\<\\\<EOF    
{    
  "llm": {    
    "provider": "anythingllm",    
    "endpoint": "http://anythingllm:3001/api/chat",    
    "defaultModel": "${OLLAMA\\\_MODEL}"    
  },    
  "vectorDB": {    
    "provider": "qdrant",    
    "endpoint": "http://qdrant:6333",    
    "apiKey": "${QDRANT\\\_API\\\_KEY}",    
    "collection": "openclaw-knowledge"    
  },    
  "automation": {    
    "screenshotPath": "/app/data/screenshots",    
    "maxRetries": 3    
  }    
}    
EOF    
        
    docker restart open-claw    
    log\\\_success "OpenClaw linked to AnythingLLM \\+ Qdrant"    
}

\#\#\#\# \*\*7\\. OpenClaw via Tailscale HTTPS\*\*

configure\\\_openclaw\\\_tailscale() {    
    log "Configuring OpenClaw access via Tailscale..."    
        
    \\\# Get Tailscale IP    
    TAILSCALE\\\_IP=$(docker exec tailscale tailscale ip \\-4)    
        
    \\\# Update Caddy/Nginx to serve OpenClaw on Tailscale interface    
    if \\\[\\\[ "$PROXY\\\_TYPE" \\== "caddy" \\\]\\\]; then    
        cat \\\>\\\> /mnt/data/ai-services/config/caddy/Caddyfile \\\<\\\<EOF

\\\# OpenClaw via Tailscale HTTPS    
https://${TAILSCALE\\\_IP}:8443 {    
    reverse\\\_proxy open-claw:3000    
    tls internal    
}    
EOF    
        docker exec caddy caddy reload \\--config /etc/caddy/Caddyfile    
    fi    
        
    log\\\_success "OpenClaw accessible at: https://${TAILSCALE\\\_IP}:8443"    
    log "  OR: https://$(docker exec tailscale tailscale status \\--json | jq \\-r '.Self.DNSName'):18789"    
}

\#\#\#\# \*\*8\\. Dify Configuration\*\*

configure\\\_dify() {    
    log "Configuring Dify..."    
        
    \\\# Point Dify to LiteLLM for intelligent routing    
    docker exec dify sh \\-c 'cat \\\> /app/.env' \\\<\\\<EOF    
LLM\\\_PROVIDER=openai    
OPENAI\\\_API\\\_BASE=http://litellm:4000    
OPENAI\\\_API\\\_KEY=${LITELLM\\\_MASTER\\\_KEY}    
VECTOR\\\_STORE=qdrant    
QDRANT\\\_URL=http://qdrant:6333    
QDRANT\\\_API\\\_KEY=${QDRANT\\\_API\\\_KEY}    
EOF    
        
    docker restart dify    
    log\\\_success "Dify configured with LiteLLM \\+ Qdrant"    
}

\#\#\#\# \*\*9\\. Signal Bot Configuration (Optional)\*\*

configure\\\_signal\\\_bot() {    
    read \\-p "Link Signal messaging bot? \\\[y/N\\\]: " \\-n 1 \\-r    
    echo    
        
    if \\\[\\\[ $REPLY \\=\\\~ ^\\\[Yy\\\]$ \\\]\\\]; then    
        log "Signal bot requires:"    
        log "  1\\. Signal CLI installed: https://github.com/AsamK/signal-cli"    
        log "  2\\. Phone number registered"    
            
        read \\-p "Enter Signal phone number (with country code): " SIGNAL\\\_PHONE    
            
        \\\# Link device    
        docker run \\--rm \\-it \\\\    
          \\-v signal-data:/root/.local/share/signal-cli \\\\    
          bbernhard/signal-cli:latest \\\\    
          \\-u "$SIGNAL\\\_PHONE" link    
            
        log "Scan QR code with Signal app (Settings â†’ Linked Devices)"    
            
        \\\# Deploy Signal bridge    
        docker run \\-d \\--name signal-bridge \\\\    
          \\--network ai-network \\\\    
          \\-v signal-data:/root/.local/share/signal-cli \\\\    
          \\-e SIGNAL\\\_PHONE="$SIGNAL\\\_PHONE" \\\\    
          \\-e OPENCLAW\\\_ENDPOINT="http://open-claw:3000" \\\\    
          \\--restart unless-stopped \\\\    
          custom/signal-openclaw-bridge:latest    
            
        log\\\_success "Signal bot linked \\- messages route to OpenClaw"    
    fi    
}

\#\#\#\# \*\*10\\. Google Drive Rsync Configuration\*\*

configure\\\_gdrive\\\_sync() {    
    log "Configuring Google Drive sync..."    
        
    read \\-p "Enable Google Drive sync to /mnt/data/gdrive? \\\[Y/n\\\]: " \\-n 1 \\-r    
    echo    
        
    if \\\[\\\[ \\\! $REPLY \\=\\\~ ^\\\[Nn\\\]$ \\\]\\\]; then    
        read \\-p "Enter Google Drive source path (rclone remote:path): " GDRIVE\\\_SOURCE    
        read \\-p "Sync interval (hours) \\\[6\\\]: " SYNC\\\_INTERVAL    
        SYNC\\\_INTERVAL=${SYNC\\\_INTERVAL:-6}    
            
        \\\# Create systemd timer    
        cat \\\> /etc/systemd/system/gdrive-sync.service \\\<\\\<EOF    
\\\[Unit\\\]    
Description=Google Drive Sync to /mnt/data/gdrive    
After=network.target

\\\[Service\\\]    
Type=oneshot    
ExecStart=/usr/bin/rclone sync ${GDRIVE\\\_SOURCE} /mnt/data/gdrive/ \\-v \\--log-file=/var/log/gdrive-sync.log    
User=root

\\\[Install\\\]    
WantedBy=multi-user.target    
EOF

        cat \\\> /etc/systemd/system/gdrive-sync.timer \\\<\\\<EOF    
\\\[Unit\\\]    
Description=Google Drive Sync Timer

\\\[Timer\\\]    
OnBootSec=5min    
OnUnitActiveSec=${SYNC\\\_INTERVAL}h    
Persistent=true

\\\[Install\\\]    
WantedBy=timers.target    
EOF

        systemctl daemon-reload    
        systemctl enable \\--now gdrive-sync.timer    
            
        log\\\_success "Google Drive sync enabled (every ${SYNC\\\_INTERVAL}h)"    
        log "  Source: ${GDRIVE\\\_SOURCE}"    
        log "  Target: /mnt/data/gdrive/"    
    fi    
}

\#\#\#\# \*\*11\\. Port Reconfiguration\*\*

reconfigure\\\_port() {    
    local service=$1    
    local current\\\_port=$2    
        
    read \\-p "Change port for $service (current: $current\\\_port)? \\\[y/N\\\]: " \\-n 1 \\-r    
    echo    
        
    if \\\[\\\[ $REPLY \\=\\\~ ^\\\[Yy\\\]$ \\\]\\\]; then    
        read \\-p "Enter new port: " new\\\_port    
            
        \\\# Validate and stop service    
        docker stop "$service"    
        docker rm "$service"    
            
        \\\# Update ENV    
        sed \\-i "s/^${service^^}\\\_PORT=.\\\*/${service^^}\\\_PORT=$new\\\_port/" "$ENV\\\_FILE"    
            
        \\\# Re-deploy with new port    
        source "$ENV\\\_FILE"    
        deploy\\\_${service}    
            
        log\\\_success "$service moved to port $new\\\_port"    
    fi    
}

\#\#\#\# \*\*12\\. System Integration (Systemd)\*\*

make\\\_persistent() {    
    log "Creating systemd service for container auto-start..."    
        
    \\\# Docker containers already have \\--restart unless-stopped    
    \\\# but ensure Docker starts on boot    
    systemctl enable docker    
        
    \\\# Optionally create a wrapper service    
    cat \\\> /etc/systemd/system/ai-platform.service \\\<\\\<'EOF'    
\\\[Unit\\\]    
Description=AI Platform Container Stack    
After=docker.service    
Requires=docker.service

\\\[Service\\\]    
Type=oneshot    
RemainAfterExit=yes    
ExecStart=/usr/bin/docker start ollama litellm qdrant anythingllm dify openclaw caddy tailscale    
ExecStop=/usr/bin/docker stop ollama litellm qdrant anythingllm dify openclaw caddy tailscale    
User=root

\\\[Install\\\]    
WantedBy=multi-user.target    
EOF

    systemctl daemon-reload    
    systemctl enable ai-platform.service    
        
    log\\\_success "AI Platform will auto-start on boot"    
}

\#\#\#\# \*\*13\\. Configuration Menu\*\*

show\\\_configuration\\\_menu() {    
    while true; do    
        echo ""    
        echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"    
        echo "â•‘    SERVICE CONFIGURATION MENU          â•‘"    
        echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"    
        echo ""    
        echo "  1\\) View service status"    
        echo "  2\\) Configure Ollama models"    
        echo "  3\\) Configure LiteLLM routing"    
        echo "  4\\) Deploy LiteLLM Web UI"    
        echo "  5\\) Configure AnythingLLM"    
        echo "  6\\) Configure OpenClaw"    
        echo "  7\\) Configure OpenClaw via Tailscale"    
        echo "  8\\) Configure Dify"    
        echo "  9\\) Configure Signal bot"    
        echo " 10\\) Configure Google Drive sync"    
        echo " 11\\) Reconfigure service ports"    
        echo " 12\\) Make configuration permanent (systemd)"    
        echo " 13\\) Restart all services"    
        echo "  0\\) Exit"    
        echo ""    
        read \\-p "Select option: " choice    
            
        case $choice in    
            1\\) display\\\_service\\\_status ;;    
            2\\) configure\\\_ollama\\\_models ;;    
            3\\) configure\\\_litellm\\\_routing ;;    
            4\\) deploy\\\_litellm\\\_ui ;;    
            5\\) configure\\\_anythingllm ;;    
            6\\) configure\\\_openclaw ;;    
            7\\) configure\\\_openclaw\\\_tailscale ;;    
            8\\) configure\\\_dify ;;    
            9\\) configure\\\_signal\\\_bot ;;    
            10\\) configure\\\_gdrive\\\_sync ;;    
            11\\) reconfigure\\\_ports\\\_menu ;;    
            12\\) make\\\_persistent ;;    
            13\\) restart\\\_all\\\_services ;;    
            0\\) break ;;    
            \\\*) log\\\_error "Invalid option" ;;    
        esac    
    done    
}

\#\#\# \*\*Success Definition\*\*

\* âœ… Ollama model pulled and active    
\* âœ… LiteLLM routing rules configured (local-first, cloud fallback)    
\* âœ… AnythingLLM connected to Qdrant \\+ Ollama    
\* âœ… OpenClaw linked to AnythingLLM vector DB    
\* âœ… OpenClaw accessible via Tailscale HTTPS    
\* âœ… Dify using LiteLLM for intelligent routing    
\* âœ… Google Drive sync active (if enabled)    
\* âœ… Signal bot linked (if enabled)    
\* âœ… All services set to auto-start on boot    
\* âœ… User can modify configuration via menu (no manual file editing)

\#\# \*\*SCRIPT 4: ADD SERVICE (FUTURE EXTENSIBILITY)\*\*

\#\#\# \*\*Intent\*\*

Provide a framework to add new Docker-based AI services without modifying core scripts.

\#\#\# \*\*Template Structure\*\*

add\\\_new\\\_service() {    
    local service\\\_name=$1    
        
    echo "Adding new service: $service\\\_name"    
        
    \\\# 1\\. Collect configuration    
    read \\-p "Docker image: " image    
    read \\-p "Port: " port    
        
    \\\# 2\\. Validate port availability    
    if \\\! is\\\_port\\\_available "$port"; then    
        log\\\_error "Port $port already in use"    
        return 1    
    fi    
        
    \\\# 3\\. Deploy container    
    docker run \\-d \\\\    
      \\--name "$service\\\_name" \\\\    
      \\--network ai-network \\\\    
      \\-p "${port}:${port}" \\\\    
      \\-v "${service\\\_name}-data:/data" \\\\    
      \\--restart unless-stopped \\\\    
      "$image"    
        
    \\\# 4\\. Update ENV    
    echo "${service\\\_name^^}\\\_PORT=$port" \\\>\\\> "$ENV\\\_FILE"    
        
    \\\# 5\\. Update credentials file    
    cat \\\>\\\> "$CREDS\\\_FILE" \\\<\\\<EOF

${service\\\_name^^}:    
  URL: http://localhost:${port}    
  Added: $(date)    
EOF    
        
    \\\# 6\\. Add to reverse proxy    
    update\\\_reverse\\\_proxy\\\_config "$service\\\_name" "$port"    
        
    log\\\_success "$service\\\_name deployed on port $port"    
}

\#\#\# \*\*Success Definition\*\*

\* âœ… New service deployed without editing Scripts 1-3    
\* âœ… Port automatically allocated    
\* âœ… Service added to reverse proxy routing    
\* âœ… ENV file updated    
\* âœ… Credentials documented

\#\# \*\*KEY ARCHITECTURE DEFINITIONS\*\*

\#\#\# \*\*Primary Access Flows\*\*

\#\#\#\# \*\*1\\. Public Access (via Reverse Proxy)\*\*

User â†’ https://ai.example.com/anythingllm â†’ Caddy â†’ AnythingLLM:3001    
User â†’ https://ai.example.com/dify â†’ Caddy â†’ Dify:3000

\#\#\#\# \*\*2\\. Tailscale Private Access (OpenClaw)\*\*

User â†’ https://tailscale-ip:18789 â†’ Tailscale â†’ OpenClaw:18789

\#\#\#\# \*\*3\\. LLM Request Routing\*\*

Dify â†’ LiteLLM:4000 â†’ \\\[Simple query\\\] â†’ Ollama:11434 â†’ llama3.2:3b    
                    â†’ \\\[Complex query\\\] â†’ OpenAI API â†’ gpt-4-turbo

\#\#\#\# \*\*4\\. Vector DB Flow\*\*

AnythingLLM â†’ Qdrant:6333 (embeddings storage)    
OpenClaw â†’ Qdrant:6333 (knowledge retrieval)    
Dify â†’ Qdrant:6333 (RAG queries)

\#\#\#\# \*\*5\\. Data Sync Flow\*\*

Google Drive â†’ rclone (systemd timer) â†’ /mnt/data/gdrive/ â†’ AnythingLLM ingestion

\---

\#\# \*\*TECHNOLOGY STACK SUMMARY\*\*

| Component | Technology | Purpose | Port |  
| \----- | \----- | \----- | \----- |  
| \*\*LLM Backend\*\* | Ollama | Local inference | 11434 |  
| \*\*Tailscale auth\*\* | tailscale | Retrieve tailscale ip | na |  
| \*\*LLM Gateway\*\* | LiteLLM | Unified API \\+ routing | 4000 |  
| \*\*Vector DB\*\* | Qdrant | Embeddings storage | 6333 |  
| \*\*Document Chat\*\* | AnythingLLM | RAG interface | 3001 |  
| \*\*AI Workflows\*\* | Dify | Visual AI builder | 3000 |  
| \*\*Web Automation\*\* | OpenClaw | Browser automation | 18789 |  
| \*\*Workflow Automation\*\* | n8n | General automation | 5678 |  
| \*\*Reverse Proxy\*\* | Caddy OR Nginx OR traefik | HTTPS termination | 80/443 |  
| \*\*VPN\*\* | Tailscale | Secure remote access | 8443  |  
| \*\*Signal API\*\* | Signal | Pair device for openclaw integration | 8081 |  
| \*\*Data Sync\*\* | Rclone | Google Drive â†’ local | N/A  |  
| \*\*Ai workflows\*\* | Flowise | Visual ai builder | 3000 |  
| \*\*Vector db\*\* | weaviate | Embeddings storage | 50051 |  
| \*\*Vector DB\*\* | Redis | Embeddings storage | 7379 |  
| \*\*Vector db\*\* | Milvus | Embeddings storage | 1953 |  
| \*\*Database\*\* | Postgres | storage | 5432 |  
| \*\*LLM observability\*\* | langfuse | Metrics | 3000 |  
| \*\*LLM observability\*\* | grafana | logging | 3000 |  
| \*\*LLM monitoring\*\* | prometheus | MEtrics | 9090 |  
| \*\*LLM Monitoring\*\* | LOKI=promtai | MEtrics | 3100 |

\#\# \*\*TABLE 1: COMPLETE SERVICE INVENTORY & GAPS\*\* 

In order to initialise step 2 without errors, we reviewed the official documentation for all the stack components and identified additional key variables to generate at step1

| \\\# | Service | Category | Variables Required | File Outputs | Integration Points | Priority |  
| \----- | \----- | \----- | \----- | \----- | \----- | \----- |  
| \*\*REVERSE PROXY\*\* |  |  |  |  |  |  |  
| 1 | \*\*Nginx\*\* | Proxy Option 1 | \`PROXY\_TYPE=nginx\`\\\<br\\\>\`HTTP\_PORT=80\`\\\<br\\\>\`HTTPS\_PORT=443\`\\\<br\\\>\`SSL\_TYPE=letsencrypt/self/none\` | \`compose/nginx.yml\`\\\<br\\\>\`env/nginx.env\`\\\<br\\\>\`config/nginx/nginx.conf\`\\\<br\\\>\`config/nginx/sites/\*.conf\` | All services routed through | ğŸ”´ CRITICAL |  
| 2 | \*\*Traefik\*\* | Proxy Option 2 | \`PROXY\_TYPE=traefik\`\\\<br\\\>\`TRAEFIK\_DASHBOARD=true\`\\\<br\\\>\`TRAEFIK\_API=true\`\\\<br\\\>\`ACME\_EMAIL=\` | \`compose/traefik.yml\`\\\<br\\\>\`env/traefik.env\`\\\<br\\\>\`config/traefik/traefik.yml\`\\\<br\\\>\`config/traefik/dynamic/\*.yml\` | Auto-discovers services via labels | ğŸ”´ CRITICAL |  
| 3 | \*\*Caddy\*\* | Proxy Option 3 | \`PROXY\_TYPE=caddy\`\\\<br\\\>\`CADDY\_AUTO\_HTTPS=true\` | \`compose/caddy.yml\`\\\<br\\\>\`env/caddy.env\`\\\<br\\\>\`config/caddy/Caddyfile\` | Auto HTTPS, simple config | ğŸ”´ CRITICAL |  
| \*\*CORE INFRASTRUCTURE\*\* |  |  |  |  |  |  |  
| 4 | \*\*PostgreSQL\*\* | Database | \`POSTGRES\_VERSION=16-alpine\`\\\<br\\\>\`POSTGRES\_PORT=5432\`\\\<br\\\>Per-service DBs:\\\<br\\\>\`N8N\_DB\`, \`DIFY\_DB\`, \`FLOWISE\_DB\`, \`LITELLM\_DB\`, \`LANGFUSE\_DB\`\\\<br\\\>Each with user/pass | \`compose/postgres.yml\`\\\<br\\\>\`env/postgres.env\`\\\<br\\\>\`config/postgres/init.sql\` | N8N, Dify, Flowise, LiteLLM, Langfuse | ğŸ”´ CRITICAL |  
| 5 | \*\*Redis\*\* | Cache/Queue | \`REDIS\_PORT=6379\`\\\<br\\\>\`REDIS\_PASSWORD=\`\\\<br\\\>\`REDIS\_MAXMEMORY=256mb\`\\\<br\\\>\`REDIS\_POLICY=allkeys-lru\` | \`compose/redis.yml\`\\\<br\\\>\`env/redis.env\`\\\<br\\\>\`config/redis/redis.conf\` | N8N (queue), Dify (cache) | ğŸ”´ CRITICAL |  
| 6 | \*\*Qdrant\*\* | Vector DB | \`QDRANT\_PORT=6333\`\\\<br\\\>\`QDRANT\_GRPC\_PORT=6334\`\\\<br\\\>\`QDRANT\_API\_KEY=\`\\\<br\\\>\`QDRANT\_ALLOW\_ANONYMOUS=false\` | \`compose/qdrant.yml\`\\\<br\\\>\`env/qdrant.env\` | Dify, AnythingLLM, OpenWebUI, Flowise | ğŸ”´ CRITICAL |  
| 7 | \*\*Weaviate\*\* | Vector DB Alt | \`WEAVIATE\_PORT=8080\`\\\<br\\\>\`WEAVIATE\_GRPC\_PORT=50051\`\\\<br\\\>\`AUTHENTICATION\_API\_KEY=\` | \`compose/weaviate.yml\`\\\<br\\\>\`env/weaviate.env\` | Alternative to Qdrant | ğŸŸ¡ HIGH |  
| 8 | \*\*Milvus\*\* | Vector DB Alt | \`MILVUS\_PORT=19530\`\\\<br\\\>\`MILVUS\_USER=\`\\\<br\\\>\`MILVUS\_PASSWORD=\`\\\<br\\\>\`ETCD\_ENDPOINTS=\` | \`compose/milvus.yml\`\\\<br\\\>\`env/milvus.env\`\\\<br\\\>\`config/milvus/milvus.yaml\` | Alternative to Qdrant | ğŸŸ¡ HIGH |  
| \*\*COMMUNICATION & STORAGE\*\* |  |  |  |  |  |  |  
| 9 | \*\*Signal-API\*\* | Messaging | \*\*QR Method:\*\*\\\<br\\\>\`SIGNAL\_NUMBER=+1234567890\`\\\<br\\\>\`SIGNAL\_DEVICE\_NAME=\`\\\<br\\\>\`MODE=native\`\\\<br\\\>\\\<br\\\>\*\*API Method:\*\*\\\<br\\\>\`SIGNAL\_NUMBER=+1234567890\`\\\<br\\\>\`SIGNAL\_CAPTCHA\_TOKEN=\`\\\<br\\\>\`SIGNAL\_VERIFICATION\_CODE=\`\\\<br\\\>\`MODE=json-rpc\` | \`compose/signal-api.yml\`\\\<br\\\>\`env/signal-api.env\` | N8N webhooks, notifications | ğŸ”´ CRITICAL |  
| 10 | \*\*Google Drive\*\* | Storage | \*\*OAuth:\*\*\\\<br\\\>\`GDRIVE\_CLIENT\_ID=\`\\\<br\\\>\`GDRIVE\_CLIENT\_SECRET=\`\\\<br\\\>\`GDRIVE\_REDIRECT\_URI=\`\\\<br\\\>\`GDRIVE\_REFRESH\_TOKEN=\`\\\<br\\\>\\\<br\\\>\*\*Service Account:\*\*\\\<br\\\>\`GDRIVE\_SERVICE\_ACCOUNT\_EMAIL=\`\\\<br\\\>\`GDRIVE\_SERVICE\_ACCOUNT\_KEY=\` (base64)\\\<br\\\>\\\<br\\\>\*\*API Key:\*\*\\\<br\\\>\`GDRIVE\_API\_KEY=\`\\\<br\\\>\`GDRIVE\_FOLDER\_ID=\` | \`compose/gdrive.yml\`\\\<br\\\>\`env/gdrive.env\`\\\<br\\\>\`config/gdrive/credentials.json\` | N8N workflows, Dify uploads | ğŸ”´ CRITICAL |  
| \*\*LLM ENGINES\*\* |  |  |  |  |  |  |  
| 11 | \*\*Ollama\*\* | LLM Runtime | \`OLLAMA\_HOST=0.0.0.0\`\\\<br\\\>\`OLLAMA\_ORIGINS=\*\`\\\<br\\\>\`OLLAMA\_PORT=11434\`\\\<br\\\>\`OLLAMA\_MODELS=\` (comma-separated)\\\<br\\\>\`OLLAMA\_KEEP\_ALIVE=5m\` | \`compose/ollama.yml\`\\\<br\\\>\`env/ollama.env\`\\\<br\\\>\`metadata/ollama\_models.json\` | All AI platforms, LiteLLM | ğŸ”´ CRITICAL |  
| 12 | \*\*LiteLLM\*\* | LLM Proxy | \`LITELLM\_PORT=4000\`\\\<br\\\>\`LITELLM\_MASTER\_KEY=\`\\\<br\\\>\`LITELLM\_SALT\_KEY=\`\\\<br\\\>\`DATABASE\_URL=postgresql://...\`\\\<br\\\>\`STORE\_MODEL\_IN\_DB=true\`\\\<br\\\>\`UI\_USERNAME=\`\\\<br\\\>\`UI\_PASSWORD=\`\\\<br\\\>\\\<br\\\>\*\*Per Provider:\*\*\\\<br\\\>\`OPENAI\_API\_KEY=\`\\\<br\\\>\`ANTHROPIC\_API\_KEY=\`\\\<br\\\>\`GOOGLE\_API\_KEY=\`\\\<br\\\>\`GROQ\_API\_KEY=\`\\\<br\\\>\`MISTRAL\_API\_KEY=\`\\\<br\\\>\`COHERE\_API\_KEY=\`\\\<br\\\>\`TOGETHER\_API\_KEY=\`\\\<br\\\>\`PERPLEXITY\_API\_KEY=\`\\\<br\\\>\`DEEPSEEK\_API\_KEY=\`\\\<br\\\>\`XAI\_API\_KEY=\`\\\<br\\\>\`FIREWORKS\_API\_KEY=\`\\\<br\\\>\`OPENROUTER\_API\_KEY=\`\\\<br\\\>\\\<br\\\>\*\*Routing:\*\*\\\<br\\\>\`ROUTING\_STRATEGY=complexity-based/internal-only/external-only/fallback\`\\\<br\\\>\`COMPLEXITY\_THRESHOLD=2000\`\\\<br\\\>\`FALLBACK\_MODELS=\` (comma-separated) | \`compose/litellm.yml\`\\\<br\\\>\`env/litellm.env\`\\\<br\\\>\`config/litellm/config.yaml\` (routing rules) | All AI platforms | ğŸ”´ CRITICAL |  
| 13 | \*\*LocalAI\*\* | LLM Alt | \`LOCALAI\_PORT=8080\`\\\<br\\\>\`LOCALAI\_MODELS\_PATH=/models\`\\\<br\\\>\`THREADS=4\`\\\<br\\\>\`CONTEXT\_SIZE=4096\` | \`compose/localai.yml\`\\\<br\\\>\`env/localai.env\` | Alternative to Ollama | ğŸŸ¢ LOW |  
| \*\*AI PLATFORMS\*\* |  |  |  |  |  |  |  
| 14 | \*\*OpenWebUI\*\* | Chat UI | \`WEBUI\_PORT=8080\`\\\<br\\\>\`OLLAMA\_BASE\_URL=http://ollama:11434\`\\\<br\\\>\`WEBUI\_SECRET\_KEY=\`\\\<br\\\>\`WEBUI\_NAME="AI Platform"\`\\\<br\\\>\`DEFAULT\_MODELS=\`\\\<br\\\>\`DEFAULT\_USER\_ROLE=user\`\\\<br\\\>\\\<br\\\>\*\*RAG Config:\*\*\\\<br\\\>\`RAG\_EMBEDDING\_MODEL=nomic-embed-text\`\\\<br\\\>\`RAG\_VECTOR\_DB=qdrant\`\\\<br\\\>\`QDRANT\_URL=http://qdrant:6333\`\\\<br\\\>\`QDRANT\_API\_KEY=\` | \`compose/openwebui.yml\`\\\<br\\\>\`env/openwebui.env\` | Ollama, Qdrant | ğŸ”´ CRITICAL |  
| 15 | \*\*AnythingLLM\*\* | Document AI | \`SERVER\_PORT=3001\`\\\<br\\\>\`STORAGE\_DIR=/app/storage\`\\\<br\\\>\`JWT\_SECRET=\`\\\<br\\\>\`LLM\_PROVIDER=ollama\`\\\<br\\\>\`EMBEDDING\_ENGINE=ollama\`\\\<br\\\>\`EMBEDDING\_MODEL=nomic-embed-text\`\\\<br\\\>\\\<br\\\>\*\*Vector DB:\*\*\\\<br\\\>\`VECTOR\_DB=qdrant\`\\\<br\\\>\`QDRANT\_ENDPOINT=http://qdrant:6333\`\\\<br\\\>\`QDRANT\_API\_KEY=\` | \`compose/anythingllm.yml\`\\\<br\\\>\`env/anythingllm.env\` | Ollama, Qdrant | ğŸ”´ CRITICAL |  
| 16 | \*\*Dify\*\* | AI Workflow | \`DIFY\_PORT=80\`\\\<br\\\>\`MODE=production\`\\\<br\\\>\`SECRET\_KEY=\`\\\<br\\\>\`INIT\_PASSWORD=\`\\\<br\\\>\`CONSOLE\_WEB\_URL=https://domain/dify\`\\\<br\\\>\`SERVICE\_API\_URL=https://domain/dify/api\`\\\<br\\\>\\\<br\\\>\*\*Database:\*\*\\\<br\\\>\`DB\_HOST=postgres\`\\\<br\\\>\`DB\_PORT=5432\`\\\<br\\\>\`DB\_DATABASE=dify\`\\\<br\\\>\`DB\_USERNAME=dify\`\\\<br\\\>\`DB\_PASSWORD=\`\\\<br\\\>\\\<br\\\>\*\*Redis:\*\*\\\<br\\\>\`REDIS\_HOST=redis\`\\\<br\\\>\`REDIS\_PORT=6379\`\\\<br\\\>\`REDIS\_PASSWORD=\`\\\<br\\\>\`REDIS\_USE\_SSL=false\`\\\<br\\\>\`REDIS\_DB=0\`\\\<br\\\>\\\<br\\\>\*\*Vector DB:\*\*\\\<br\\\>\`VECTOR\_STORE=qdrant\`\\\<br\\\>\`QDRANT\_URL=http://qdrant:6333\`\\\<br\\\>\`QDRANT\_API\_KEY=\`\\\<br\\\>\`QDRANT\_CLIENT\_TIMEOUT=20\`\\\<br\\\>\\\<br\\\>\*\*Storage:\*\*\\\<br\\\>\`STORAGE\_TYPE=local\`\\\<br\\\>\`STORAGE\_LOCAL\_PATH=/app/storage\` | \`compose/dify.yml\` (api \\+ worker \\+ web)\\\<br\\\>\`env/dify.env\` | Postgres, Redis, Qdrant, Ollama, LiteLLM | ğŸ”´ CRITICAL |  
| 17 | \*\*N8N\*\* | Workflow | \`N8N\_PORT=5678\`\\\<br\\\>\`N8N\_HOST=n8n\`\\\<br\\\>\`N8N\_PROTOCOL=https\`\\\<br\\\>\`N8N\_EDITOR\_BASE\_URL=https://domain/n8n\`\\\<br\\\>\`WEBHOOK\_URL=https://domain/n8n\`\\\<br\\\>\`N8N\_ENCRYPTION\_KEY=\`\\\<br\\\>\\\<br\\\>\*\*Database:\*\*\\\<br\\\>\`DB\_TYPE=postgresdb\`\\\<br\\\>\`DB\_POSTGRESDB\_HOST=postgres\`\\\<br\\\>\`DB\_POSTGRESDB\_PORT=5432\`\\\<br\\\>\`DB\_POSTGRESDB\_DATABASE=n8n\`\\\<br\\\>\`DB\_POSTGRESDB\_USER=n8n\`\\\<br\\\>\`DB\_POSTGRESDB\_PASSWORD=\`\\\<br\\\>\\\<br\\\>\*\*Redis Queue:\*\*\\\<br\\\>\`QUEUE\_BULL\_REDIS\_HOST=redis\`\\\<br\\\>\`QUEUE\_BULL\_REDIS\_PORT=6379\`\\\<br\\\>\`QUEUE\_BULL\_REDIS\_PASSWORD=\`\\\<br\\\>\`EXECUTIONS\_MODE=queue\`\\\<br\\\>\\\<br\\\>\*\*User Management:\*\*\\\<br\\\>\`N8N\_USER\_MANAGEMENT\_DISABLED=false\`\\\<br\\\>\`N8N\_EMAIL\_MODE=smtp\` (optional) | \`compose/n8n.yml\`\\\<br\\\>\`env/n8n.env\` | Postgres, Redis, Signal, GDrive | ğŸ”´ CRITICAL |  
| 18 | \*\*Flowise\*\* | Low-code AI | \`FLOWISE\_PORT=3000\`\\\<br\\\>\`FLOWISE\_USERNAME=\`\\\<br\\\>\`FLOWISE\_PASSWORD=\`\\\<br\\\>\`PASSPHRASE=\`\\\<br\\\>\\\<br\\\>\*\*Database:\*\*\\\<br\\\>\`DATABASE\_TYPE=postgres\`\\\<br\\\>\`DATABASE\_HOST=postgres\`\\\<br\\\>\`DATABASE\_PORT=5432\`\\\<br\\\>\`DATABASE\_NAME=flowise\`\\\<br\\\>\`DATABASE\_USER=flowise\`\\\<br\\\>\`DATABASE\_PASSWORD=\`\\\<br\\\>\\\<br\\\>\*\*Vector DB (in flows):\*\*\\\<br\\\>Configured via UI to connect to Qdrant | \`compose/flowise.yml\`\\\<br\\\>\`env/flowise.env\` | Postgres, Qdrant (via UI), Ollama | ğŸ”´ CRITICAL |  
| \*\*OBSERVABILITY\*\* |  |  |  |  |  |  |  
| 19 | \*\*Langfuse\*\* | LLM Observability | \`LANGFUSE\_PORT=3000\`\\\<br\\\>\`NEXTAUTH\_URL=https://domain/langfuse\`\\\<br\\\>\`NEXTAUTH\_SECRET=\`\\\<br\\\>\`SALT=\`\\\<br\\\>\`ENCRYPTION\_KEY=\`\\\<br\\\>\\\<br\\\>\*\*Database:\*\*\\\<br\\\>\`DATABASE\_URL=postgresql://langfuse:pass@postgres:5432/langfuse\`\\\<br\\\>\\\<br\\\>\*\*Auth:\*\*\\\<br\\\>\`LANGFUSE\_INIT\_USER\_EMAIL=\`\\\<br\\\>\`LANGFUSE\_INIT\_USER\_PASSWORD=\`\\\<br\\\>\`LANGFUSE\_INIT\_PROJECT\_NAME="AI Platform"\`\\\<br\\\>\`LANGFUSE\_INIT\_PROJECT\_PUBLIC\_KEY=\`\\\<br\\\>\`LANGFUSE\_INIT\_PROJECT\_SECRET\_KEY=\` | \`compose/langfuse.yml\`\\\<br\\\>\`env/langfuse.env\` | Postgres, LiteLLM integration | ğŸŸ¡ HIGH |  
| 20 | \*\*Prometheus\*\* | Metrics | \`PROMETHEUS\_PORT=9090\`\\\<br\\\>\`SCRAPE\_INTERVAL=15s\`\\\<br\\\>\`RETENTION\_TIME=15d\`\\\<br\\\>\\\<br\\\>\*\*Scrape Configs:\*\*\\\<br\\\>- Node Exporter\\\<br\\\>- cAdvisor\\\<br\\\>- All services with /metrics | \`compose/prometheus.yml\`\\\<br\\\>\`env/prometheus.env\`\\\<br\\\>\`config/prometheus/prometheus.yml\` | All services | ğŸŸ¡ MEDIUM |  
| 21 | \*\*Grafana\*\* | Dashboards | \`GRAFANA\_PORT=3000\`\\\<br\\\>\`GF\_SECURITY\_ADMIN\_USER=admin\`\\\<br\\\>\`GF\_SECURITY\_ADMIN\_PASSWORD=\`\\\<br\\\>\`GF\_SERVER\_ROOT\_URL=https://domain/grafana\`\\\<br\\\>\`GF\_AUTH\_ANONYMOUS\_ENABLED=false\`\\\<br\\\>\\\<br\\\>\*\*Datasources:\*\*\\\<br\\\>- Prometheus\\\<br\\\>- Loki\\\<br\\\>- Postgres (optional) | \`compose/grafana.yml\`\\\<br\\\>\`env/grafana.env\`\\\<br\\\>\`config/grafana/datasources.yml\`\\\<br\\\>\`config/grafana/dashboards/\*.json\` | Prometheus, Loki | ğŸŸ¡ MEDIUM |  
| 22 | \*\*Loki \\+ Promtail\*\* | Logs | \`LOKI\_PORT=3100\`\\\<br\\\>\`LOKI\_RETENTION\_PERIOD=168h\`\\\<br\\\>\`PROMTAIL\_PORT=9080\` | \`compose/loki.yml\`\\\<br\\\>\`compose/promtail.yml\`\\\<br\\\>\`env/loki.env\`\\\<br\\\>\`config/loki/loki-config.yaml\`\\\<br\\\>\`config/promtail/promtail-config.yaml\` | Grafana, all containers | ğŸŸ¡ MEDIUM |  
| 23 | \*\*cAdvisor\*\* | Container Stats | \`CADVISOR\_PORT=8080\` | \`compose/cadvisor.yml\`\\\<br\\\>\`env/cadvisor.env\` | Prometheus | ğŸŸ¡ MEDIUM |  
| 24 | \*\*Node Exporter\*\* | Host Metrics | \`NODE\_EXPORTER\_PORT=9100\` | \`compose/node-exporter.yml\`\\\<br\\\>\`env/node-exporter.env\` | Prometheus | ğŸŸ¡ MEDIUM |

\---

\#\# \*\*TABLE 2: CORRECTED USER INTERACTION FLOW\*\*

Copy table

| Step | Phase | Interaction | Output Files | Next Script Uses | Priority |  
| \----- | \----- | \----- | \----- | \----- | \----- |  
| \*\*0\*\* | \*\*PRE-FLIGHT\*\* | Port availability check (80, 443, all services) | \`metadata/port\_check.json\` | Script 2 validates before deploy | ğŸ”´ CRITICAL |  
| \*\*1\*\* | \*\*PROXY SELECTION\*\* | \*\*CORRECTED:\*\*\\\<br\\\>1) Nginx\\\<br\\\>2) Traefik\\\<br\\\>3) \*\*Caddy\*\*\\\<br\\\>4) None\\\<br\\\>\\\<br\\\>+ SSL type selection | \`compose/{nginx|traefik|caddy}.yml\`\\\<br\\\>\`env/{proxy}.env\`\\\<br\\\>\`config/{proxy}/...\`\\\<br\\\>\`metadata/proxy\_config.json\` | Script 2 deploys proxy first | ğŸ”´ CRITICAL |  
| 2 | \*\*DOMAIN/IP\*\* | Domain input â†’ DNS resolution â†’ Store public IP | \`metadata/network\_config.json\` | Script 2 configures proxy routing | ğŸ”´ CRITICAL |  
| 3 | \*\*DIRECTORY\*\* | Validate \`/mnt/data\`, create structure | \`metadata/directory\_structure.json\` | Script 2 mounts volumes | ğŸ”´ CRITICAL |  
| 4 | \*\*VECTOR DB\*\* | Qdrant / Weaviate / Milvus choice | \`compose/{vectordb}.yml\`\\\<br\\\>\`env/{vectordb}.env\`\\\<br\\\>\`metadata/vectordb\_choice.json\` | Script 3 configures AI platforms to use it | ğŸ”´ CRITICAL |  
| 5 | \*\*OLLAMA MODELS\*\* | \*\*Dynamic fetch\*\* from \`ollama.ai/library/api\`, user selects | \`metadata/ollama\_models.json\`\\\<br\\\>\`env/ollama.env\` | Script 2 downloads selected models | ğŸ”´ CRITICAL |  
| 6 | \*\*LLM PROVIDERS\*\* | \*\*All 12 providers\*\* (OpenAI, Anthropic, Google, Groq, Mistral, Cohere, Together, Perplexity, DeepSeek, xAI, Fireworks, OpenRouter) | \`env/litellm.env\` (per-provider API keys)\\\<br\\\>\`metadata/providers.json\` | Script 2 configures LiteLLM | ğŸ”´ CRITICAL |  
| 7 | \*\*LITELLM ROUTING\*\* | Strategy selection:\\\<br\\\>1) Internal-only\\\<br\\\>2) External-only\\\<br\\\>3) \*\*Hybrid (complexity-based)\*\*\\\<br\\\>4) Fallback chain | \`config/litellm/config.yaml\`\\\<br\\\>\`metadata/routing\_strategy.json\` | Script 2 loads routing config | ğŸ”´ CRITICAL |  
| 8 | \*\*AI PLATFORMS\*\* | Service selection \\+ per-service config | Individual \`compose/\*.yml\` \\+ \`env/\*.env\` files | Script 2 deploys selected services | ğŸ”´ CRITICAL |  
| 9 | \*\*VECTOR DB INTEGRATION\*\* | \*\*Auto-configure\*\* all selected AI platforms to use chosen vector DB | Updates to \`env/dify.env\`, \`env/anythingllm.env\`, \`env/openwebui.env\` | Script 3 verifies connections | ğŸ”´ CRITICAL |  
| 10 | \*\*SIGNAL-API\*\* | Method selection:\\\<br\\\>1) QR Code\\\<br\\\>2) API registration | \`compose/signal-api.yml\`\\\<br\\\>\`env/signal-api.env\`\\\<br\\\>\`metadata/signal\_config.json\` | Script 2 starts pairing process | ğŸ”´ CRITICAL |  
| 11 | \*\*GOOGLE DRIVE\*\* | Auth method:\\\<br\\\>1) OAuth\\\<br\\\>2) Service Account\\\<br\\\>3) API Key | \`compose/gdrive.yml\`\\\<br\\\>\`env/gdrive.env\`\\\<br\\\>\`config/gdrive/credentials.json\`\\\<br\\\>\`metadata/gdrive\_config.json\` | Script 3 completes OAuth flow if needed | ğŸ”´ CRITICAL |  
| 12 | \*\*MONITORING\*\* | Service selection (Langfuse, Prometheus, etc.) | Individual \`compose/\*.yml\` \\+ \`env/\*.env\` | Script 2 deploys monitoring stack | ğŸŸ¡ HIGH |  
| 13 | \*\*SUMMARY\*\* | Display all choices, confirm | \`metadata/deployment\_summary.json\` | Script 2 reads as deployment plan | ğŸ”´ CRITICAL |

\#\# \*\*SUCCESS CRITERIA BY PHASE\*\*

\#\#\# \*\*Phase 0 (Cleanup)\*\*

\* âœ… System returned to clean state    
\* âœ… No orphaned containers/volumes/networks    
\* âœ… Ready for fresh deployment

\#\#\# \*\*Phase 1 (Setup)\*\*

\* âœ… All ports allocated without conflicts    
\* âœ… User reviewed and approved configuration    
\* âœ… \`.env\` file generated (pure text, no ANSI codes)    
\* âœ… Docker group permissions active    
\* âœ… Reverse proxy config files created    
\* âœ… NO containers running yet

\#\#\# \*\*Phase 2 (Deployment)\*\*

\* âœ… All containers running with \`restart: unless-stopped\`    
\* âœ… All services pass health checks    
\* âœ… Inter-service communication works    
\* âœ… Reverse proxy routes traffic correctly    
\* âœ… Tailscale connected    
\* âœ… Credentials documented

\#\#\# \*\*Phase 3 (Configuration)\*\*

\* âœ… LLM models loaded in Ollama    
\* âœ… LiteLLM routing configured (local â†’ cloud fallback)    
\* âœ… AnythingLLM using Qdrant \\+ Ollama    
\* âœ… OpenClaw linked to AnythingLLM    
\* âœ… OpenClaw accessible via Tailscale HTTPS    
\* âœ… Dify using LiteLLM gateway    
\* âœ… Google Drive sync active    
\* âœ… Services auto-start on reboot

\#\#\# \*\*Phase 4 (Extensibility)\*\*

\* âœ… New services can be added via standardized script    
\* âœ… No modification of core scripts required    
\* âœ… Automatic integration with existing infrastructure

\---

\#\# \*\*FINAL VALIDATION CHECKLIST\*\*

\*\*After completing all 4 scripts, the system MUST:\*\*

1\. âœ… \*\*Deploy from scratch\*\* on fresh Ubuntu in \\\<30 minutes    
2\. âœ… \*\*Survive reboots\*\* (all services auto-restart)    
3\. âœ… \*\*Route LLM requests\*\* intelligently (local-first, cloud fallback)    
4\. âœ… \*\*Expose services\*\* via HTTPS (Caddy auto-cert OR Nginx self-signed)    
5\. âœ… \*\*Secure remote access\*\* via Tailscale VPN    
6\. âœ… \*\*Sync Google Drive\*\* to \`/mnt/data/gdrive/\` automatically    
7\. âœ… \*\*Link OpenClaw\*\* to AnythingLLM vector DB    
8\. âœ… \*\*Accessible endpoints:\*\*    
   \* Public: \`https://domain/anythingllm\`, \`https://domain/dify\`    
   \* Private: \`https://tailscale-ip:18789\` (OpenClaw)    
9\. âœ… \*\*Zero manual configuration\*\* (all via scripts)    
10\. âœ… \*\*Fully documented\*\* (credentials.txt, .env, logs)

Codebase : 

\* The repository \[https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation\](https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation)    
\* high level objectives here : \[https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/refs/heads/main/README.md\](https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/refs/heads/main/README.md)    
\* The previous (superseeded) high level objectives : \[https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/3176f2f3da7ee9ccb2908380387df3e38923a8d4/README.md\](https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/3176f2f3da7ee9ccb2908380387df3e38923a8d4/README.md)    
\* script 0 : https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/refs/heads/main/scripts/0-complete-cleanup.sh    
\* script 1: https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/refs/heads/main/scripts/1-setup-system.sh    
\* script 2 : https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/refs/heads/main/scripts/2-deploy-services.sh    
\* script 3 : https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/refs/heads/main/scripts/3-configure-services.sh    
\* script 4 : https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/refs/heads/main/scripts/3-configure-services.sh    
      
\* This was a good start ui wise : \[https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/7df4b977d0d66f7dcdd0b099a38fb4011402d280/scripts/1-setup-system.sh\](https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/7df4b977d0d66f7dcdd0b099a38fb4011402d280/scripts/1-setup-system.sh)    
\* This was the iteration with more bugs : \[https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/0af338937926c8d052d9a413b79409376e8c7dfa/scripts/1-setup-system.sh\](https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/0af338937926c8d052d9a413b79409376e8c7dfa/scripts/1-setup-system.sh)    
\* This was a good attempt to incorporate all mandatory variables from services : https://raw.githubusercontent.com/jgla1ne/AIPlatformAutomation/069b5f51f047319120a1a97080116bbe4a1d322b/scripts/1-setup-system.sh

 

