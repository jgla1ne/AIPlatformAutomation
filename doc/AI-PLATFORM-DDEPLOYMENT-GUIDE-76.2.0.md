\# AI Platform Deployment Guide

\> \*\*Version:\*\* 76.2.0  
\> \*\*Date:\*\* 2025-07-12  
\> \*\*Repository:\*\* \[github.com/jgla1ne/AIPlatformAutomation\](https://github.com/jgla1ne/AIPlatformAutomation)  
\> \*\*Previous:\*\* 76.1.0 â†’ 76.2.0 (full rewrite â€” architecture corrections, Hetzner references removed, EC2-native)

| Script | Status |  
|--------|--------|  
| \`0-complete-cleanup.sh\` | Tested âœ… |  
| \`1-setup-system.sh\` | In Progress ğŸ”§ (fails at dependency install) |  
| \`2-deploy-services.sh\` | Projected ğŸ“ |  
| \`3-configure-services.sh\` | Projected ğŸ“ |  
| \`4-add-service.sh\` | Projected ğŸ“ |

\---

\#\# Table of Contents

1\. \[Executive Summary\](\#1-executive-summary)  
2\. \[System Architecture\](\#2-system-architecture)  
3\. \[Repository & Folder Structure\](\#3-repository--folder-structure)  
4\. \[Service Inventory & Port Map\](\#4-service-inventory--port-map)  
5\. \[Storage Architecture\](\#5-storage-architecture)  
6\. \[Network & Access Architecture\](\#6-network--access-architecture)  
7\. \[LLM Models â€” Local (Ollama)\](\#7-llm-models--local-ollama)  
8\. \[LLM Models â€” External (Cloud APIs)\](\#8-llm-models--external-cloud-apis)  
9\. \[LiteLLM Routing Strategy\](\#9-litellm-routing-strategy)  
10\. \[Vector Database â€” Fluid Selection & OpenClaw Integration\](\#10-vector-database--fluid-selection--openclaw-integration)  
11\. \[Google Drive Rsync â€” Authentication & Embedding Pipeline\](\#11-google-drive-rsync--authentication--embedding-pipeline)  
12\. \[Reverse Proxy â€” Fluid Selection\](\#12-reverse-proxy--fluid-selection)  
13\. \[Monitoring â€” Optional Prometheus & Grafana Stack\](\#13-monitoring--optional-prometheus--grafana-stack)  
14\. \[Security Model\](\#14-security-model)  
15\. \[Credentials & Secrets Management\](\#15-credentials--secrets-management)  
16\. \[Script Pipeline Overview\](\#16-script-pipeline-overview)  
17\. \[Script 0 â€” Nuclear Clean\](\#17-script-0--nuclear-clean)  
18\. \[Script 1 â€” System Setup\](\#18-script-1--system-setup)  
19\. \[Script 2 â€” Deploy Services\](\#19-script-2--deploy-services)  
20\. \[Script 3 â€” Configure Services\](\#20-script-3--configure-services)  
21\. \[Script 4 â€” Add Service\](\#21-script-4--add-service)  
22\. \[Per-Service Configuration Details\](\#22-per-service-configuration-details)  
23\. \[Backup & Disaster Recovery\](\#23-backup--disaster-recovery)  
24\. \[Startup Order & Dependency Chain\](\#24-startup-order--dependency-chain)  
25\. \[Lessons Learnt & No-Go Rules\](\#25-lessons-learnt--no-go-rules)  
26\. \[Changelog\](\#26-changelog)

\---

\#\# 1\. Executive Summary

This document is the complete specification for deploying a self-hosted AI platform on a \*\*single EC2 instance\*\* running Ubuntu 24.04 LTS. The platform provides:

| Capability | Service(s) |  
|------------|-----------|  
| AI workflow orchestration | Dify, n8n, Flowise |  
| Chat interface | Open WebUI |  
| Local LLM inference | Ollama (native systemd) |  
| Cloud LLM routing | LiteLLM proxy |  
| Knowledge base / RAG | OpenClaw \+ Vector DB (fluid) |  
| Document storage & sync | Google Drive â†’ local rsync â†’ embedding pipeline |  
| Object storage | MinIO |  
| Reverse proxy & TLS | Caddy or nginx (fluid) |  
| Monitoring (optional) | Prometheus \+ Grafana \+ Dozzle |  
| Container log viewer | Dozzle |  
| Backup | Local snapshots \+ Google Drive |

\#\#\# Design Principles

1\. \*\*Single machine\*\* â€” one EC2 instance, no cluster orchestration  
2\. \*\*One compose per service\*\* â€” modular, independently manageable  
3\. \*\*Tailscale-only access\*\* â€” no public ports, no UFW (EC2 security groups \+ Tailscale)  
4\. \*\*Fluid selections\*\* â€” vector DB, reverse proxy, LLM models chosen at deploy time  
5\. \*\*Script separation\*\* â€” setup (1) â†’ deploy (2) â†’ configure (3) â†’ extend (4)  
6\. \*\*All logs under \`$ROOT\_PATH/logs/\`\*\* â€” every script logs everything  
7\. \*\*Secrets auto-generated, user-confirmed\*\* â€” visible via interactive prompts

\---

\#\# 2\. System Architecture

\#\#\# Hardware (EC2)

| Resource | Minimum | Recommended |  
|----------|---------|-------------|  
| vCPU | 4 | 8+ |  
| RAM | 16 GB | 32 GB+ |  
| OS Disk | 30 GB | 50 GB |  
| Data Disk (EBS) | 100 GB | 200 GB+ |  
| GPU | Optional | NVIDIA (for local LLM) |

\#\#\# Architecture Tiers

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ EC2 INSTANCE (Ubuntu 24.04) â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ Tier 0: OS \+ System Packages \[Script 1\] â”‚ â”‚ â””â”€â”€ Docker, Docker Compose, NVIDIA drivers (if GPU) â”‚ â”‚ â”‚ â”‚ Tier 1: Networking \[Script 1\] â”‚ â”‚ â””â”€â”€ Tailscale VPN, Docker networks â”‚ â”‚ â”‚ â”‚ Tier 2: Storage \[Script 1\] â”‚ â”‚ â””â”€â”€ /mnt/data mount, directory tree, fstab â”‚ â”‚ â”‚ â”‚ Tier 3: Data Layer \[Script 2\] â”‚ â”‚ â””â”€â”€ PostgreSQL, Redis, Vector DB, MinIO â”‚ â”‚ â”‚ â”‚ Tier 4: Infrastructure Services \[Script 2\] â”‚ â”‚ â””â”€â”€ Ollama (systemd), LiteLLM, Reverse Proxy â”‚ â”‚ â”‚ â”‚ Tier 5: Application Services \[Script 2\] â”‚ â”‚ â””â”€â”€ Dify, n8n, Open WebUI, Flowise, AnythingLLM â”‚ â”‚ â”‚ â”‚ Tier 6: Knowledge & RAG \[Script 2 \+ 3\] â”‚ â”‚ â””â”€â”€ OpenClaw, Google Drive sync, embedding pipeline â”‚ â”‚ â”‚ â”‚ Tier 7: Monitoring (Optional) \[Script 2 \+ 3\] â”‚ â”‚ â””â”€â”€ Prometheus, Grafana, Dozzle â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ Configuration & Wiring \[Script 3\] â”‚ â”‚ Service Addition \[Script 4\] â”‚ â”‚ Full Reset \[Script 0\] â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\#\#\# Data Flow

User (browser) â”‚ â–¼ Tailscale VPN tunnel â”‚ â–¼ Reverse Proxy (Caddy/nginx) :443 â”‚ â”œâ”€â”€â–º Dify :3000 â”€â”€â–º LiteLLM :4000 â”€â”€â–º Ollama :11434 â”œâ”€â”€â–º n8n :5678 â”€â”€â–º LiteLLM :4000 â”€â”€â–º Cloud APIs â”œâ”€â”€â–º Open WebUI :8080 â”€â”€â–º Ollama :11434 (direct) â”œâ”€â”€â–º Flowise :3001 â”€â”€â–º LiteLLM :4000 â”œâ”€â”€â–º AnythingLLM :3002 â”€â”€â–º LiteLLM :4000 â”œâ”€â”€â–º OpenClaw :8888 â”€â”€â–º Vector DB :6333/:19530/:8001 â”œâ”€â”€â–º Grafana :3100 (optional) â”œâ”€â”€â–º Dozzle :9999 â””â”€â”€â–º MinIO :9001 (console) â”‚ â–¼ MinIO API :9000 (S3-compatible, internal)

\---

\#\# 3\. Repository & Folder Structure

\#\#\# Git Repository (OS Disk)

\~/AIPlatformAutomation/ â”œâ”€â”€ scripts/ â”‚ â”œâ”€â”€ 0-complete-cleanup.sh â”‚ â”œâ”€â”€ 1-setup-system.sh â”‚ â”œâ”€â”€ 2-deploy-services.sh â”‚ â”œâ”€â”€ 3-configure-services.sh â”‚ â””â”€â”€ 4-add-service.sh â”œâ”€â”€ lib/ â”‚ â””â”€â”€ common.sh \# Shared functions (logging, prompts, checks) â”œâ”€â”€ templates/ â”‚ â”œâ”€â”€ docker-compose/ â”‚ â”‚ â”œâ”€â”€ dify.yaml.tpl â”‚ â”‚ â”œâ”€â”€ n8n.yaml.tpl â”‚ â”‚ â”œâ”€â”€ open-webui.yaml.tpl â”‚ â”‚ â”œâ”€â”€ flowise.yaml.tpl â”‚ â”‚ â”œâ”€â”€ anythingllm.yaml.tpl â”‚ â”‚ â”œâ”€â”€ openclaw.yaml.tpl â”‚ â”‚ â”œâ”€â”€ litellm.yaml.tpl â”‚ â”‚ â”œâ”€â”€ postgres.yaml.tpl â”‚ â”‚ â”œâ”€â”€ redis.yaml.tpl â”‚ â”‚ â”œâ”€â”€ qdrant.yaml.tpl â”‚ â”‚ â”œâ”€â”€ weaviate.yaml.tpl â”‚ â”‚ â”œâ”€â”€ milvus.yaml.tpl â”‚ â”‚ â”œâ”€â”€ minio.yaml.tpl â”‚ â”‚ â”œâ”€â”€ caddy.yaml.tpl â”‚ â”‚ â”œâ”€â”€ nginx.yaml.tpl â”‚ â”‚ â”œâ”€â”€ prometheus.yaml.tpl â”‚ â”‚ â”œâ”€â”€ grafana.yaml.tpl â”‚ â”‚ â””â”€â”€ dozzle.yaml.tpl â”‚ â”œâ”€â”€ configs/ â”‚ â”‚ â”œâ”€â”€ litellm-config.yaml.tpl â”‚ â”‚ â”œâ”€â”€ Caddyfile.tpl â”‚ â”‚ â”œâ”€â”€ nginx.conf.tpl â”‚ â”‚ â”œâ”€â”€ prometheus.yml.tpl â”‚ â”‚ â””â”€â”€ grafana-datasources.yaml.tpl â”‚ â””â”€â”€ env/ â”‚ â””â”€â”€ master.env.tpl â”œâ”€â”€ doc/ â”‚ â””â”€â”€ AI-PLATFORM-DEPLOYMENT-GUIDE-76.2.0.md â”œâ”€â”€ logs/ \# All script output logs â”‚ â”œâ”€â”€ script-0-*.log â”‚ â”œâ”€â”€ script-1-*.log â”‚ â”œâ”€â”€ script-2-*.log â”‚ â”œâ”€â”€ script-3-*.log â”‚ â””â”€â”€ script-4-\*.log â””â”€â”€ README.md

\#\#\# Runtime Data (/mnt/data)

/mnt/data/ â”œâ”€â”€ docker/ â”‚ â”œâ”€â”€ dify/ â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml \# Generated from template â”‚ â”‚ â”œâ”€â”€ .env \# Service-specific env â”‚ â”‚ â””â”€â”€ data/ \# Persistent volumes â”‚ â”œâ”€â”€ n8n/ â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ .env â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ open-webui/ â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ .env â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ flowise/ â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ .env â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ anythingllm/ â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ .env â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ openclaw/ â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ .env â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ litellm/ â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ .env â”‚ â”‚ â”œâ”€â”€ config.yaml \# LiteLLM routing config â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ postgres/ â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ .env â”‚ â”‚ â”œâ”€â”€ init/ \# SQL init scripts â”‚ â”‚ â”‚ â””â”€â”€ 00-create-databases.sql â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ redis/ â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ vector-db/ \# Whichever was selected â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ .env â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ minio/ â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ .env â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ caddy/ (or nginx/) â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ Caddyfile (or nginx.conf) â”‚ â”‚ â”œâ”€â”€ data/ â”‚ â”‚ â””â”€â”€ config/ â”‚ â”œâ”€â”€ prometheus/ \# Optional â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ prometheus.yml â”‚ â”‚ â””â”€â”€ data/ â”‚ â”œâ”€â”€ grafana/ \# Optional â”‚ â”‚ â”œâ”€â”€ docker-compose.yaml â”‚ â”‚ â”œâ”€â”€ provisioning/ â”‚ â”‚ â””â”€â”€ data/ â”‚ â””â”€â”€ dozzle/ â”‚ â””â”€â”€ docker-compose.yaml â”œâ”€â”€ ollama/ â”‚ â””â”€â”€ models/ \# Ollama model storage â”œâ”€â”€ gdrive/ \# Google Drive rsync target â”‚ â””â”€â”€ (synced files for embedding) â”œâ”€â”€ backups/ â”‚ â”œâ”€â”€ daily/ â”‚ â””â”€â”€ manual/ â”œâ”€â”€ secrets/ â”‚ â””â”€â”€ master.env \# All credentials (chmod 600\) â””â”€â”€ logs/ \# Symlink â†’ \~/AIPlatformAutomation/logs/

\---

\#\# 4\. Service Inventory & Port Map

\#\#\# Core Services

| Service | Container Name | Port | Network(s) | Depends On | Disk |  
|---------|---------------|------|------------|------------|------|  
| PostgreSQL | postgres | 5432 | backend | â€” | data |  
| Redis | redis | 6379 | backend | â€” | data |  
| Vector DB (fluid) | qdrant/weaviate/milvus | 6333/8080/19530 | backend | â€” | data |  
| MinIO | minio | 9000 (API), 9001 (console) | backend, frontend | â€” | data |  
| Ollama | â€” (systemd) | 11434 | host network | â€” | data |

\#\#\# Infrastructure Services

| Service | Container Name | Port | Network(s) | Depends On | Disk |  
|---------|---------------|------|------------|------------|------|  
| LiteLLM | litellm | 4000 | backend, frontend | PostgreSQL | data |  
| Caddy (or nginx) | caddy | 80, 443 | frontend | â€” | data |

\#\#\# Application Services

| Service | Container Name | Port | Network(s) | Depends On | Disk |  
|---------|---------------|------|------------|------------|------|  
| Dify | dify-\* (multi-container) | 3000 | backend, frontend | PostgreSQL, Redis, MinIO | data |  
| n8n | n8n | 5678 | backend, frontend | PostgreSQL | data |  
| Open WebUI | open-webui | 8080 | frontend | Ollama | data |  
| Flowise | flowise | 3001 | backend, frontend | â€” | data |  
| AnythingLLM | anythingllm | 3002 | backend, frontend | â€” | data |  
| OpenClaw | openclaw | 8888 | backend, frontend | Vector DB | data |

\#\#\# Monitoring Services (Optional)

| Service | Container Name | Port | Network(s) | Depends On | Disk |  
|---------|---------------|------|------------|------------|------|  
| Dozzle | dozzle | 9999 | frontend | Docker socket | data |  
| Prometheus | prometheus | 9090 | monitoring, backend | â€” | data |  
| Grafana | grafana | 3100 | monitoring, frontend | Prometheus | data |

\#\#\# Docker Networks

| Network | Purpose | Services |  
|---------|---------|----------|  
| \`frontend\` | Reverse proxy â†’ application UIs | Caddy, all apps, Grafana, Dozzle |  
| \`backend\` | App â†’ database/API communication | All apps, PostgreSQL, Redis, Vector DB, MinIO, LiteLLM |  
| \`monitoring\` | Metrics collection (optional) | Prometheus, Grafana, exporters |

\---

\#\# 5\. Storage Architecture

\#\#\# Disk Detection Logic (Script 1\)

Boot â†’ detect\_disks() â”œâ”€â”€ List all block devices (lsblk) â”œâ”€â”€ Identify OS disk (mounted at /) â”œâ”€â”€ Find additional EBS volumes â”‚ â”œâ”€â”€ Found â†’ prompt user to confirm â”‚ â”‚ â”œâ”€â”€ Already mounted â†’ use existing mount â”‚ â”‚ â”œâ”€â”€ Not mounted â†’ format ext4, mount at /mnt/data, add to fstab â”‚ â”‚ â””â”€â”€ Multiple found â†’ user selects one â”‚ â””â”€â”€ Not found â†’ use /mnt/data on OS disk (mkdir) â””â”€â”€ Set DATA\_DISK=/mnt/data

\#\#\# Volume Mapping Principle

\- \*\*OS Disk (\`\~/AIPlatformAutomation/\`)\*\*: Git repo, scripts, templates, logs  
\- \*\*Data Disk (\`/mnt/data/\`)\*\*: All Docker volumes, model files, synced data, backups, secrets  
\- \*\*Rationale\*\*: OS disk can be rebuilt from Git; data disk holds state

\#\#\# fstab Entry (if separate EBS)

/dev/xvdf /mnt/data ext4 defaults,nofail 0 2

The \`nofail\` flag ensures the instance boots even if the EBS volume is detached.

\---

\#\# 6\. Network & Access Architecture

\#\#\# Access Model: Tailscale-Only

Internet â”€â”€Xâ”€â”€â–º EC2 (no public ports) â”‚ Tailscale Network â”€â”€â–º EC2 Tailscale IP (100.x.y.z) â”‚ â–¼ Caddy :443 â”€â”€â–º services

\*\*No UFW.\*\* Firewall is handled by:  
1\. \*\*EC2 Security Group\*\* â€” allow only Tailscale UDP (port 41641\) \+ SSH from Tailscale  
2\. \*\*Tailscale ACLs\*\* â€” control which users/devices reach which ports  
3\. \*\*Docker network isolation\*\* â€” services only see networks they're attached to

\#\#\# Tailscale Setup (Script 1\)

install\_tailscale() â”œâ”€â”€ curl \-fsSL [https://tailscale.com/install.sh](https://tailscale.com/install.sh) | sh â”œâ”€â”€ Prompt for auth key (or use TAILSCALE\_AUTH\_KEY env var) â”œâ”€â”€ tailscale up \--authkey=$key \--hostname=ai-platform â”œâ”€â”€ Wait for connection (poll tailscale status) â”œâ”€â”€ Retrieve Tailscale IP: tailscale ip \-4 â”œâ”€â”€ Store TAILSCALE\_IP in master.env â””â”€â”€ Verify: tailscale status shows "authenticated"

\#\#\# DNS Strategy

| Method | URL Pattern | How |  
|--------|-------------|-----|  
| Tailscale MagicDNS | \`ai-platform.tail-net:port\` | Automatic |  
| Custom domain | \`\*.ai.datasquiz.net\` | DNS CNAME â†’ Tailscale IP, Caddy handles TLS |

The \`$DOMAIN\_NAME\` variable (default: \`ai.datasquiz.net\`) is set interactively in Script 1 and used by Script 2 to generate reverse proxy config.

Subdomains generated:  
\- \`dify.ai.datasquiz.net\`  
\- \`n8n.ai.datasquiz.net\`  
\- \`chat.ai.datasquiz.net\` (Open WebUI)  
\- \`flowise.ai.datasquiz.net\`  
\- \`anythingllm.ai.datasquiz.net\`  
\- \`openclaw.ai.datasquiz.net\`  
\- \`minio.ai.datasquiz.net\`  
\- \`grafana.ai.datasquiz.net\` (optional)  
\- \`dozzle.ai.datasquiz.net\`  
\- \`litellm.ai.datasquiz.net\`  
\---

\#\# 7\. LLM Models â€” Local (Ollama)

\#\#\# Installation Method

Ollama runs as a \*\*native systemd service\*\* (NOT in Docker) for direct GPU passthrough.

install\_ollama() â”œâ”€â”€ curl \-fsSL [https://ollama.com/install.sh](https://ollama.com/install.sh) | sh â”œâ”€â”€ Create override: /etc/systemd/system/ollama.service.d/override.conf â”‚ \[Service\] â”‚ Environment="OLLAMA\_HOST=0.0.0.0:11434" â”‚ Environment="OLLAMA\_MODELS=/mnt/data/ollama/models" â”œâ”€â”€ systemctl daemon-reload â”œâ”€â”€ systemctl enable \--now ollama â”œâ”€â”€ Wait for health: curl \-s [http://localhost:11434/api/version](http://localhost:11434/api/version) â””â”€â”€ Store OLLAMA\_BASE\_URL=[http://host.docker.internal:11434](http://host.docker.internal:11434) in master.env

\#\#\# Model Categories

| Category | Models | Size Range | Use Case |  
|----------|--------|------------|----------|  
| Lightweight | tinyllama, phi3:mini, gemma2:2b | 1â€“3 GB | Testing, quick inference |  
| Midrange | mistral, llama3.1:8b, codellama:7b | 4â€“8 GB | General chat, code assist |  
| Heavy | llama3.1:70b, deepseek-coder-v2, mixtral:8x7b | 20â€“50 GB | Complex reasoning, RAG |  
| Embedding | nomic-embed-text, mxbai-embed-large | 0.3â€“1 GB | Vector DB ingestion |

\#\#\# Model Pull Strategy (Script 2\)

pull\_ollama\_models() â”œâ”€â”€ Read MODEL\_TIER from master.env (set interactively in Script 1\) â”‚ TIER=minimal â†’ tinyllama, nomic-embed-text â”‚ TIER=standard â†’ above \+ mistral, llama3.1:8b, codellama:7b â”‚ TIER=full â†’ above \+ llama3.1:70b, mixtral:8x7b, deepseek-coder-v2 â”œâ”€â”€ Check available disk: df \-BG /mnt/data | awk 'NR==2{print $4}' â”‚ If \<50GB remaining, warn and skip heavy models â”œâ”€â”€ For each model: â”‚ â”œâ”€â”€ Check if already present: ollama list | grep $model â”‚ â”œâ”€â”€ If missing: ollama pull $model â”‚ â”œâ”€â”€ Verify: ollama list | grep $model â”‚ â””â”€â”€ Log result to $LOG\_DIR/ollama-pulls.log â””â”€â”€ Generate $CONFIG\_DIR/ollama-models.json (consumed by LiteLLM)

\#\#\# Model Storage

/mnt/data/ollama/ â””â”€â”€ models/ â”œâ”€â”€ manifests/ â”‚ â””â”€â”€ registry.ollama.ai/ â”‚ â””â”€â”€ library/ â”‚ â”œâ”€â”€ mistral/ â”‚ â”œâ”€â”€ llama3.1/ â”‚ â”œâ”€â”€ nomic-embed-text/ â”‚ â””â”€â”€ ... â””â”€â”€ blobs/ â”œâ”€â”€ sha256-xxxxxx (model weights) â””â”€â”€ ...

All models live on the EBS data volume. Instance stop/start preserves them. Instance terminate loses them unless EBS is set to persist.

\---

\#\# 8\. LLM Models â€” External (Cloud APIs)

\#\#\# Supported Providers

| Provider | Env Variable | Models Available | Cost Model |  
|----------|-------------|------------------|------------|  
| OpenAI | OPENAI\_API\_KEY | gpt-4o, gpt-4o-mini, gpt-4-turbo, o1, o1-mini | Per-token |  
| Anthropic | ANTHROPIC\_API\_KEY | claude-sonnet-4, claude-3.5-haiku, opus | Per-token |  
| Google | GOOGLE\_API\_KEY | gemini-2.5-pro, gemini-2.5-flash | Per-token (free tier available) |  
| Mistral | MISTRAL\_API\_KEY | mistral-large, mistral-medium, codestral | Per-token |  
| Groq | GROQ\_API\_KEY | llama-3.1-70b, mixtral-8x7b (fast inference) | Per-token (free tier) |  
| OpenRouter | OPENROUTER\_API\_KEY | 200+ models via single key | Per-token (markup) |  
| AWS Bedrock | AWS\_ACCESS\_KEY\_ID \+ AWS\_SECRET\_ACCESS\_KEY | claude, titan, llama via AWS | Per-token |

\#\#\# Key Collection (Script 1\)

collect\_api\_keys() â”œâ”€â”€ echo "Enter API keys (press Enter to skip any):" â”œâ”€â”€ For each provider: â”‚ â”œâ”€â”€ read \-sp " $PROVIDER API key: " key â”‚ â”œâ”€â”€ If non-empty: â”‚ â”‚ â”œâ”€â”€ Validate format (basic regex per provider) â”‚ â”‚ â”œâ”€â”€ Write to master.env: ${PROVIDER}\_API\_KEY=$key â”‚ â”‚ â””â”€â”€ Add provider to ENABLED\_CLOUD\_PROVIDERS list â”‚ â””â”€â”€ If empty: skip (log "Skipped $PROVIDER") â”œâ”€â”€ At least one of: local Ollama OR one cloud key must be set â”‚ Otherwise: error "No LLM backend configured" â””â”€â”€ Write ENABLED\_CLOUD\_PROVIDERS to master.env

\#\#\# Security

\- Keys stored in \`master.env\` with \`chmod 600\`  
\- Docker services receive keys via \`env\_file:\` directive (never baked into images)  
\- \`master.env\` is in \`.gitignore\` â€” never committed  
\- Backup: keys are included in encrypted config backup (Section 16\)

\---

\#\# 9\. LiteLLM Routing Strategy

\#\#\# Purpose

LiteLLM provides a \*\*single OpenAI-compatible endpoint\*\* that routes to any backend â€” local Ollama models, OpenAI, Anthropic, etc. Every service on the platform points to LiteLLM instead of directly to individual providers.

\#\#\# Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Dify â”‚â”€â”€â” â”‚ n8n â”‚â”€â”€â”¤ â”‚ Open WebUIâ”‚â”€â”€â”¼â”€â”€â–º [http://litellm:4000/v1](http://litellm:4000/v1) â”€â”€â–º Router â”‚ Flowise â”‚â”€â”€â”¤ â”‚ â”‚ AnythingLLMâ”€â”€â”˜ â”‚ â”‚ OpenClaw â”‚â”€â”€â”˜ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Model Map â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ gpt-4o â†’ OpenAI API â”‚ â”‚ claude-sonnet â†’ Anthropic API â”‚ â”‚ mistral â†’ ollama/mistral â”‚ â”‚ llama3.1 â†’ ollama/llama3.1:8b â”‚ â”‚ embed â†’ ollama/nomic-embed-text â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\#\#\# Configuration File Generation (Script 2\)

generate\_litellm\_config() â”œâ”€â”€ Source master.env â”œâ”€â”€ Start config YAML: â”‚ model\_list: \[\] â”‚ general\_settings: â”‚ master\_key: $ LITELLM\_MASTER\_KEY â”‚ database\_url: postgresql:// $ POSTGRES\_USER:$POSTGRES\_PASSWORD@postgres:5432/litellm â”‚ â”œâ”€â”€ Add Ollama models (from ollama-models.json): â”‚ For each model in ollama list: â”‚ \- model\_name: $ friendly\_name â”‚ litellm\_params: â”‚ model: ollama/ $ model\_tag â”‚ api\_base: [http://host.docker.internal:11434](http://host.docker.internal:11434) â”‚ â”œâ”€â”€ Add cloud models (from ENABLED\_CLOUD\_PROVIDERS): â”‚ If OPENAI\_API\_KEY set: â”‚ \- model\_name: gpt-4o â”‚ litellm\_params: â”‚ model: openai/gpt-4o â”‚ api\_key: os.environ/OPENAI\_API\_KEY â”‚ \- model\_name: gpt-4o-mini â”‚ ... â”‚ If ANTHROPIC\_API\_KEY set: â”‚ \- model\_name: claude-sonnet â”‚ litellm\_params: â”‚ model: anthropic/claude-sonnet-4 â”‚ api\_key: os.environ/ANTHROPIC\_API\_KEY â”‚ ... (repeat per provider) â”‚ â”œâ”€â”€ Add embedding models: â”‚ \- model\_name: text-embedding â”‚ litellm\_params: â”‚ model: ollama/nomic-embed-text â”‚ api\_base: [http://host.docker.internal:11434](http://host.docker.internal:11434) â”‚ â”œâ”€â”€ Add router settings: â”‚ router\_settings: â”‚ routing\_strategy: least-busy â”‚ num\_retries: 3 â”‚ timeout: 120 â”‚ fallbacks: â”‚ \- gpt-4o: \[claude-sonnet, llama3.1\] â”‚ \- claude-sonnet: \[gpt-4o, mistral\] â”‚ â””â”€â”€ Write to $CONFIG\_DIR/litellm/config.yaml

\#\#\# Docker Compose Block

\`\`\`yaml  
litellm:  
  image: ghcr.io/berriai/litellm:main-latest  
  container\_name: litellm  
  restart: unless-stopped  
  ports:  
    \- "127.0.0.1:4000:4000"  
  volumes:  
    \- ${CONFIG\_DIR}/litellm/config.yaml:/app/config.yaml:ro  
  env\_file:  
    \- ${ENV\_DIR}/master.env  
  environment:  
    \- LITELLM\_MASTER\_KEY=${LITELLM\_MASTER\_KEY}  
    \- DATABASE\_URL=postgresql://${POSTGRES\_USER}:${POSTGRES\_PASSWORD}@postgres:5432/litellm  
    \- LITELLM\_LOG\_LEVEL=INFO  
  extra\_hosts:  
    \- "host.docker.internal:host-gateway"  
  networks:  
    \- backend  
    \- db-network  
  depends\_on:  
    postgres:  
      condition: service\_healthy  
  healthcheck:  
    test: \["CMD", "curl", "-f", "http://localhost:4000/health"\]  
    interval: 30s  
    timeout: 10s  
    retries: 5

### **How Services Connect**

Each service is configured to use LiteLLM as its OpenAI-compatible backend:

Copy table

| Service | Configuration Setting | Value |
| ----- | ----- | ----- |
| Dify | Custom Model Provider â†’ OpenAI-compatible | `http://litellm:4000/v1` \+ master key |
| n8n | OpenAI credentials node | Base URL: `http://litellm:4000/v1` |
| Open WebUI | Settings â†’ Connections â†’ OpenAI | `http://litellm:4000/v1` |
| Flowise | ChatOpenAI node | Base URL: `http://litellm:4000/v1` |
| AnythingLLM | LLM Provider â†’ Generic OpenAI | `http://litellm:4000/v1` |
| OpenClaw | Provider config | `http://litellm:4000/v1` |

---

## **10\. Vector Database â€” Fluid Selection & OpenClaw Integration**

### **Design Principle**

The platform does **not** hardcode a single vector DB. The user selects during Script 1, and Script 2 deploys accordingly. All services that need vector search are configured to point at whichever was chosen.

### **Supported Options**

Copy table

| Option | Image | Port | Strength | When to Choose |
| ----- | ----- | ----- | ----- | ----- |
| Qdrant | qdrant/qdrant:latest | 6333 | Rust-fast, rich filtering, REST+gRPC | Default recommendation. Best all-around. |
| Weaviate | semitechnologies/weaviate:latest | 8080 | GraphQL, multi-modal, auto-vectorize | If you want schema-based search |
| Milvus | milvusdb/milvus:latest | 19530 | Massive scale, GPU index | If dataset \>10M vectors |
| ChromaDB | chromadb/chroma:latest | 8000 | Dead simple, Python-native | Quick prototyping, small datasets |
| pgvector | (extension on Postgres) | 5432 | No extra service, SQL-native | Minimize containers, moderate scale |

### **Selection Flow (Script 1\)**

select\_vector\_db()  
  â”œâ”€â”€ echo "Select vector database:"  
  â”œâ”€â”€ echo "  1\) Qdrant     (recommended â€” fast, production-ready)"  
  â”œâ”€â”€ echo "  2\) Weaviate   (GraphQL, auto-vectorization)"  
  â”œâ”€â”€ echo "  3\) Milvus     (massive scale)"  
  â”œâ”€â”€ echo "  4\) ChromaDB   (simple, lightweight)"  
  â”œâ”€â”€ echo "  5\) pgvector   (PostgreSQL extension, no extra container)"  
  â”œâ”€â”€ read \-p "Choice \[1\]: " choice  
  â”œâ”€â”€ Set VECTOR\_DB=qdrant|weaviate|milvus|chromadb|pgvector  
  â”œâ”€â”€ Write to master.env  
  â””â”€â”€ If pgvector: set flag PGVECTOR\_ENABLED=true (handled in postgres setup)

### **Docker Compose Blocks (Script 2 generates based on selection)**

**Qdrant:**

qdrant:  
  image: qdrant/qdrant:latest  
  container\_name: qdrant  
  restart: unless-stopped  
  ports:  
    \- "127.0.0.1:6333:6333"  
    \- "127.0.0.1:6334:6334"  
  volumes:  
    \- ${DATA\_DIR}/qdrant/storage:/qdrant/storage  
    \- ${DATA\_DIR}/qdrant/snapshots:/qdrant/snapshots  
  environment:  
    \- QDRANT\_\_SERVICE\_\_API\_KEY=${QDRANT\_API\_KEY}  
  networks:  
    \- backend  
  healthcheck:  
    test: \["CMD", "curl", "-f", "http://localhost:6333/readyz"\]  
    interval: 30s  
    timeout: 10s  
    retries: 5

**Weaviate:**

weaviate:  
  image: semitechnologies/weaviate:latest  
  container\_name: weaviate  
  restart: unless-stopped  
  ports:  
    \- "127.0.0.1:8080:8080"  
  volumes:  
    \- ${DATA\_DIR}/weaviate:/var/lib/weaviate  
  environment:  
    \- QUERY\_DEFAULTS\_LIMIT=25  
    \- AUTHENTICATION\_APIKEY\_ENABLED=true  
    \- AUTHENTICATION\_APIKEY\_ALLOWED\_KEYS=${WEAVIATE\_API\_KEY}  
    \- PERSISTENCE\_DATA\_PATH=/var/lib/weaviate  
    \- DEFAULT\_VECTORIZER\_MODULE=none  
    \- CLUSTER\_HOSTNAME=node1  
  networks:  
    \- backend  
  healthcheck:  
    test: \["CMD", "curl", "-f", "http://localhost:8080/v1/.well-known/ready"\]  
    interval: 30s  
    timeout: 10s  
    retries: 5

**Milvus:**

milvus-etcd:  
  image: quay.io/coreos/etcd:v3.5.11  
  container\_name: milvus-etcd  
  restart: unless-stopped  
  environment:  
    \- ETCD\_AUTO\_COMPACTION\_MODE=revision  
    \- ETCD\_AUTO\_COMPACTION\_RETENTION=1000  
    \- ETCD\_QUOTA\_BACKEND\_BYTES=4294967296  
    \- ETCD\_SNAPSHOT\_COUNT=50000  
  volumes:  
    \- ${DATA\_DIR}/milvus/etcd:/etcd  
  command: etcd \-advertise-client-urls=http://127.0.0.1:2379 \-listen-client-urls http://0.0.0.0:2379 \--data-dir /etcd  
  networks:  
    \- backend

milvus-minio:  
  image: minio/minio:latest  
  container\_name: milvus-minio  
  restart: unless-stopped  
  environment:  
    \- MINIO\_ACCESS\_KEY=${MINIO\_ACCESS\_KEY}  
    \- MINIO\_SECRET\_KEY=${MINIO\_SECRET\_KEY}  
  volumes:  
    \- ${DATA\_DIR}/milvus/minio:/minio\_data  
  command: minio server /minio\_data \--console-address ":9001"  
  networks:  
    \- backend

milvus:  
  image: milvusdb/milvus:latest  
  container\_name: milvus  
  restart: unless-stopped  
  ports:  
    \- "127.0.0.1:19530:19530"  
    \- "127.0.0.1:9091:9091"  
  volumes:  
    \- ${DATA\_DIR}/milvus/data:/var/lib/milvus  
  environment:  
    \- ETCD\_ENDPOINTS=milvus-etcd:2379  
    \- MINIO\_ADDRESS=milvus-minio:9000  
  depends\_on:  
    \- milvus-etcd  
    \- milvus-minio  
  networks:  
    \- backend  
  healthcheck:  
    test: \["CMD", "curl", "-f", "http://localhost:9091/healthz"\]  
    interval: 30s  
    timeout: 10s  
    retries: 5

**ChromaDB:**

chromadb:  
  image: chromadb/chroma:latest  
  container\_name: chromadb  
  restart: unless-stopped  
  ports:  
    \- "127.0.0.1:8000:8000"  
  volumes:  
    \- ${DATA\_DIR}/chromadb:/chroma/chroma  
  environment:  
    \- CHROMA\_SERVER\_AUTH\_CREDENTIALS=${CHROMA\_API\_KEY}  
    \- CHROMA\_SERVER\_AUTH\_PROVIDER=chromadb.auth.token.TokenAuthServerProvider  
    \- IS\_PERSISTENT=TRUE  
    \- ANONYMIZED\_TELEMETRY=FALSE  
  networks:  
    \- backend  
  healthcheck:  
    test: \["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"\]  
    interval: 30s  
    timeout: 10s  
    retries: 5

**pgvector (extension on existing Postgres):**

No separate container. During postgres init:  
  CREATE EXTENSION IF NOT EXISTS vector;  
  CREATE EXTENSION IF NOT EXISTS pg\_trgm;

### **Service â†” Vector DB Wiring**

Script 3 configures each service based on `$VECTOR_DB`:

Copy table

| Service | Qdrant Config | Weaviate Config | ChromaDB Config | pgvector Config |
| ----- | ----- | ----- | ----- | ----- |
| Dify | VECTOR\_STORE=qdrant, QDRANT\_URL=[http://qdrant:6333](http://qdrant:6333) | VECTOR\_STORE=weaviate, WEAVIATE\_ENDPOINT=[http://weaviate:8080](http://weaviate:8080) | VECTOR\_STORE=chroma, CHROMA\_HOST=chromadb | VECTOR\_STORE=pgvector (uses internal pg) |
| AnythingLLM | Vector DB â†’ Qdrant, URL=[http://qdrant:6333](http://qdrant:6333) | Vector DB â†’ Weaviate | Vector DB â†’ Chroma | Vector DB â†’ pgvector |
| Flowise | Qdrant node | Weaviate node | Chroma node | pgvector node |
| OpenClaw | vectordb.type=qdrant | vectordb.type=weaviate | vectordb.type=chroma | vectordb.type=pgvector |

### **OpenClaw Integration Note**

OpenClaw connects to whichever vector DB is deployed. Its config is templated:

\# $CONFIG\_DIR/openclaw/config.yaml (generated by Script 3\)  
llm:  
  provider: openai-compatible  
  base\_url: http://litellm:4000/v1  
  api\_key: ${LITELLM\_MASTER\_KEY}

vectordb:  
  type: ${VECTOR\_DB}          \# qdrant | weaviate | chromadb | pgvector | milvus  
  host: ${VECTOR\_DB\_HOST}     \# resolved from VECTOR\_DB selection  
  port: ${VECTOR\_DB\_PORT}     \# resolved from VECTOR\_DB selection  
  api\_key: ${VECTOR\_DB\_KEY}   \# resolved from VECTOR\_DB selection

embedding:  
  model: text-embedding  
  base\_url: http://litellm:4000/v1

---

## **11\. Google Drive Rsync â€” Authentication & Embedding Pipeline**

### **Purpose**

Sync documents from Google Drive to `/mnt/data/gdrive/` on the EC2 instance, then feed them into the vector database for RAG (Retrieval-Augmented Generation) across all platform services.

### **Tool: rclone**

rclone is the standard tool for Google Drive â†” Linux sync. Installed in Script 1\.

### **Installation (Script 1\)**

install\_rclone()  
  â”œâ”€â”€ curl https://rclone.org/install.sh | bash  
  â”œâ”€â”€ Verify: rclone version  
  â”œâ”€â”€ Create dirs:  
  â”‚     mkdir \-p /mnt/data/gdrive  
  â”‚     mkdir \-p $CONFIG\_DIR/rclone  
  â””â”€â”€ Log: "rclone installed â€” configure with 'rclone config' or provide rclone.conf"

### **Authentication Options**

**Option A: Interactive (recommended for first setup)**

rclone config  
  â”œâ”€â”€ n (new remote)  
  â”œâ”€â”€ name: gdrive  
  â”œâ”€â”€ storage: drive (Google Drive)  
  â”œâ”€â”€ client\_id: (leave blank for default, or use own OAuth app)  
  â”œâ”€â”€ client\_secret: (leave blank for default)  
  â”œâ”€â”€ scope: drive.readonly  
  â”œâ”€â”€ root\_folder\_id: (blank \= entire drive, or specific folder ID)  
  â”œâ”€â”€ service\_account\_file: (blank unless using service account)  
  â””â”€â”€ Auto config:   
        If headless (EC2): n â†’ gives URL to visit on local machine  
        Visit URL â†’ authorize â†’ paste code back into terminal

**Option B: Service Account (recommended for automation)**

setup\_gdrive\_service\_account()  
  â”œâ”€â”€ Requires: Google Cloud project \+ service account JSON key  
  â”œâ”€â”€ Place key at: $CONFIG\_DIR/rclone/gdrive-service-account.json  
  â”œâ”€â”€ Generate rclone.conf:  
  â”‚     \[gdrive\]  
  â”‚     type \= drive  
  â”‚     scope \= drive.readonly  
  â”‚     service\_account\_file \= /opt/ai-platform/config/rclone/gdrive-service-account.json  
  â”‚     team\_drive \=           (blank unless using Shared Drive)  
  â”œâ”€â”€ Write to  $ CONFIG\_DIR/rclone/rclone.conf  
  â””â”€â”€ Test: rclone lsd gdrive: \--config= $ CONFIG\_DIR/rclone/rclone.conf

**Option C: Pre-existing rclone.conf**

If  $ CONFIG\_DIR/rclone/rclone.conf already exists:  
  â”œâ”€â”€ Validate: rclone lsd gdrive: \--config= $ CONFIG\_DIR/rclone/rclone.conf  
  â”œâ”€â”€ If valid: skip auth setup  
  â””â”€â”€ If invalid: prompt for re-auth

### **Sync Configuration (Script 3\)**

configure\_gdrive\_sync()  
  â”œâ”€â”€ Read GDRIVE\_SYNC\_ENABLED from master.env (set in Script 1\)  
  â”œâ”€â”€ If disabled: skip entirely  
  â”œâ”€â”€ Read GDRIVE\_FOLDERS from master.env  
  â”‚     Default: "" (sync entire drive)  
  â”‚     Or comma-separated: "Documents/AI,Projects/RAG-data,Research"  
  â”‚  
  â”œâ”€â”€ Generate sync script: /opt/ai-platform/scripts/sync-gdrive.sh  
  â”‚     \#\!/bin/bash  
  â”‚     RCLONE\_CONF=/opt/ai-platform/config/rclone/rclone.conf  
  â”‚     DEST=/mnt/data/gdrive  
  â”‚     LOG=/var/log/ai-platform/gdrive-sync.log  
  â”‚       
  â”‚     echo "\[$(date)\] Starting Google Drive sync" \>\>  $ LOG  
  â”‚       
  â”‚     if \[ \-z " $ GDRIVE\_FOLDERS" \]; then  
  â”‚       rclone sync gdrive:  $ DEST \\  
  â”‚         \--config= $ RCLONE\_CONF \\  
  â”‚         \--transfers=4 \\  
  â”‚         \--checkers=8 \\  
  â”‚         \--log-file= $ LOG \\  
  â”‚         \--log-level=INFO \\  
  â”‚         \--exclude=".Trash-\*/\*\*" \\  
  â”‚         \--exclude="\*.tmp" \\  
  â”‚         \--max-size=100M \\  
  â”‚         \--drive-acknowledge-abuse  
  â”‚     else  
  â”‚       IFS=',' read \-ra FOLDERS \<\<\< " $ GDRIVE\_FOLDERS"  
  â”‚       for folder in "${FOLDERS\[@\]}"; do  
  â”‚         folder= $ (echo " $ folder" | xargs)  \# trim whitespace  
  â”‚         rclone sync "gdrive: $ folder" " $ DEST/ $ folder" \\  
  â”‚           \--config= $ RCLONE\_CONF \\  
  â”‚           \--transfers=4 \\  
  â”‚           \--checkers=8 \\  
  â”‚           \--log-file= $ LOG \\  
  â”‚           \--log-level=INFO \\  
  â”‚           \--max-size=100M \\  
  â”‚           \--drive-acknowledge-abuse  
  â”‚       done  
  â”‚     fi  
  â”‚       
  â”‚     echo "\[ $ (date)\] Sync complete. Files:" \>\> $LOG  
  â”‚     find $DEST \-type f | wc \-l \>\>  $ LOG  
  â”‚       
  â”‚     \# Trigger embedding pipeline if configured  
  â”‚     if \[ " $ AUTO\_EMBED\_AFTER\_SYNC" \= "true" \]; then  
  â”‚       /opt/ai-platform/scripts/embed-documents.sh  
  â”‚     fi  
  â”‚  
  â”œâ”€â”€ chmod \+x /opt/ai-platform/scripts/sync-gdrive.sh  
  â”‚  
  â”œâ”€â”€ Create systemd timer for scheduled sync:  
  â”‚     /etc/systemd/system/gdrive-sync.service  
  â”‚       \[Unit\]  
  â”‚       Description=Google Drive Sync  
  â”‚       \[Service\]  
  â”‚       Type=oneshot  
  â”‚       ExecStart=/opt/ai-platform/scripts/sync-gdrive.sh  
  â”‚       User=root  
  â”‚       EnvironmentFile=/opt/ai-platform/env/master.env  
  â”‚  
  â”‚     /etc/systemd/system/gdrive-sync.timer  
  â”‚       \[Unit\]  
  â”‚       Description=Google Drive Sync Timer  
  â”‚       \[Timer\]  
  â”‚       OnCalendar=\*-\*-\* 02:00:00    \# Daily at 2 AM  
  â”‚       Persistent=true  
  â”‚       \[Install\]  
  â”‚       WantedBy=timers.target  
  â”‚  
  â”œâ”€â”€ systemctl daemon-reload  
  â”œâ”€â”€ systemctl enable \--now gdrive-sync.timer  
  â””â”€â”€ Run initial sync: systemctl start gdrive-sync.service

### **Embedding Pipeline**

After sync, documents need to be vectorized and inserted into the selected vector DB.

generate\_embed\_script()  
  â”œâ”€â”€ Generate /opt/ai-platform/scripts/embed-documents.sh:  
  â”‚  
  â”‚     \#\!/bin/bash  
  â”‚     source /opt/ai-platform/env/master.env  
  â”‚       
  â”‚     GDRIVE\_DIR=/mnt/data/gdrive  
  â”‚     PROCESSED\_LOG=/mnt/data/gdrive/.processed\_files  
  â”‚     LOG=/var/log/ai-platform/embedding.log  
  â”‚       
  â”‚     touch  $ PROCESSED\_LOG  
  â”‚       
  â”‚     echo "\[ $ (date)\] Starting embedding pipeline" \>\> $LOG  
  â”‚       
  â”‚     \# Find new/modified files since last run  
  â”‚     find $GDRIVE\_DIR \-type f \\  
  â”‚       $  \-name "\*.pdf" \-o \-name "\*.txt" \-o \-name "\*.md" \\  â”‚          \-o \-name "\*.docx" \-o \-name "\*.csv" \-o \-name "\*.json" \\  â”‚          \-o \-name "\*.html" \-o \-name "\*.epub"  $  \\  
  â”‚       \-newer  $ PROCESSED\_LOG \\  
  â”‚       \> /tmp/files\_to\_embed.txt  
  â”‚       
  â”‚     FILE\_COUNT= $ (wc \-l \< /tmp/files\_to\_embed.txt)  
  â”‚     echo "\[$(date)\] Found $FILE\_COUNT new/modified files" \>\>  $ LOG  
  â”‚       
  â”‚     if \[ " $ FILE\_COUNT" \-eq 0 \]; then  
  â”‚       echo "\[$(date)\] No new files to embed" \>\>  $ LOG  
  â”‚       exit 0  
  â”‚     fi  
  â”‚       
  â”‚     \# Method depends on which services are running:  
  â”‚     \# Priority: Dify API \> AnythingLLM API \> Direct script  
  â”‚       
  â”‚     if docker ps \--format '{{.Names}}' | grep \-q "^dify-api $ "; then  
  â”‚       \# Use Dify's document API  
  â”‚       echo "\[$(date)\] Embedding via Dify API" \>\>  $ LOG  
  â”‚       while IFS= read \-r file; do  
  â”‚         curl \-s \-X POST "http://localhost:5001/v1/datasets/ $ {DIFY\_KNOWLEDGE\_DATASET\_ID}/document/create\_by\_file" \\  
  â”‚           \-H "Authorization: Bearer ${DIFY\_API\_KEY}" \\  
  â”‚           \-F "file=@$file" \\  
  â”‚           \-F 'data={"indexing\_technique":"high\_quality","process\_rule":{"mode":"automatic"}}' \\  
  â”‚           \>\>  $ LOG 2\>&1  
  â”‚       done \< /tmp/files\_to\_embed.txt  
  â”‚       
  â”‚     elif docker ps \--format '{{.Names}}' | grep \-q "^anythingllm $ "; then  
  â”‚       \# Use AnythingLLM's document API  
  â”‚       echo "\[$(date)\] Embedding via AnythingLLM API" \>\> $LOG  
  â”‚       while IFS= read \-r file; do  
  â”‚         curl \-s \-X POST "http://localhost:3001/api/v1/document/upload" \\  
  â”‚           \-H "Authorization: Bearer ${ANYTHINGLLM\_API\_KEY}" \\  
  â”‚           \-F "file=@$file" \\  
  â”‚           \>\>  $ LOG 2\>&1  
  â”‚       done \< /tmp/files\_to\_embed.txt  
  â”‚       
  â”‚     else  
  â”‚       \# Direct embedding via LiteLLM \+ vector DB API  
  â”‚       echo "\[ $ (date)\] Direct embedding via LiteLLM â†’ ${VECTOR\_DB}" \>\> $LOG  
  â”‚       python3 /opt/ai-platform/scripts/direct-embed.py \\  
  â”‚         \--files /tmp/files\_to\_embed.txt \\  
  â”‚         \--litellm-url http://localhost:4000/v1 \\  
  â”‚         \--litellm-key $LITELLM\_MASTER\_KEY \\  
  â”‚         \--vector-db $VECTOR\_DB \\  
  â”‚         \--vector-db-url  $ VECTOR\_DB\_HOST: $ VECTOR\_DB\_PORT \\  
  â”‚         \--vector-db-key $VECTOR\_DB\_KEY \\  
  â”‚         \>\> $LOG 2\>&1  
  â”‚     fi  
  â”‚       
  â”‚     \# Update processed timestamp  
  â”‚     touch  $ PROCESSED\_LOG  
  â”‚     echo "\[ $ (date)\] Embedding pipeline complete" \>\> $LOG  
  â”‚  
  â””â”€â”€ chmod \+x /opt/ai-platform/scripts/embed-documents.sh

### **Storage Layout**

/mnt/data/gdrive/  
â”œâ”€â”€ .processed\_files              (timestamp marker)  
â”œâ”€â”€ Documents/  
â”‚   â””â”€â”€ AI/  
â”‚       â”œâ”€â”€ research-paper.pdf  
â”‚       â”œâ”€â”€ meeting-notes.md  
â”‚       â””â”€â”€ dataset-docs.txt  
â”œâ”€â”€ Projects/  
â”‚   â””â”€â”€ RAG-data/  
â”‚       â”œâ”€â”€ knowledge-base.json  
â”‚       â””â”€â”€ faq.csv  
â””â”€â”€ Research/  
    â”œâ”€â”€ arxiv-papers/  
    â””â”€â”€ notes/

### **Manual Operations**

\# Manual sync (immediate)  
sudo systemctl start gdrive-sync.service

\# Manual embed (immediate)    
sudo /opt/ai-platform/scripts/embed-documents.sh

\# Check sync status  
sudo systemctl status gdrive-sync.timer  
sudo journalctl \-u gdrive-sync.service \-f

\# Check last sync log  
tail \-50 /var/log/ai-platform/gdrive-sync.log

\# Check embedding log  
tail \-50 /var/log/ai-platform/embedding.log

\# Re-embed everything (reset processed marker)  
rm /mnt/data/gdrive/.processed\_files  
sudo /opt/ai-platform/scripts/embed-documents.sh

\# List synced files  
find /mnt/data/gdrive \-type f | head \-50  
du \-sh /mnt/data/gdrive

\---

\#\# 12\. Reverse Proxy â€” Caddy with Auto-SSL

\#\#\# Why Caddy (Not Nginx/Traefik)

\- \*\*Automatic HTTPS\*\* via Let's Encrypt / ZeroSSL â€” zero config  
\- \*\*Automatic certificate renewal\*\* â€” no cron jobs  
\- \*\*Single config file\*\* â€” Caddyfile is human-readable  
\- \*\*HTTP/2 and HTTP/3\*\* out of the box  
\- \*\*On-demand TLS\*\* support for wildcard subdomains

\#\#\# Architecture

Internet â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ EC2 Security Group â”‚ â”‚ Port 80 (â†’ Caddy â†’ auto-redirect 443\) â”‚ â”‚ Port 443 (â†’ Caddy â†’ TLS termination) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Caddy Container (:80, :443) â”‚ â”‚ â”‚ â”‚ dify.yourdomain.com â†’ dify:80 â”‚ â”‚ n8n.yourdomain.com â†’ n8n:5678 â”‚ â”‚ webui.yourdomain.com â†’ open-webui:8080â”‚ â”‚ flowise.yourdomain.com â†’ flowise:3000 â”‚ â”‚ llm.yourdomain.com â†’ litellm:4000 â”‚ â”‚ grafana.yourdomain.com â†’ grafana:3000 â”‚ â”‚ anything.yourdomain.com â†’ anythingllm:3001â”‚ â”‚ openclaw.yourdomain.com â†’ openclaw:PORT â”‚ â”‚ portainer.yourdomain.comâ†’ portainer:9000 â”‚ â”‚ supertokens.yourdomain.com â†’ supertokens:3567â”‚ â”‚ â”‚ â”‚ If no domain: localhost mode (self-signed)â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\#\#\# Domain Configuration (Script 1\)

configure\_domain() â”œâ”€â”€ read \-p "Do you have a domain name pointing to this server? (y/n) \[n\]: " has\_domain â”‚ â”œâ”€â”€ If YES: â”‚ â”œâ”€â”€ read \-p "Base domain (e.g., ai.example.com): " BASE\_DOMAIN â”‚ â”œâ”€â”€ read \-p "Email for Let's Encrypt: " ACME\_EMAIL â”‚ â”œâ”€â”€ Validate domain resolves to this server's public IP: â”‚ â”‚ SERVER\_IP= $ (curl \-s ifconfig.me) â”‚ â”‚ DOMAIN\_IP= (dig+short BASE\_DOMAIN) â”‚ â”‚ If mismatch: warn "Domain does not resolve to this server yet" â”‚ â”œâ”€â”€ Set DOMAIN\_MODE=production â”‚ â”œâ”€â”€ Write to master.env: â”‚ â”‚ BASE\_DOMAIN= $ BASE\_DOMAIN â”‚ â”‚ ACME\_EMAIL=$ACME\_EMAIL â”‚ â”‚ DOMAIN\_MODE=production â”‚ â””â”€â”€ Generate subdomain map (see below) â”‚ â””â”€â”€ If NO: â”œâ”€â”€ Set DOMAIN\_MODE=local â”œâ”€â”€ Write to master.env: â”‚ BASE\_DOMAIN=localhost â”‚ DOMAIN\_MODE=local â””â”€â”€ Caddy will serve on IP with self-signed certs or HTTP only

\#\#\# Subdomain Map Generation

generate\_subdomain\_map() â”œâ”€â”€ Read ENABLED\_SERVICES from master.env â”œâ”€â”€ For each service, assign subdomain: â”‚ DIFY\_DOMAIN=dify.${BASE\_DOMAIN} â”‚ N8N\_DOMAIN=n8n.${BASE\_DOMAIN} â”‚ WEBUI\_DOMAIN=webui.${BASE\_DOMAIN} â”‚ FLOWISE\_DOMAIN=flowise.${BASE\_DOMAIN} â”‚ LITELLM\_DOMAIN=llm.${BASE\_DOMAIN} â”‚ GRAFANA\_DOMAIN=grafana.${BASE\_DOMAIN} â”‚ ANYTHINGLLM\_DOMAIN=anything.${BASE\_DOMAIN} â”‚ OPENCLAW\_DOMAIN=openclaw.${BASE\_DOMAIN} â”‚ PORTAINER\_DOMAIN=portainer.${BASE\_DOMAIN} â”‚ SUPERTOKENS\_DOMAIN=auth.${BASE\_DOMAIN} â”œâ”€â”€ Write all to master.env â””â”€â”€ echo "Configure these DNS records (A or CNAME) pointing to $SERVER\_IP:" For each: echo " $subdomain â†’ $SERVER\_IP"

\#\#\# Caddyfile Generation (Script 2\)

generate\_caddyfile() â”œâ”€â”€ Source master.env â”œâ”€â”€ Start Caddyfile: â”‚ â”‚ \# Global options â”‚ { â”‚ email ${ACME\_EMAIL} â”‚ acme\_ca [https://acme-v02.api.letsencrypt.org/directory](https://acme-v02.api.letsencrypt.org/directory) â”‚ \# For local mode: â”‚ \# local\_certs â”‚ } â”‚ â”œâ”€â”€ If DOMAIN\_MODE=production: â”‚ For each enabled service: â”‚ ${SERVICE\_DOMAIN} { â”‚ reverse\_proxy ${service\_container}:${service\_port} â”‚ encode gzip â”‚ header { â”‚ Strict-Transport-Security "max-age=31536000; includeSubDomains; preload" â”‚ X-Content-Type-Options "nosniff" â”‚ X-Frame-Options "SAMEORIGIN" â”‚ Referrer-Policy "strict-origin-when-cross-origin" â”‚ } â”‚ log { â”‚ output file /var/log/caddy/${service}.log { â”‚ roll\_size 10mb â”‚ roll\_keep 5 â”‚ } â”‚ } â”‚ } â”‚ â”œâ”€â”€ If DOMAIN\_MODE=local: â”‚ :80 { â”‚ \# Landing page with links to all services â”‚ respond "AI Platform Running. Services available at localhost:PORT" â”‚ } â”‚ For each enabled service: â”‚ :${external\_port} { â”‚ reverse\_proxy ${service\_container}:${service\_port} â”‚ } â”‚ â”œâ”€â”€ Add special handling for WebSocket services: â”‚ \# n8n needs WebSocket for workflow editor â”‚ ${N8N\_DOMAIN} { â”‚ reverse\_proxy n8n:5678 { â”‚ header\_up X-Forwarded-Proto {scheme} â”‚ } â”‚ } â”‚ \# Open WebUI uses WebSocket â”‚ ${WEBUI\_DOMAIN} { â”‚ reverse\_proxy open-webui:8080 { â”‚ header\_up Connection {\>Connection} â”‚ header\_up Upgrade {\>Upgrade} â”‚ } â”‚ } â”‚ â””â”€â”€ Write to $CONFIG\_DIR/caddy/Caddyfile

\#\#\# Docker Compose Block

\`\`\`yaml  
caddy:  
  image: caddy:2-alpine  
  container\_name: caddy  
  restart: unless-stopped  
  ports:  
    \- "80:80"  
    \- "443:443"  
    \- "443:443/udp"   \# HTTP/3  
  volumes:  
    \- ${CONFIG\_DIR}/caddy/Caddyfile:/etc/caddy/Caddyfile:ro  
    \- ${DATA\_DIR}/caddy/data:/data          \# TLS certificates  
    \- ${DATA\_DIR}/caddy/config:/config      \# Caddy config state  
    \- ${LOG\_DIR}/caddy:/var/log/caddy       \# Access logs  
  environment:  
    \- ACME\_EMAIL=${ACME\_EMAIL}  
  networks:  
    \- frontend  
    \- backend  
  healthcheck:  
    test: \["CMD", "caddy", "validate", "--config", "/etc/caddy/Caddyfile"\]  
    interval: 60s  
    timeout: 10s  
    retries: 3

### **Local Mode Port Map (no domain)**

When DOMAIN\_MODE=local, services are accessed directly via IP:port:

Copy table

| Service | URL |
| ----- | ----- |
| Dify | [http://SERVER\_IP:3000](http://SERVER_IP:3000) |
| n8n | [http://SERVER\_IP:5678](http://SERVER_IP:5678) |
| Open WebUI | [http://SERVER\_IP:8080](http://SERVER_IP:8080) |
| Flowise | [http://SERVER\_IP:3001](http://SERVER_IP:3001) |
| LiteLLM | [http://SERVER\_IP:4000](http://SERVER_IP:4000) |
| Grafana | [http://SERVER\_IP:3002](http://SERVER_IP:3002) |
| Portainer | [http://SERVER\_IP:9000](http://SERVER_IP:9000) |
| AnythingLLM | [http://SERVER\_IP:3003](http://SERVER_IP:3003) |
| OpenClaw | [http://SERVER\_IP:3004](http://SERVER_IP:3004) |

Note: In local mode, ports are mapped to `0.0.0.0:PORT` instead of `127.0.0.1:PORT` since there's no reverse proxy handling external access. Security group still controls access.

---

## **13\. Monitoring Stack â€” Prometheus, Grafana, cAdvisor, node\_exporter**

### **Purpose**

Full observability of the platform: container health, system resources, GPU utilization, service response times, cost tracking (via LiteLLM), and alerting.

### **Architecture**

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚                    Grafana                           â”‚  
â”‚              (Dashboards & Alerts)                   â”‚  
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  
â”‚    â”‚System    â”‚Docker    â”‚GPU      â”‚LLM Cost  â”‚    â”‚  
â”‚    â”‚Dashboard â”‚Dashboard â”‚Dashboardâ”‚Dashboard â”‚    â”‚  
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â”‚  
â”‚         â”‚          â”‚          â”‚         â”‚           â”‚  
â”‚         â–¼          â–¼          â–¼         â–¼           â”‚  
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  
â”‚    â”‚           Prometheus                     â”‚      â”‚  
â”‚    â”‚    (Scrape â†’ Store â†’ Query)              â”‚      â”‚  
â”‚    â”‚    Retention: 30 days                    â”‚      â”‚  
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  
â”‚         â”‚     â”‚      â”‚      â”‚      â”‚                â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
          â”‚     â”‚      â”‚      â”‚      â”‚  
          â–¼     â–¼      â–¼      â–¼      â–¼  
       node\_  cAdvisor nvidia  litellm  caddy  
       export          dcgm   /metrics  /metrics  
       er               export  
                        er

### **Components**

Copy table

| Component | Image | Purpose | Port |
| ----- | ----- | ----- | ----- |
| Prometheus | prom/prometheus:latest | Time-series metrics store | 9090 |
| Grafana | grafana/grafana:latest | Visualization & alerts | 3002 |
| node\_exporter | prom/node-exporter:latest | Host CPU/RAM/disk/network | 9100 |
| cAdvisor | gcr.io/cadvisor/cadvisor:latest | Per-container metrics | 9080 |
| dcgm-exporter | nvcr.io/nvidia/k8s/dcgm-exporter:latest | GPU metrics (if GPU instance) | 9400 |

### **Prometheus Configuration (Script 2\)**

generate\_prometheus\_config()  
  â”œâ”€â”€ Create $CONFIG\_DIR/prometheus/prometheus.yml:  
  â”‚  
  â”‚   global:  
  â”‚     scrape\_interval: 15s  
  â”‚     evaluation\_interval: 15s  
  â”‚     scrape\_timeout: 10s  
  â”‚  
  â”‚   rule\_files:  
  â”‚     \- /etc/prometheus/alert\_rules.yml  
  â”‚  
  â”‚   alerting:  
  â”‚     alertmanagers:  
  â”‚       \- static\_configs:  
  â”‚           \- targets: \[\]   \# Add alertmanager if needed  
  â”‚  
  â”‚   scrape\_configs:  
  â”‚     \- job\_name: 'prometheus'  
  â”‚       static\_configs:  
  â”‚         \- targets: \['localhost:9090'\]  
  â”‚  
  â”‚     \- job\_name: 'node'  
  â”‚       static\_configs:  
  â”‚         \- targets: \['node-exporter:9100'\]  
  â”‚  
  â”‚     \- job\_name: 'cadvisor'  
  â”‚       static\_configs:  
  â”‚         \- targets: \['cadvisor:8080'\]  
  â”‚  
  â”‚     \- job\_name: 'litellm'  
  â”‚       metrics\_path: /metrics  
  â”‚       static\_configs:  
  â”‚         \- targets: \['litellm:4000'\]  
  â”‚  
  â”‚     \- job\_name: 'caddy'  
  â”‚       static\_configs:  
  â”‚         \- targets: \['caddy:2019'\]   \# Caddy admin API  
  â”‚  
  â”‚   \# Conditional: only if GPU instance  
  â”‚     \- job\_name: 'gpu'  
  â”‚       static\_configs:  
  â”‚         \- targets: \['dcgm-exporter:9400'\]  
  â”‚  
  â”œâ”€â”€ Create $CONFIG\_DIR/prometheus/alert\_rules.yml:  
  â”‚  
  â”‚   groups:  
  â”‚     \- name: platform\_alerts  
  â”‚       rules:  
  â”‚         \- alert: HighCPUUsage  
  â”‚           expr: 100 \- (avg by(instance) (irate(node\_cpu\_seconds\_total{mode="idle"}\[5m\])) \* 100\) \> 85  
  â”‚           for: 5m  
  â”‚           labels:  
  â”‚             severity: warning  
  â”‚           annotations:  
  â”‚             summary: "High CPU usage detected ({{ $value }}%)"  
  â”‚  
  â”‚         \- alert: HighMemoryUsage  
  â”‚           expr: (1 \- node\_memory\_MemAvailable\_bytes / node\_memory\_MemTotal\_bytes) \* 100 \> 90  
  â”‚           for: 5m  
  â”‚           labels:  
  â”‚             severity: critical  
  â”‚           annotations:  
  â”‚             summary: "Memory usage above 90% ({{ $value }}%)"  
  â”‚  
  â”‚         \- alert: DiskSpaceLow  
  â”‚           expr: (1 \- node\_filesystem\_avail\_bytes{mountpoint="/mnt/data"} / node\_filesystem\_size\_bytes{mountpoint="/mnt/data"}) \* 100 \> 85  
  â”‚           for: 10m  
  â”‚           labels:  
  â”‚             severity: warning  
  â”‚           annotations:  
  â”‚             summary: "Disk usage above 85% on /mnt/data"  
  â”‚  
  â”‚         \- alert: ContainerDown  
  â”‚           expr: absent(container\_last\_seen{name=\~"dify-api|n8n|litellm|open-webui"}) \== 1  
  â”‚           for: 2m  
  â”‚           labels:  
  â”‚             severity: critical  
  â”‚           annotations:  
  â”‚             summary: "Container {{ $labels.name }} is down"  
  â”‚  
  â”‚         \- alert: GPUTemperatureHigh  
  â”‚           expr: DCGM\_FI\_DEV\_GPU\_TEMP \> 85  
  â”‚           for: 5m  
  â”‚           labels:  
  â”‚             severity: warning  
  â”‚           annotations:  
  â”‚             summary: "GPU temperature above 85Â°C"  
  â”‚  
  â””â”€â”€ Set permissions: chmod 644 on all config files

### **Grafana Provisioning (Script 2\)**

generate\_grafana\_config()  
  â”œâ”€â”€ Create $CONFIG\_DIR/grafana/provisioning/datasources/prometheus.yml:  
  â”‚  
  â”‚   apiVersion: 1  
  â”‚   datasources:  
  â”‚     \- name: Prometheus  
  â”‚       type: prometheus  
  â”‚       access: proxy  
  â”‚       url: http://prometheus:9090  
  â”‚       isDefault: true  
  â”‚       editable: false  
  â”‚  
  â”œâ”€â”€ Create $CONFIG\_DIR/grafana/provisioning/dashboards/dashboards.yml:  
  â”‚  
  â”‚   apiVersion: 1  
  â”‚   providers:  
  â”‚     \- name: 'default'  
  â”‚       folder: 'AI Platform'  
  â”‚       type: file  
  â”‚       options:  
  â”‚         path: /var/lib/grafana/dashboards  
  â”‚  
  â”œâ”€â”€ Download/generate dashboard JSONs:  
  â”‚     $CONFIG\_DIR/grafana/dashboards/  
  â”‚     â”œâ”€â”€ system-overview.json      (node\_exporter metrics)  
  â”‚     â”œâ”€â”€ docker-containers.json    (cAdvisor metrics)  
  â”‚     â”œâ”€â”€ gpu-metrics.json          (DCGM metrics â€” if GPU)  
  â”‚     â”œâ”€â”€ litellm-costs.json        (LiteLLM token/cost tracking)  
  â”‚     â””â”€â”€ platform-health.json      (composite service health)  
  â”‚  
  â””â”€â”€ Generate grafana.ini overrides:  
        $CONFIG\_DIR/grafana/grafana.ini:  
          \[security\]  
          admin\_user \= ${GRAFANA\_ADMIN\_USER}  
          admin\_password \= ${GRAFANA\_ADMIN\_PASSWORD}  
          \[server\]  
          root\_url \= https://${GRAFANA\_DOMAIN}  
          \[auth.anonymous\]  
          enabled \= false

### **Docker Compose Blocks**

prometheus:  
  image: prom/prometheus:latest  
  container\_name: prometheus  
  restart: unless-stopped  
  ports:  
    \- "127.0.0.1:9090:9090"  
  volumes:  
    \- ${CONFIG\_DIR}/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro  
    \- ${CONFIG\_DIR}/prometheus/alert\_rules.yml:/etc/prometheus/alert\_rules.yml:ro  
    \- ${DATA\_DIR}/prometheus:/prometheus  
  command:  
    \- '--config.file=/etc/prometheus/prometheus.yml'  
    \- '--storage.tsdb.path=/prometheus'  
    \- '--storage.tsdb.retention.time=30d'  
    \- '--web.console.libraries=/etc/prometheus/console\_libraries'  
    \- '--web.console.templates=/etc/prometheus/consoles'  
    \- '--web.enable-lifecycle'  
  networks:  
    \- monitoring  
    \- backend  
  healthcheck:  
    test: \["CMD", "wget", "--tries=1", "--spider", "http://localhost:9090/-/healthy"\]  
    interval: 30s  
    timeout: 10s  
    retries: 3

grafana:  
  image: grafana/grafana:latest  
  container\_name: grafana  
  restart: unless-stopped  
  ports:  
    \- "127.0.0.1:3002:3000"  
  volumes:  
    \- ${DATA\_DIR}/grafana:/var/lib/grafana  
    \- ${CONFIG\_DIR}/grafana/provisioning:/etc/grafana/provisioning:ro  
    \- ${CONFIG\_DIR}/grafana/dashboards:/var/lib/grafana/dashboards:ro  
    \- ${CONFIG\_DIR}/grafana/grafana.ini:/etc/grafana/grafana.ini:ro  
  environment:  
    \- GF\_SECURITY\_ADMIN\_USER=${GRAFANA\_ADMIN\_USER}  
    \- GF\_SECURITY\_ADMIN\_PASSWORD=${GRAFANA\_ADMIN\_PASSWORD}  
    \- GF\_INSTALL\_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource  
  networks:  
    \- monitoring  
    \- frontend  
  depends\_on:  
    prometheus:  
      condition: service\_healthy  
  healthcheck:  
    test: \["CMD", "curl", "-f", "http://localhost:3000/api/health"\]  
    interval: 30s  
    timeout: 10s  
    retries: 5

node-exporter:  
  image: prom/node-exporter:latest  
  container\_name: node-exporter  
  restart: unless-stopped  
  ports:  
    \- "127.0.0.1:9100:9100"  
  volumes:  
    \- /proc:/host/proc:ro  
    \- /sys:/host/sys:ro  
    \- /:/rootfs:ro  
  command:  
    \- '--path.procfs=/host/proc'  
    \- '--path.sysfs=/host/sys'  
    \- '--path.rootfs=/rootfs'  
    \- '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'  
  networks:  
    \- monitoring

cadvisor:  
  image: gcr.io/cadvisor/cadvisor:latest  
  container\_name: cadvisor  
  restart: unless-stopped  
  ports:  
    \- "127.0.0.1:9080:8080"  
  volumes:  
    \- /:/rootfs:ro  
    \- /var/run:/var/run:ro  
    \- /sys:/sys:ro  
    \- /var/lib/docker/:/var/lib/docker:ro  
    \- /dev/disk/:/dev/disk:ro  
  privileged: true  
  devices:  
    \- /dev/kmsg  
  networks:  
    \- monitoring

\# Conditional: only on GPU instances  
dcgm-exporter:  
  image: nvcr.io/nvidia/k8s/dcgm-exporter:3.3.5-3.4.0-ubuntu22.04  
  container\_name: dcgm-exporter  
  restart: unless-stopped  
  ports:  
    \- "127.0.0.1:9400:9400"  
  deploy:  
    resources:  
      reservations:  
        devices:  
          \- driver: nvidia  
            count: all  
            capabilities: \[gpu\]  
  networks:  
    \- monitoring

### **Key Metrics Tracked**

Copy table

| Metric Category | Source | Key Metrics |
| ----- | ----- | ----- |
| System | node\_exporter | CPU%, RAM%, disk I/O, network throughput |
| Containers | cAdvisor | Per-container CPU, RAM, restart count, network |
| GPU | dcgm-exporter | Utilization%, temperature, memory used/free, power draw |
| LLM Usage | LiteLLM | Tokens in/out per model, latency p50/p95/p99, cost per model, errors |
| Proxy | Caddy | Requests/sec, response times, status codes, bytes transferred |

---

## **14\. Script 0 â€” AWS Infrastructure Provisioning**

### **Scope**

Script 0 creates the AWS resources needed BEFORE SSH into the instance. It is the **only script that runs from your local machine** (or CloudShell).

### **Prerequisites**

* AWS CLI v2 installed and configured (`aws configure`)  
* IAM user/role with EC2, EBS, VPC, SecurityGroup, IAM permissions  
* SSH key pair (existing or script creates one)

### **Full Logic Flow**

script\_0\_aws\_provisioner.sh  
â”‚  
â”œâ”€â”€ PHASE 0: Preflight  
â”‚   â”œâ”€â”€ Verify AWS CLI: aws \--version || error "Install AWS CLI first"  
â”‚   â”œâ”€â”€ Verify credentials: aws sts get-caller-identity || error "Run 'aws configure' first"  
â”‚   â”œâ”€â”€ Detect/select region:  
â”‚   â”‚     DEFAULT\_REGION= $ (aws configure get region)  
â”‚   â”‚     read \-p "AWS Region \[ $ DEFAULT\_REGION\]: " REGION  
â”‚   â”‚     REGION=${REGION:-$DEFAULT\_REGION}  
â”‚   â””â”€â”€ Verify region is valid: aws ec2 describe-regions \--region-names  $ REGION  
â”‚  
â”œâ”€â”€ PHASE 1: Configuration Interview  
â”‚   â”œâ”€â”€ Instance sizing:  
â”‚   â”‚     echo "Select instance type:"  
â”‚   â”‚     echo "  1\) g4dn.xlarge    â€” 1x T4 GPU, 4 vCPU, 16 GB  (\~ $ 0.53/hr)"  
â”‚   â”‚     echo "  2\) g4dn.2xlarge   â€” 1x T4 GPU, 8 vCPU, 32 GB  (\~ $ 0.75/hr)"  
â”‚   â”‚     echo "  3\) g5.xlarge      â€” 1x A10G GPU, 4 vCPU, 16 GB (\~ $ 1.01/hr)"  
â”‚   â”‚     echo "  4\) g5.2xlarge     â€” 1x A10G GPU, 8 vCPU, 32 GB (\~ $ 1.21/hr)"  
â”‚   â”‚     echo "  5\) g5.4xlarge     â€” 1x A10G GPU, 16 vCPU, 64 GB(\~ $ 1.62/hr)"  
â”‚   â”‚     echo "  6\) p3.2xlarge     â€” 1x V100 GPU, 8 vCPU, 61 GB (\~ $ 3.06/hr)"  
â”‚   â”‚     echo "  7\) t3.2xlarge     â€” NO GPU, 8 vCPU, 32 GB       (\~ $ 0.33/hr)"  
â”‚   â”‚     echo "  8\) Custom         â€” enter instance type manually"  
â”‚   â”‚     read \-p "Choice \[2\]: " choice  
â”‚   â”‚     Map choice to INSTANCE\_TYPE  
â”‚   â”‚  
â”‚   â”œâ”€â”€ EBS volume sizing:  
â”‚   â”‚     echo "Data volume size (for models, databases, documents):"  
â”‚   â”‚     echo "  Minimum: 100 GB (lightweight models only)"  
â”‚   â”‚     echo "  Recommended: 200 GB (standard model set)"  
â”‚   â”‚     echo "  Large: 500 GB (full model set \+ large document corpus)"  
â”‚   â”‚     read \-p "Data volume size in GB \[200\]: " EBS\_SIZE  
â”‚   â”‚     EBS\_SIZE=${EBS\_SIZE:-200}  
â”‚   â”‚     EBS\_TYPE=gp3  \# Always gp3 for cost efficiency  
â”‚   â”‚  
â”‚   â”œâ”€â”€ SSH key:  
â”‚   â”‚     \# Check for existing key pairs  
â”‚   â”‚     EXISTING\_KEYS=$(aws ec2 describe-key-pairs \--query 'KeyPairs\[\].KeyName' \--output text)  
â”‚   â”‚     echo "Existing key pairs:  $ EXISTING\_KEYS"  
â”‚   â”‚     echo "  1\) Use existing key pair"  
â”‚   â”‚     echo "  2\) Create new key pair"  
â”‚   â”‚     read \-p "Choice \[1\]: " key\_choice  
â”‚   â”‚     If 1: read \-p "Key pair name: " KEY\_NAME  
â”‚   â”‚     If 2:  
â”‚   â”‚       KEY\_NAME="ai-platform- $ (date \+%Y%m%d)"  
â”‚   â”‚       aws ec2 create-key-pair \--key-name  $ KEY\_NAME \\  
â”‚   â”‚         \--query 'KeyMaterial' \--output text \> \~/.ssh/ $ {KEY\_NAME}.pem  
â”‚   â”‚       chmod 400 \~/.ssh/${KEY\_NAME}.pem  
â”‚   â”‚       echo "Key saved to \~/.ssh/${KEY\_NAME}.pem"  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Network:  
â”‚   â”‚     echo "Network configuration:"  
â”‚   â”‚     echo "  1\) Default VPC (simplest â€” recommended)"  
â”‚   â”‚     echo "  2\) Specify VPC and subnet"  
â”‚   â”‚     read \-p "Choice \[1\]: " net\_choice  
â”‚   â”‚     If 1:  
â”‚   â”‚       VPC\_ID= $ (aws ec2 describe-vpcs \--filters "Name=is-default,Values=true" \\  
â”‚   â”‚         \--query 'Vpcs\[0\].VpcId' \--output text)  
â”‚   â”‚       SUBNET\_ID= $ (aws ec2 describe-subnets \--filters "Name=vpc-id,Values= $ VPC\_ID" \\  
â”‚   â”‚         \--query 'Subnets\[0\].SubnetId' \--output text)  
â”‚   â”‚     If 2:  
â”‚   â”‚       read \-p "VPC ID: " VPC\_ID  
â”‚   â”‚       read \-p "Subnet ID: " SUBNET\_ID  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Access control:  
â”‚   â”‚     MY\_IP= $ (curl \-s ifconfig.me)  
â”‚   â”‚     echo "Your current public IP:  $ MY\_IP"  
â”‚   â”‚     echo "SSH access:"  
â”‚   â”‚     echo "  1\) My IP only ( $ MY\_IP/32) â€” recommended"  
â”‚   â”‚     echo "  2\) Specific CIDR range"  
â”‚   â”‚     echo "  3\) Open to all (0.0.0.0/0) â€” NOT recommended"  
â”‚   â”‚     read \-p "Choice \[1\]: " ssh\_choice  
â”‚   â”‚     Map to SSH\_CIDR  
â”‚   â”‚       
â”‚   â”‚     echo "Web access (ports 80/443):"  
â”‚   â”‚     echo "  1\) Open to all (0.0.0.0/0) â€” standard for web services"  
â”‚   â”‚     echo "  2\) My IP only ( $ MY\_IP/32)"  
â”‚   â”‚     echo "  3\) Specific CIDR range"  
â”‚   â”‚     read \-p "Choice \[1\]: " web\_choice  
â”‚   â”‚     Map to WEB\_CIDR  
â”‚   â”‚  
â”‚   â””â”€â”€ Naming:  
â”‚       read \-p "Instance name tag \[ai-platform\]: " INSTANCE\_NAME  
â”‚       INSTANCE\_NAME= $ {INSTANCE\_NAME:-ai-platform}  
â”‚  
â”œâ”€â”€ PHASE 2: AMI Selection  
â”‚   â”œâ”€â”€ Determine AMI:  
â”‚   â”‚     If GPU instance (g4\*, g5\*, p3\*):  
â”‚   â”‚       \# Use NVIDIA Deep Learning AMI (has CUDA \+ drivers pre-installed)  
â”‚   â”‚       AMI\_ID= $ (aws ec2 describe-images \\  
â”‚   â”‚         \--owners amazon \\  
â”‚   â”‚         \--filters \\  
â”‚   â”‚           "Name=name,Values=Deep Learning AMI GPU PyTorch\*Ubuntu 22.04\*" \\  
â”‚   â”‚           "Name=state,Values=available" \\  
â”‚   â”‚         \--query 'sort\_by(Images, \&CreationDate)\[-1\].ImageId' \\  
â”‚   â”‚         \--output text)  
â”‚   â”‚         
â”‚   â”‚       If AMI\_ID is empty or "None":  
â”‚   â”‚         \# Fallback: Ubuntu 22.04 (will install CUDA in Script 1\)  
â”‚   â”‚         AMI\_ID= $ (aws ec2 describe-images \\  
â”‚   â”‚           \--owners 099720109477 \\  
â”‚   â”‚           \--filters \\  
â”‚   â”‚             "Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-\*" \\  
â”‚   â”‚             "Name=state,Values=available" \\  
â”‚   â”‚           \--query 'sort\_by(Images, \&CreationDate)\[-1\].ImageId' \\  
â”‚   â”‚           \--output text)  
â”‚   â”‚         GPU\_DRIVERS\_NEEDED=true  
â”‚   â”‚  
â”‚   â”‚     If non-GPU instance:  
â”‚   â”‚       AMI\_ID=$(aws ec2 describe-images \\  
â”‚   â”‚         \--owners 099720109477 \\  
â”‚   â”‚         \--filters \\  
â”‚   â”‚           "Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-\*" \\  
â”‚   â”‚           "Name=state,Values=available" \\  
â”‚   â”‚         \--query 'sort\_by(Images, \&CreationDate)\[-1\].ImageId' \\  
â”‚   â”‚         \--output text)  
â”‚   â”‚       GPU\_DRIVERS\_NEEDED=false  
â”‚   â”‚  
â”‚   â””â”€â”€ echo "Selected AMI:  $ AMI\_ID"  
â”‚  
â”œâ”€â”€ PHASE 3: Security Group  
â”‚   â”œâ”€â”€ SG\_NAME="ai-platform-sg- $ (date \+%Y%m%d%H%M)"  
â”‚   â”œâ”€â”€ SG\_ID= $ (aws ec2 create-security-group \\  
â”‚   â”‚     \--group-name " $ SG\_NAME" \\  
â”‚   â”‚     \--description "AI Platform security group" \\  
â”‚   â”‚     \--vpc-id $VPC\_ID \\  
â”‚   â”‚     \--query 'GroupId' \--output text)  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Ingress rules:  
â”‚   â”‚     \# SSH  
â”‚   â”‚     aws ec2 authorize-security-group-ingress \--group-id $SG\_ID \\  
â”‚   â”‚       \--protocol tcp \--port 22 \--cidr $SSH\_CIDR  
â”‚   â”‚     \# HTTP  
â”‚   â”‚     aws ec2 authorize-security-group-ingress \--group-id $SG\_ID \\  
â”‚   â”‚       \--protocol tcp \--port 80 \--cidr $WEB\_CIDR  
â”‚   â”‚     \# HTTPS  
â”‚   â”‚     aws ec2 authorize-security-group-ingress \--group-id $SG\_ID \\  
â”‚   â”‚       \--protocol tcp \--port 443 \--cidr $WEB\_CIDR  
â”‚   â”‚     \# HTTPS UDP (HTTP/3)  
â”‚   â”‚     aws ec2 authorize-security-group-ingress \--group-id $SG\_ID \\  
â”‚   â”‚       \--protocol udp \--port 443 \--cidr $WEB\_CIDR  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Egress: default (all outbound allowed)  
â”‚   â””â”€â”€ Tag: aws ec2 create-tags \--resources  $ SG\_ID \--tags Key=Name,Value= $ SG\_NAME  
â”‚  
â”œâ”€â”€ PHASE 4: IAM Instance Profile (for S3 backup access)  
â”‚   â”œâ”€â”€ Create IAM role:  
â”‚   â”‚     ROLE\_NAME="ai-platform-ec2-role"  
â”‚   â”‚     aws iam create-role \--role-name  $ ROLE\_NAME \\  
â”‚   â”‚       \--assume-role-policy-document '{  
â”‚   â”‚         "Version": "2012-10-17",  
â”‚   â”‚         "Statement": \[{  
â”‚   â”‚           "Effect": "Allow",  
â”‚   â”‚           "Principal": {"Service": "ec2.amazonaws.com"},  
â”‚   â”‚           "Action": "sts:AssumeRole"  
â”‚   â”‚         }\]  
â”‚   â”‚       }'  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Attach S3 policy (limited to platform bucket):  
â”‚   â”‚     BUCKET\_NAME="ai-platform-backups- $ (aws sts get-caller-identity \--query Account \--output text)"  
â”‚   â”‚     aws iam put-role-policy \--role-name  $ ROLE\_NAME \\  
â”‚   â”‚       \--policy-name ai-platform-s3 \\  
â”‚   â”‚       \--policy-document '{  
â”‚   â”‚         "Version": "2012-10-17",  
â”‚   â”‚         "Statement": \[{  
â”‚   â”‚           "Effect": "Allow",  
â”‚   â”‚           "Action": \["s3:PutObject","s3:GetObject","s3:ListBucket","s3:DeleteObject"\],  
â”‚   â”‚           "Resource": \[  
â”‚   â”‚             "arn:aws:s3:::'" $ BUCKET\_NAME"'",  
â”‚   â”‚             "arn:aws:s3:::'"$BUCKET\_NAME"'/\*"  
â”‚   â”‚           \]  
â”‚   â”‚         }\]  
â”‚   â”‚       }'  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Create instance profile:  
â”‚   â”‚     aws iam create-instance-profile \--instance-profile-name $ROLE\_NAME  
â”‚   â”‚     aws iam add-role-to-instance-profile \\  
â”‚   â”‚       \--instance-profile-name $ROLE\_NAME \--role-name  $ ROLE\_NAME  
â”‚   â”‚     sleep 10   \# Wait for IAM propagation  
â”‚   â”‚  
â”‚   â””â”€â”€ Create S3 bucket:  
â”‚         aws s3 mb s3:// $ BUCKET\_NAME \--region $REGION  
â”‚         aws s3api put-bucket-encryption \--bucket  $ BUCKET\_NAME \\  
â”‚           \--server-side-encryption-configuration '{  
â”‚             "Rules": \[{"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}\]  
â”‚           }'  
â”‚  
â”œâ”€â”€ PHASE 5: Launch Instance  
â”‚   â”œâ”€â”€ Generate user-data script (cloud-init):  
â”‚   â”‚     USER\_DATA= $ (cat \<\<'USERDATA'  
â”‚   â”‚     \#\!/bin/bash  
â”‚   â”‚     \# Minimal bootstrap â€” just enough for Script 1  
â”‚   â”‚     apt-get update \-qq  
â”‚   â”‚     apt-get install \-y \-qq git curl wget  
â”‚   â”‚       
â”‚   â”‚     \# Clone the platform repo  
â”‚   â”‚     git clone https://github.com/YOUR\_REPO/ai-platform.git /opt/ai-platform-setup  
â”‚   â”‚       
â”‚   â”‚     \# Signal that instance is ready  
â”‚   â”‚     touch /tmp/cloud-init-complete  
â”‚   â”‚     USERDATA  
â”‚   â”‚     )  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Launch:  
â”‚   â”‚     INSTANCE\_ID=$(aws ec2 run-instances \\  
â”‚   â”‚       \--image-id $AMI\_ID \\  
â”‚   â”‚       \--instance-type $INSTANCE\_TYPE \\  
â”‚   â”‚       \--key-name $KEY\_NAME \\  
â”‚   â”‚       \--security-group-ids $SG\_ID \\  
â”‚   â”‚       \--subnet-id  $ SUBNET\_ID \\  
â”‚   â”‚       \--iam-instance-profile Name= $ ROLE\_NAME \\  
â”‚   â”‚       \--block-device-mappings "\[  
â”‚   â”‚         {\\"DeviceName\\":\\"/dev/sda1\\",\\"Ebs\\":{\\"VolumeSize\\":50,\\"VolumeType\\":\\"gp3\\",\\"DeleteOnTermination\\":true}},  
â”‚   â”‚         {\\"DeviceName\\":\\"/dev/sdf\\",\\"Ebs\\":{\\"VolumeSize\\":${EBS\_SIZE},\\"VolumeType\\":\\"gp3\\",\\"DeleteOnTermination\\":false,\\"Iops\\":3000,\\"Throughput\\":125}}  
â”‚   â”‚       \]" \\  
â”‚   â”‚       \--tag-specifications "ResourceType=instance,Tags=\[  
â”‚   â”‚         {Key=Name,Value= $ INSTANCE\_NAME},  
â”‚   â”‚         {Key=Project,Value=ai-platform},  
â”‚   â”‚         {Key=ManagedBy,Value=script-0}  
â”‚   â”‚       \]" \\  
â”‚   â”‚       \--user-data " $ USER\_DATA" \\  
â”‚   â”‚       \--query 'Instances\[0\].InstanceId' \\  
â”‚   â”‚       \--output text)  
â”‚   â”‚  
â”‚   â”œâ”€â”€ echo "Instance launching: $INSTANCE\_ID"  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Wait for running:  
â”‚   â”‚     aws ec2 wait instance-running \--instance-ids  $ INSTANCE\_ID  
â”‚   â”‚     echo "Instance is running"  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Get public IP:  
â”‚   â”‚     PUBLIC\_IP= $ (aws ec2 describe-instances \--instance-ids  $ INSTANCE\_ID \\  
â”‚   â”‚       \--query 'Reservations\[0\].Instances\[0\].PublicIpAddress' \--output text)  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Tag EBS data volume:  
â”‚   â”‚     DATA\_VOL\_ID= $ (aws ec2 describe-volumes \\  
â”‚   â”‚       \--filters "Name=attachment.instance-id,Values=$INSTANCE\_ID" \\  
â”‚   â”‚                 "Name=attachment.device,Values=/dev/sdf" \\  
â”‚   â”‚       \--query 'Volumes\[0\].VolumeId' \--output text)  
â”‚   â”‚     aws ec2 create-tags \--resources  $ DATA\_VOL\_ID \\  
â”‚   â”‚       \--tags Key=Name,Value=" $ {INSTANCE\_NAME}-data"  
â”‚   â”‚  
â”‚   â””â”€â”€ Wait for SSH ready:  
â”‚         echo "Waiting for SSH to become available..."  
â”‚         for i in  $ (seq 1 30); do  
â”‚           if ssh \-o StrictHostKeyChecking=no \-o ConnectTimeout=5 \\  
â”‚                \-i \~/.ssh/ $ {KEY\_NAME}.pem ubuntu@$PUBLIC\_IP "echo ready" 2\>/dev/null; then  
â”‚             SSH\_READY=true  
â”‚             break  
â”‚           fi  
â”‚           sleep 10  
â”‚         done  
â”‚  
â”œâ”€â”€ PHASE 6: Generate Connection Info  
â”‚   â”œâ”€â”€ Create local info file: ai-platform-connection.txt  
â”‚   â”‚     \==========================================  
â”‚   â”‚     AI Platform Instance Information  
â”‚   â”‚     \==========================================  
â”‚   â”‚     Instance ID:    $INSTANCE\_ID  
â”‚   â”‚     Instance Type:  $INSTANCE\_TYPE  
â”‚   â”‚     Public IP:      $PUBLIC\_IP  
â”‚   â”‚     Region:         $REGION  
â”‚   â”‚     Key Pair:       $KEY\_NAME  
â”‚   â”‚     Security Group:  $ SG\_ID ( $ SG\_NAME)  
â”‚   â”‚     Data Volume:     $ DATA\_VOL\_ID ( $ {EBS\_SIZE} GB)  
â”‚   â”‚     S3 Bucket:      $BUCKET\_NAME  
â”‚   â”‚     AMI:            $AMI\_ID  
â”‚   â”‚     GPU Drivers:    ${GPU\_DRIVERS\_NEEDED}  
â”‚   â”‚       
â”‚   â”‚     SSH Command:  
â”‚   â”‚       ssh \-i \~/.ssh/${KEY\_NAME}.pem ubuntu@ $ PUBLIC\_IP  
â”‚   â”‚       
â”‚   â”‚     Next Step:  
â”‚   â”‚       1\. SSH into the instance  
â”‚   â”‚       2\. Run: sudo bash /opt/ai-platform-setup/script-1-setup.sh  
â”‚   â”‚     \==========================================  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Display connection info  
â”‚   â””â”€â”€ Optionally copy Script 1 to instance:  
â”‚         scp \-i \~/.ssh/ $ {KEY\_NAME}.pem \\  
â”‚           ./script-1-setup.sh ubuntu@ $ PUBLIC\_IP:/tmp/  
â”‚  
â””â”€â”€ PHASE 7: Optional â€” Elastic IP  
    â”œâ”€â”€ read \-p "Allocate Elastic IP (static IP survives stop/start)? (y/n) \[y\]: " eip\_choice  
    â”œâ”€â”€ If yes:  
    â”‚     EIP\_ALLOC= $ (aws ec2 allocate-address \--domain vpc \--query 'AllocationId' \--output text)  
    â”‚     aws ec2 associate-address \--instance-id $INSTANCE\_ID \--allocation-id  $ EIP\_ALLOC  
    â”‚     STATIC\_IP= $ (aws ec2 describe-addresses \--allocation-ids $EIP\_ALLOC \\  
    â”‚       \--query 'Addresses\[0\].PublicIp' \--output text)  
    â”‚     echo "Elastic IP: $STATIC\_IP (replaces  $ PUBLIC\_IP)"  
    â”‚     PUBLIC\_IP= $ STATIC\_IP  
    â”‚     Update connection info file  
    â””â”€â”€ If no: echo "Warning: Public IP will change on instance stop/start"

### **Estimated Costs**

Copy table

| Resource | Cost |
| ----- | ----- |
| g4dn.2xlarge (on-demand) | \~$0.75/hr \= \~$540/month |
| g4dn.2xlarge (1yr reserved) | \~$0.35/hr \= \~$252/month |
| g5.2xlarge (on-demand) | \~$1.21/hr \= \~$871/month |
| 200 GB gp3 EBS | \~$16/month |
| 500 GB gp3 EBS | \~$40/month |
| Elastic IP (attached) | Free |
| Elastic IP (detached) | \~$3.65/month |
| S3 backup (50 GB) | \~$1.15/month |
| Data transfer out (first 100 GB) | Free tier |

---

## **15\. Script 1 â€” System Setup & Configuration Interview**

### **Scope**

Script 1 runs on the EC2 instance. It installs all system-level dependencies, conducts the configuration interview, and prepares everything for Script 2 (Docker deployment).

### **Full Logic Flow**

script\_1\_system\_setup.sh  
â”‚  
â”œâ”€â”€ PHASE 0: Validation & Root Check  
â”‚   â”œâ”€â”€ if \[\[ $EUID \-ne 0 \]\]; then error "Run as root: sudo bash  $ 0"; fi  
â”‚   â”œâ”€â”€ Detect OS: must be Ubuntu 22.04 or 24.04  
â”‚   â”‚     source /etc/os-release  
â”‚   â”‚     if \[\[ " $ ID" \!= "ubuntu" \]\] || \[\[ \! " $ VERSION\_ID" \=\~ ^(22.04|24.04) $  \]\]; then  
â”‚   â”‚       error "Requires Ubuntu 22.04 or 24.04"  
â”‚   â”‚     fi  
â”‚   â”œâ”€â”€ Check internet: curl \-s \--max-time 5 https://google.com \> /dev/null || error "No internet"  
â”‚   â”œâ”€â”€ Check minimum resources:  
â”‚   â”‚     TOTAL\_RAM\_GB=$(free \-g | awk '/Mem:/{print $2}')  
â”‚   â”‚     if \[\[  $ TOTAL\_RAM\_GB \-lt 8 \]\]; then warn "Less than 8 GB RAM â€” performance will suffer"; fi  
â”‚   â”‚     TOTAL\_DISK\_GB= $ (df \-BG / | awk 'NR==2{gsub(/G/,""); print $4}')  
â”‚   â”‚     if \[\[  $ TOTAL\_DISK\_GB \-lt 20 \]\]; then error "Less than 20 GB free on root â€” cannot proceed"; fi  
â”‚   â”œâ”€â”€ Create log directory: mkdir \-p /var/log/ai-platform  
â”‚   â”œâ”€â”€ Start transcript: exec \> \>(tee \-a /var/log/ai-platform/script-1.log) 2\>&1  
â”‚   â””â”€â”€ Record start time: START\_TIME= $ (date \+%s)  
â”‚  
â”œâ”€â”€ PHASE 1: Directory Structure  
â”‚   â”œâ”€â”€ Create base directories:  
â”‚   â”‚     BASE\_DIR=/opt/ai-platform  
â”‚   â”‚     mkdir \-p  $ BASE\_DIR/{config,env,scripts,backups}  
â”‚   â”‚     mkdir \-p /var/log/ai-platform  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Detect and mount EBS data volume:  
â”‚   â”‚     \# Find unmounted EBS volume (the one from Script 0\)  
â”‚   â”‚     DATA\_DEVICE=""  
â”‚   â”‚     for dev in /dev/nvme1n1 /dev/xvdf /dev/sdf; do  
â”‚   â”‚       if \[\[ \-b " $ dev" \]\]; then  
â”‚   â”‚         DATA\_DEVICE= $ dev  
â”‚   â”‚         break  
â”‚   â”‚       fi  
â”‚   â”‚     done  
â”‚   â”‚       
â”‚   â”‚     if \[\[ \-z " $ DATA\_DEVICE" \]\]; then  
â”‚   â”‚       warn "No separate data volume found â€” using /opt/ai-platform/data"  
â”‚   â”‚       DATA\_DIR=$BASE\_DIR/data  
â”‚   â”‚       mkdir \-p $DATA\_DIR  
â”‚   â”‚     else  
â”‚   â”‚       \# Check if already formatted  
â”‚   â”‚       if \! blkid $DATA\_DEVICE | grep \-q ext4; then  
â”‚   â”‚         echo "Formatting $DATA\_DEVICE as ext4..."  
â”‚   â”‚         mkfs.ext4 \-m 0 \-F $DATA\_DEVICE  
â”‚   â”‚       fi  
â”‚   â”‚       DATA\_DIR=/mnt/data  
â”‚   â”‚       mkdir \-p $DATA\_DIR  
â”‚   â”‚       mount $DATA\_DEVICE  $ DATA\_DIR  
â”‚   â”‚         
â”‚   â”‚       \# Add to fstab for persistence  
â”‚   â”‚       UUID= $ (blkid \-s UUID \-o value  $ DATA\_DEVICE)  
â”‚   â”‚       if \! grep \-q " $ UUID" /etc/fstab; then  
â”‚   â”‚         echo "UUID=$UUID $DATA\_DIR ext4 defaults,nofail 0 2" \>\> /etc/fstab  
â”‚   â”‚       fi  
â”‚   â”‚     fi  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Create data subdirectories:  
â”‚   â”‚     mkdir \-p $DATA\_DIR/{postgres,redis,qdrant,ollama/models,n8n,dify,flowise}  
â”‚   â”‚     mkdir \-p $DATA\_DIR/{anythingllm,openclaw,caddy,prometheus,grafana}  
â”‚   â”‚     mkdir \-p  $ DATA\_DIR/{chromadb,weaviate,milvus,gdrive}  
â”‚   â”‚  
â”‚   â””â”€â”€ Initialize master.env:  
â”‚         ENV\_DIR= $ BASE\_DIR/env  
â”‚         CONFIG\_DIR=$BASE\_DIR/config  
â”‚         cat \> $ENV\_DIR/master.env \<\< EOF  
â”‚         \# AI Platform Master Configuration  
â”‚         \# Generated:  $ (date \-u \+"%Y-%m-%dT%H:%M:%SZ")  
â”‚         \# Script 1 version: 1.0.0  
â”‚           
â”‚         \# Paths  
â”‚         BASE\_DIR= $ BASE\_DIR  
â”‚         DATA\_DIR= $ DATA\_DIR  
â”‚         CONFIG\_DIR= $ CONFIG\_DIR  
â”‚         ENV\_DIR= $ ENV\_DIR  
â”‚         LOG\_DIR=/var/log/ai-platform  
â”‚           
â”‚         \# Instance Info  
â”‚         INSTANCE\_TYPE= $ (curl \-s http://169.254.169.254/latest/meta-data/instance-type 2\>/dev/null || echo "unknown")  
â”‚         INSTANCE\_ID= $ (curl \-s http://169.254.169.254/latest/meta-data/instance-id 2\>/dev/null || echo "unknown")  
â”‚         PUBLIC\_IP= $ (curl \-s http://169.254.169.254/latest/meta-data/public-ipv4 2\>/dev/null || echo "unknown")  
â”‚         REGION=$(curl \-s http://169.254.169.254/latest/meta-data/placement/region 2\>/dev/null || echo "unknown")  
â”‚         EOF  
â”‚         chmod 600  $ ENV\_DIR/master.env  
â”‚  
â”œâ”€â”€ PHASE 2: System Packages  
â”‚   â”œâ”€â”€ export DEBIAN\_FRONTEND=noninteractive  
â”‚   â”œâ”€â”€ apt-get update \-qq  
â”‚   â”œâ”€â”€ apt-get upgrade \-y \-qq  
â”‚   â”œâ”€â”€ apt-get install \-y \-qq \\  
â”‚   â”‚     curl wget git jq yq unzip htop tree ncdu tmux \\  
â”‚   â”‚     ca-certificates gnupg lsb-release software-properties-common \\  
â”‚   â”‚     build-essential python3 python3-pip python3-venv \\  
â”‚   â”‚     apache2-utils openssl uuid-runtime \\  
â”‚   â”‚     dnsutils net-tools iotop sysstat \\  
â”‚   â”‚     fail2ban ufw  
â”‚   â””â”€â”€ pip3 install \--quiet langchain chromadb sentence-transformers  \# For direct embed script  
â”‚  
â”œâ”€â”€ PHASE 3: Docker Installation  
â”‚   â”œâ”€â”€ Remove old Docker (if any):  
â”‚   â”‚     apt-get remove \-y docker docker-engine docker.io containerd runc 2\>/dev/null  
â”‚   â”œâ”€â”€ Add Docker GPG key and repo:  
â”‚   â”‚     install \-m 0755 \-d /etc/apt/keyrings  
â”‚   â”‚     curl \-fsSL https://download.docker.com/linux/ubuntu/gpg | \\  
â”‚   â”‚       gpg \--dearmor \-o /etc/apt/keyrings/docker.gpg  
â”‚   â”‚     chmod a+r /etc/apt/keyrings/docker.gpg  
â”‚   â”‚     echo "deb \[arch= $ (dpkg \--print-architecture) signed-by=/etc/apt/keyrings/docker.gpg\] \\  
â”‚   â”‚       https://download.docker.com/linux/ubuntu  $ (lsb\_release \-cs) stable" | \\  
â”‚   â”‚       tee /etc/apt/sources.list.d/docker.list \> /dev/null  
â”‚   â”œâ”€â”€ apt-get update \-qq  
â”‚   â”œâ”€â”€ apt-get install \-y \-qq docker-ce docker-ce-cli containerd.io \\  
â”‚   â”‚     docker-buildx-plugin docker-compose-plugin  
â”‚   â”œâ”€â”€ systemctl enable \--now docker  
â”‚   â”œâ”€â”€ usermod \-aG docker ubuntu   \# Allow non-root docker  
â”‚   â”œâ”€â”€ Verify: docker \--version && docker compose version  
â”‚   â””â”€â”€ Write DOCKER\_VERSION to master.env  
â”‚  
â”œâ”€â”€ PHASE 4: NVIDIA GPU Setup (Conditional)  
â”‚   â”œâ”€â”€ Detect GPU:  
â”‚   â”‚     HAS\_GPU=false  
â”‚   â”‚     if lspci | grep \-i nvidia \> /dev/null 2\>&1; then  
â”‚   â”‚       HAS\_GPU=true  
â”‚   â”‚     fi  
â”‚   â”‚     echo "HAS\_GPU= $ HAS\_GPU" \>\>  $ ENV\_DIR/master.env  
â”‚   â”‚  
â”‚   â”œâ”€â”€ If HAS\_GPU=true:  
â”‚   â”‚     â”œâ”€â”€ Check if NVIDIA drivers already installed (Deep Learning AMI):  
â”‚   â”‚     â”‚     if nvidia-smi \> /dev/null 2\>&1; then  
â”‚   â”‚     â”‚       echo "NVIDIA drivers already installed"  
â”‚   â”‚     â”‚       DRIVER\_VERSION= $ (nvidia-smi \--query-gpu=driver\_version \--format=csv,noheader)  
â”‚   â”‚     â”‚       echo "NVIDIA\_DRIVER\_VERSION=$DRIVER\_VERSION" \>\>  $ ENV\_DIR/master.env  
â”‚   â”‚     â”‚     else  
â”‚   â”‚     â”‚       echo "Installing NVIDIA drivers..."  
â”‚   â”‚     â”‚       apt-get install \-y \-qq nvidia-driver-535 nvidia-utils-535  
â”‚   â”‚     â”‚       \# May require reboot â€” script detects and handles  
â”‚   â”‚     â”‚     fi  
â”‚   â”‚     â”‚  
â”‚   â”‚     â”œâ”€â”€ Install NVIDIA Container Toolkit:  
â”‚   â”‚     â”‚     distribution= $ (. /etc/os-release; echo  $ ID $ VERSION\_ID)  
â”‚   â”‚     â”‚     curl \-fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \\  
â”‚   â”‚     â”‚       gpg \--dearmor \-o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg  
â”‚   â”‚     â”‚     curl \-s \-L https://nvidia.github.io/libnvidia-container/ $ distribution/libnvidia-container.list | \\  
â”‚   â”‚     â”‚       sed 's\#deb https://\#deb \[signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\] https://\#g' | \\  
â”‚   â”‚     â”‚       tee /etc/apt/sources.list.d/nvidia-container-toolkit.list  
â”‚   â”‚     â”‚     apt-get update \-qq  
â”‚   â”‚     â”‚     apt-get install \-y \-qq nvidia-container-toolkit  
â”‚   â”‚     â”‚     nvidia-ctk runtime configure \--runtime=docker  
â”‚   â”‚     â”‚     systemctl restart docker  
â”‚   â”‚     â”‚  
â”‚   â”‚     â”œâ”€â”€ Verify GPU in Docker:  
â”‚   â”‚     â”‚     docker run \--rm \--gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi  
â”‚   â”‚     â”‚     If fails: error "GPU not accessible in Docker â€” check drivers"  
â”‚   â”‚     â”‚  
â”‚   â”‚     â””â”€â”€ Record GPU info:  
â”‚   â”‚           GPU\_NAME= $ (nvidia-smi \--query-gpu=gpu\_name \--format=csv,noheader | head \-1)  
â”‚   â”‚           GPU\_MEMORY= $ (nvidia-smi \--query-gpu=memory.total \--format=csv,noheader | head \-1)  
â”‚   â”‚           echo "GPU\_NAME= $ GPU\_NAME" \>\>  $ ENV\_DIR/master.env  
â”‚   â”‚           echo "GPU\_MEMORY= $ GPU\_MEMORY" \>\> $ENV\_DIR/master.env  
â”‚   â”‚  
â”‚   â””â”€â”€ If HAS\_GPU=false:  
â”‚         echo "No NVIDIA GPU detected â€” Ollama will run CPU-only"  
â”‚         echo "GPU\_NAME=none" \>\>  $ ENV\_DIR/master.env  
â”‚  
â”œâ”€â”€ PHASE 5: Ollama Installation  
â”‚   â”œâ”€â”€ curl \-fsSL https://ollama.com/install.sh | sh  
â”‚   â”œâ”€â”€ Create systemd override:  
â”‚   â”‚     mkdir \-p /etc/systemd/system/ollama.service.d  
â”‚   â”‚     cat \> /etc/systemd/system/ollama.service.d/override.conf \<\< EOF  
â”‚   â”‚     \[Service\]  
â”‚   â”‚     Environment="OLLAMA\_HOST=0.0.0.0:11434"  
â”‚   â”‚     Environment="OLLAMA\_MODELS= $ {DATA\_DIR}/ollama/models"  
â”‚   â”‚     Environment="OLLAMA\_KEEP\_ALIVE=24h"  
â”‚   â”‚     Environment="OLLAMA\_MAX\_LOADED\_MODELS=2"  
â”‚   â”‚     EOF  
â”‚   â”œâ”€â”€ systemctl daemon-reload  
â”‚   â”œâ”€â”€ systemctl enable \--now ollama  
â”‚   â”œâ”€â”€ Wait for ready:  
â”‚   â”‚     for i in $(seq 1 30); do  
â”‚   â”‚       curl \-s http://localhost:11434/api/version \> /dev/null 2\>&1 && break  
â”‚   â”‚       sleep 2  
â”‚   â”‚     done  
â”‚   â”œâ”€â”€ Verify: curl \-s http://localhost:11434/api/version | jq .  
â”‚   â””â”€â”€ echo "OLLAMA\_BASE\_URL=http://host.docker.internal:11434" \>\> $ENV\_DIR/master.env  
â”‚  
â”œâ”€â”€ PHASE 6: rclone Installation  
â”‚   â”œâ”€â”€ curl https://rclone.org/install.sh | bash  
â”‚   â”œâ”€â”€ mkdir \-p $CONFIG\_DIR/rclone  
â”‚   â”œâ”€â”€ rclone version  
â”‚   â””â”€â”€ echo "RCLONE\_INSTALLED=true" \>\>  $ ENV\_DIR/master.env  
â”‚  
â”œâ”€â”€ PHASE 7: Security Hardening  
â”‚   â”œâ”€â”€ Configure fail2ban:  
â”‚   â”‚     cat \> /etc/fail2ban/jail.local \<\< EOF  
â”‚   â”‚     \[sshd\]  
â”‚   â”‚     enabled \= true  
â”‚   â”‚     port \= ssh  
â”‚   â”‚     filter \= sshd  
â”‚   â”‚     logpath \= /var/log/auth.log  
â”‚   â”‚     maxretry \= 5  
â”‚   â”‚     bantime \= 3600  
â”‚   â”‚     findtime \= 600  
â”‚   â”‚     EOF  
â”‚   â”‚     systemctl enable \--now fail2ban  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Configure UFW:  
â”‚   â”‚     ufw default deny incoming  
â”‚   â”‚     ufw default allow outgoing  
â”‚   â”‚     ufw allow 22/tcp     \# SSH  
â”‚   â”‚     ufw allow 80/tcp     \# HTTP  
â”‚   â”‚     ufw allow 443/tcp    \# HTTPS  
â”‚   â”‚     ufw allow 443/udp    \# HTTP/3  
â”‚   â”‚     ufw \--force enable  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Kernel tuning:  
â”‚   â”‚     cat \>\> /etc/sysctl.d/99-ai-platform.conf \<\< EOF  
â”‚   â”‚     \# Network performance  
â”‚   â”‚     net.core.somaxconn \= 65535  
â”‚   â”‚     net.ipv4.tcp\_max\_syn\_backlog \= 65535  
â”‚   â”‚     net.core.netdev\_max\_backlog \= 65535  
â”‚   â”‚       
â”‚   â”‚     \# Memory  
â”‚   â”‚     vm.overcommit\_memory \= 1  
â”‚   â”‚     vm.swappiness \= 10  
â”‚   â”‚       
â”‚   â”‚     \# File descriptors  
â”‚   â”‚     fs.file-max \= 2097152  
â”‚   â”‚     fs.inotify.max\_user\_watches \= 524288  
â”‚   â”‚     EOF  
â”‚   â”‚     sysctl \-p /etc/sysctl.d/99-ai-platform.conf  
â”‚   â”‚  
â”‚   â””â”€â”€ Set Docker log limits:  
â”‚         cat \> /etc/docker/daemon.json \<\< EOF  
â”‚         {  
â”‚           "log-driver": "json-file",  
â”‚           "log-opts": {  
â”‚             "max-size": "10m",  
â”‚             "max-file": "5"  
â”‚           },  
â”‚           "default-runtime": " $ (if \[\[  $ HAS\_GPU \== true \]\]; then echo nvidia; else echo runc; fi)",  
â”‚           "runtimes": {  
â”‚             "nvidia": {  
â”‚               "path": "nvidia-container-runtime",  
â”‚               "runtimeArgs": \[\]  
â”‚             }  
â”‚           },  
â”‚           "storage-driver": "overlay2",  
â”‚           "live-restore": true  
â”‚         }  
â”‚         EOF  
â”‚         systemctl restart docker  
â”‚  
â”œâ”€â”€ PHASE 8: Configuration Interview  
â”‚   â”œâ”€â”€ echo "============================================"  
â”‚   â”œâ”€â”€ echo "    AI Platform Configuration Interview"  
â”‚   â”œâ”€â”€ echo "============================================"  
â”‚   â”‚  
â”‚   â”œâ”€â”€ 8a. Service Selection:  
â”‚   â”‚     echo ""  
â”‚   â”‚     echo "Select services to deploy:"  
â”‚   â”‚     echo "  Core (always installed):"  
â”‚   â”‚     echo "    âœ“ PostgreSQL \+ Redis"  
â”‚   â”‚     echo "    âœ“ LiteLLM (model router)"  
â”‚   â”‚     echo "    âœ“ Caddy (reverse proxy)"  
â”‚   â”‚     echo "    âœ“ Portainer (container management)"  
â”‚   â”‚     echo ""  
â”‚   â”‚     echo "  AI Platforms:"  
â”‚   â”‚     read \-p "    Install Dify? (y/n) \[y\]: " INSTALL\_DIFY  
â”‚   â”‚     read \-p "    Install n8n? (y/n) \[y\]: " INSTALL\_N8N  
â”‚   â”‚     read \-p "    Install Open WebUI? (y/n) \[y\]: " INSTALL\_OPENWEBUI  
â”‚   â”‚     read \-p "    Install Flowise? (y/n) \[y\]: " INSTALL\_FLOWISE  
â”‚   â”‚     read \-p "    Install AnythingLLM? (y/n) \[n\]: " INSTALL\_ANYTHINGLLM  
â”‚   â”‚     read \-p "    Install OpenClaw? (y/n) \[n\]: " INSTALL\_OPENCLAW  
â”‚   â”‚     echo ""  
â”‚   â”‚     echo "  Monitoring:"  
â”‚   â”‚     read \-p "    Install monitoring stack (Prometheus+Grafana)? (y/n) \[y\]: " INSTALL\_MONITORING  
â”‚   â”‚     echo ""  
â”‚   â”‚     echo "  Auth:"  
â”‚   â”‚     read \-p "    Install SuperTokens (centralized auth)? (y/n) \[n\]: " INSTALL\_SUPERTOKENS  
â”‚   â”‚       
â”‚   â”‚     \# Build ENABLED\_SERVICES list  
â”‚   â”‚     ENABLED\_SERVICES="postgres,redis,litellm,caddy,portainer"  
â”‚   â”‚     \[\[ " $ {INSTALL\_DIFY:-y}" \=\~ ^\[Yy\] \]\] && ENABLED\_SERVICES+=",dify"  
â”‚   â”‚     \[\[ "${INSTALL\_N8N:-y}" \=\~ ^\[Yy\] \]\] && ENABLED\_SERVICES+=",n8n"  
â”‚   â”‚     \[\[ "${INSTALL\_OPENWEBUI:-y}" \=\~ ^\[Yy\] \]\] && ENABLED\_SERVICES+=",open-webui"  
â”‚   â”‚     \[\[ "${INSTALL\_FLOWISE:-y}" \=\~ ^\[Yy\] \]\] && ENABLED\_SERVICES+=",flowise"  
â”‚   â”‚     \[\[ "${INSTALL\_ANYTHINGLLM:-n}" \=\~ ^\[Yy\] \]\] && ENABLED\_SERVICES+=",anythingllm"  
â”‚   â”‚     \[\[ "${INSTALL\_OPENCLAW:-n}" \=\~ ^\[Yy\] \]\] && ENABLED\_SERVICES+=",openclaw"  
â”‚   â”‚     \[\[ "${INSTALL\_MONITORING:-y}" \=\~ ^\[Yy\] \]\] && ENABLED\_SERVICES+=",monitoring"  
â”‚   â”‚     \[\[ "${INSTALL\_SUPERTOKENS:-n}" \=\~ ^\[Yy\] \]\] && ENABLED\_SERVICES+=",supertokens"  
â”‚   â”‚     echo "ENABLED\_SERVICES=$ENABLED\_SERVICES" \>\>  $ ENV\_DIR/master.env  
â”‚   â”‚  
â”‚   â”œâ”€â”€ 8b. Vector DB Selection:  
â”‚   â”‚     select\_vector\_db   \# (as defined in Section 10\)  
â”‚   â”‚  
â”‚   â”œâ”€â”€ 8c. Model Tier Selection:  
â”‚   â”‚     echo ""  
â”‚   â”‚     echo "Select Ollama model tier:"  
â”‚   â”‚     echo "  1\) Minimal  â€” tinyllama \+ nomic-embed-text (\~3 GB)"  
â”‚   â”‚     echo "  2\) Standard â€” \+ mistral, llama3.1:8b, codellama:7b (\~25 GB)"  
â”‚   â”‚     echo "  3\) Full     â€” \+ llama3.1:70b, mixtral, deepseek-coder (\~120 GB)"  
â”‚   â”‚     echo "  4\) Custom   â€” choose individual models later"  
â”‚   â”‚     read \-p "Choice \[2\]: " model\_choice  
â”‚   â”‚     MODEL\_TIER= $ (map\_choice\_to\_tier  $ model\_choice)  
â”‚   â”‚     echo "MODEL\_TIER= $ MODEL\_TIER" \>\> $ENV\_DIR/master.env  
â”‚   â”‚  
â”‚   â”œâ”€â”€ 8d. API Keys:  
â”‚   â”‚     collect\_api\_keys   \# (as defined in Section 8\)  
â”‚   â”‚  
â”‚   â”œâ”€â”€ 8e. Domain Configuration:  
â”‚   â”‚     configure\_domain   \# (as defined in Section 12\)  
â”‚   â”‚  
â”‚   â”œâ”€â”€ 8f. Google Drive Sync:  
â”‚   â”‚     echo ""  
â”‚   â”‚     read \-p "Enable Google Drive sync for RAG documents? (y/n) \[n\]: " ENABLE\_GDRIVE  
â”‚   â”‚     if \[\[ "${ENABLE\_GDRIVE}" \=\~ ^\[Yy\] \]\]; then  
â”‚   â”‚       echo "GDRIVE\_ENABLED=true" \>\> $ENV\_DIR/master.env  
â”‚   â”‚       echo ""  
â”‚   â”‚       echo "Google Drive sync requires OAuth2 credentials."  
â”‚   â”‚       echo "You'll need a Google Cloud project with Drive API enabled."  
â”‚   â”‚       echo "See: https://rclone.org/drive/\#making-your-own-client-id"  
â”‚   â”‚       echo ""  
â”‚   â”‚       read \-p "  Google OAuth Client ID: " GDRIVE\_CLIENT\_ID  
â”‚   â”‚       read \-p "  Google OAuth Client Secret: " GDRIVE\_CLIENT\_SECRET  
â”‚   â”‚       read \-p "  Folder ID or path to sync \[root\]: " GDRIVE\_FOLDER  
â”‚   â”‚       GDRIVE\_FOLDER=${GDRIVE\_FOLDER:-root}  
â”‚   â”‚       read \-p "  Sync interval in minutes \[60\]: " GDRIVE\_INTERVAL  
â”‚   â”‚       GDRIVE\_INTERVAL=${GDRIVE\_INTERVAL:-60}  
â”‚   â”‚       echo "GDRIVE\_CLIENT\_ID=${GDRIVE\_CLIENT\_ID}" \>\> $ENV\_DIR/master.env  
â”‚   â”‚       echo "GDRIVE\_CLIENT\_SECRET=${GDRIVE\_CLIENT\_SECRET}" \>\> $ENV\_DIR/master.env  
â”‚   â”‚       echo "GDRIVE\_FOLDER=${GDRIVE\_FOLDER}" \>\> $ENV\_DIR/master.env  
â”‚   â”‚       echo "GDRIVE\_INTERVAL=${GDRIVE\_INTERVAL}" \>\> $ENV\_DIR/master.env  
â”‚   â”‚       echo "GDRIVE\_SYNC\_DIR=${DATA\_DIR}/gdrive" \>\> $ENV\_DIR/master.env  
â”‚   â”‚       echo ""  
â”‚   â”‚       echo "NOTE: You will need to complete rclone OAuth authorization"  
â”‚   â”‚       echo "during Script 2 (requires browser or token paste)."  
â”‚   â”‚     else  
â”‚   â”‚       echo "GDRIVE\_ENABLED=false" \>\> $ENV\_DIR/master.env  
â”‚   â”‚     fi  
â”‚   â”‚  
â”‚   â”œâ”€â”€ 8g. Backup Configuration:  
â”‚   â”‚     echo ""  
â”‚   â”‚     echo "Backup configuration:"  
â”‚   â”‚     read \-p "  Enable automated S3 backups? (y/n) \[y\]: " ENABLE\_BACKUPS  
â”‚   â”‚     if \[\[ "${ENABLE\_BACKUPS:-y}" \=\~ ^\[Yy\] \]\]; then  
â”‚   â”‚       echo "BACKUP\_ENABLED=true" \>\> $ENV\_DIR/master.env  
â”‚   â”‚       \# Detect S3 bucket from instance tags or ask  
â”‚   â”‚       DETECTED\_BUCKET=$(aws s3 ls 2\>/dev/null | grep ai-platform | awk '{print $3}' | head \-1)  
â”‚   â”‚       if \[\[ \-n "$DETECTED\_BUCKET" \]\]; then  
â”‚   â”‚         read \-p "  S3 bucket \[$DETECTED\_BUCKET\]: " S3\_BUCKET  
â”‚   â”‚         S3\_BUCKET=${S3\_BUCKET:-$DETECTED\_BUCKET}  
â”‚   â”‚       else  
â”‚   â”‚         read \-p "  S3 bucket name: " S3\_BUCKET  
â”‚   â”‚       fi  
â”‚   â”‚       read \-p "  Backup frequency â€” daily/weekly \[daily\]: " BACKUP\_FREQ  
â”‚   â”‚       BACKUP\_FREQ=${BACKUP\_FREQ:-daily}  
â”‚   â”‚       read \-p "  Retention days \[30\]: " BACKUP\_RETENTION  
â”‚   â”‚       BACKUP\_RETENTION=${BACKUP\_RETENTION:-30}  
â”‚   â”‚       echo "S3\_BUCKET=${S3\_BUCKET}" \>\> $ENV\_DIR/master.env  
â”‚   â”‚       echo "BACKUP\_FREQUENCY=${BACKUP\_FREQ}" \>\> $ENV\_DIR/master.env  
â”‚   â”‚       echo "BACKUP\_RETENTION\_DAYS=${BACKUP\_RETENTION}" \>\> $ENV\_DIR/master.env  
â”‚   â”‚     else  
â”‚   â”‚       echo "BACKUP\_ENABLED=false" \>\> $ENV\_DIR/master.env  
â”‚   â”‚     fi  
â”‚   â”‚  
â”‚   â””â”€â”€ 8h. Confirmation Summary:  
â”‚         echo ""  
â”‚         echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚         echo "  CONFIGURATION SUMMARY"  
â”‚         echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚         echo ""  
â”‚         echo "  Instance:     $(grep INSTANCE\_TYPE $ENV\_DIR/master.env | cut \-d= \-f2)"  
â”‚         echo "  GPU:          $(grep GPU\_NAME $ENV\_DIR/master.env | cut \-d= \-f2)"  
â”‚         echo "  Data Volume:  $DATA\_DIR ($(df \-BG $DATA\_DIR | awk 'NR==2{print $2}'))"  
â”‚         echo ""  
â”‚         echo "  Services:     $(grep ENABLED\_SERVICES $ENV\_DIR/master.env | cut \-d= \-f2 | tr ',' '\\n' | sed 's/^/    âœ“ /')"  
â”‚         echo ""  
â”‚         echo "  Vector DB:    $(grep VECTOR\_DB= $ENV\_DIR/master.env | cut \-d= \-f2)"  
â”‚         echo "  Model Tier:   $(grep MODEL\_TIER $ENV\_DIR/master.env | cut \-d= \-f2)"  
â”‚         echo "  Domain:       $(grep BASE\_DOMAIN $ENV\_DIR/master.env | cut \-d= \-f2)"  
â”‚         echo "  Domain Mode:  $(grep DOMAIN\_MODE $ENV\_DIR/master.env | cut \-d= \-f2)"  
â”‚         echo "  GDrive Sync:  $(grep GDRIVE\_ENABLED $ENV\_DIR/master.env | cut \-d= \-f2)"  
â”‚         echo "  S3 Backups:   $(grep BACKUP\_ENABLED $ENV\_DIR/master.env | cut \-d= \-f2)"  
â”‚         echo ""  
â”‚         echo "  API Keys configured:"  
â”‚         grep \-c "\_API\_KEY=" $ENV\_DIR/master.env | xargs \-I{} echo "    {} provider keys set"  
â”‚         echo ""  
â”‚         echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚         echo ""  
â”‚         read \-p "  Proceed with this configuration? (y/n) \[y\]: " CONFIRM  
â”‚         if \[\[ \! "${CONFIRM:-y}" \=\~ ^\[Yy\] \]\]; then  
â”‚           echo "Configuration cancelled. Re-run script to reconfigure."  
â”‚           exit 0  
â”‚         fi  
â”‚  
â”œâ”€â”€ PHASE 9: Generate Credentials  
â”‚   â”œâ”€â”€ echo "Generating secure credentials..."  
â”‚   â”œâ”€â”€ Generate all passwords and secrets:  
â”‚   â”‚     POSTGRES\_PASSWORD=$(openssl rand \-base64 32 | tr \-dc 'a-zA-Z0-9' | head \-c 32\)  
â”‚   â”‚     REDIS\_PASSWORD=$(openssl rand \-base64 32 | tr \-dc 'a-zA-Z0-9' | head \-c 32\)  
â”‚   â”‚     LITELLM\_MASTER\_KEY=$(openssl rand \-hex 24\)  
â”‚   â”‚     LITELLM\_SALT\_KEY=$(openssl rand \-hex 24\)  
â”‚   â”‚     N8N\_ENCRYPTION\_KEY=$(openssl rand \-hex 32\)  
â”‚   â”‚     DIFY\_SECRET\_KEY=$(openssl rand \-hex 32\)  
â”‚   â”‚     DIFY\_INIT\_PASSWORD=$(openssl rand \-base64 16 | tr \-dc 'a-zA-Z0-9' | head \-c 16\)  
â”‚   â”‚     FLOWISE\_PASSWORD=$(openssl rand \-base64 16 | tr \-dc 'a-zA-Z0-9' | head \-c 16\)  
â”‚   â”‚     GRAFANA\_ADMIN\_PASSWORD=$(openssl rand \-base64 16 | tr \-dc 'a-zA-Z0-9' | head \-c 16\)  
â”‚   â”‚     PORTAINER\_ADMIN\_PASSWORD=$(openssl rand \-base64 16 | tr \-dc 'a-zA-Z0-9' | head \-c 16\)  
â”‚   â”‚     SUPERTOKENS\_API\_KEY=$(openssl rand \-hex 24\)  
â”‚   â”‚     ANYTHINGLLM\_API\_KEY=$(openssl rand \-hex 24\)  
â”‚   â”‚     QDRANT\_API\_KEY=$(openssl rand \-hex 24\)  
â”‚   â”‚     WEAVIATE\_API\_KEY=$(openssl rand \-hex 24\)  
â”‚   â”‚     MILVUS\_TOKEN=$(openssl rand \-hex 24\)  
â”‚   â”‚     JWT\_SECRET=$(openssl rand \-hex 32\)  
â”‚   â”‚     WEBHOOK\_SECRET=$(openssl rand \-hex 16\)  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Write all to master.env:  
â”‚   â”‚     cat \>\> $ENV\_DIR/master.env \<\< EOF  
â”‚   â”‚       
â”‚   â”‚     \# â”€â”€ Generated Credentials â”€â”€  
â”‚   â”‚     POSTGRES\_USER=aiplatform  
â”‚   â”‚     POSTGRES\_PASSWORD=${POSTGRES\_PASSWORD}  
â”‚   â”‚     POSTGRES\_HOST=postgres  
â”‚   â”‚     POSTGRES\_PORT=5432  
â”‚   â”‚     REDIS\_PASSWORD=${REDIS\_PASSWORD}  
â”‚   â”‚     REDIS\_HOST=redis  
â”‚   â”‚     REDIS\_PORT=6379  
â”‚   â”‚     LITELLM\_MASTER\_KEY=sk-${LITELLM\_MASTER\_KEY}  
â”‚   â”‚     LITELLM\_SALT\_KEY=${LITELLM\_SALT\_KEY}  
â”‚   â”‚     N8N\_ENCRYPTION\_KEY=${N8N\_ENCRYPTION\_KEY}  
â”‚   â”‚     DIFY\_SECRET\_KEY=${DIFY\_SECRET\_KEY}  
â”‚   â”‚     DIFY\_INIT\_PASSWORD=${DIFY\_INIT\_PASSWORD}  
â”‚   â”‚     FLOWISE\_USERNAME=admin  
â”‚   â”‚     FLOWISE\_PASSWORD=${FLOWISE\_PASSWORD}  
â”‚   â”‚     GRAFANA\_ADMIN\_USER=admin  
â”‚   â”‚     GRAFANA\_ADMIN\_PASSWORD=${GRAFANA\_ADMIN\_PASSWORD}  
â”‚   â”‚     PORTAINER\_ADMIN\_PASSWORD=${PORTAINER\_ADMIN\_PASSWORD}  
â”‚   â”‚     SUPERTOKENS\_API\_KEY=${SUPERTOKENS\_API\_KEY}  
â”‚   â”‚     ANYTHINGLLM\_API\_KEY=${ANYTHINGLLM\_API\_KEY}  
â”‚   â”‚     QDRANT\_API\_KEY=${QDRANT\_API\_KEY}  
â”‚   â”‚     WEAVIATE\_API\_KEY=${WEAVIATE\_API\_KEY}  
â”‚   â”‚     MILVUS\_TOKEN=${MILVUS\_TOKEN}  
â”‚   â”‚     JWT\_SECRET=${JWT\_SECRET}  
â”‚   â”‚     WEBHOOK\_SECRET=${WEBHOOK\_SECRET}  
â”‚   â”‚     EOF  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Create human-readable credentials file:  
â”‚   â”‚     cat \> $ENV\_DIR/credentials.txt \<\< EOF  
â”‚   â”‚     â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  
â”‚   â”‚     â•‘       AI PLATFORM CREDENTIALS â€” SAVE THIS FILE      â•‘  
â”‚   â”‚     â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£  
â”‚   â”‚     â•‘                                                      â•‘  
â”‚   â”‚     â•‘  PostgreSQL:                                         â•‘  
â”‚   â”‚     â•‘    User:     aiplatform                              â•‘  
â”‚   â”‚     â•‘    Password: ${POSTGRES\_PASSWORD}                    â•‘  
â”‚   â”‚     â•‘                                                      â•‘  
â”‚   â”‚     â•‘  Redis:                                              â•‘  
â”‚   â”‚     â•‘    Password: ${REDIS\_PASSWORD}                       â•‘  
â”‚   â”‚     â•‘                                                      â•‘  
â”‚   â”‚     â•‘  LiteLLM:                                            â•‘  
â”‚   â”‚     â•‘    Master Key: sk-${LITELLM\_MASTER\_KEY}              â•‘  
â”‚   â”‚     â•‘                                                      â•‘  
â”‚   â”‚     â•‘  Dify:                                               â•‘  
â”‚   â”‚     â•‘    Init Password: ${DIFY\_INIT\_PASSWORD}              â•‘  
â”‚   â”‚     â•‘    URL: https://${DIFY\_DOMAIN:-dify.localhost}       â•‘  
â”‚   â”‚     â•‘                                                      â•‘  
â”‚   â”‚     â•‘  n8n:                                                â•‘  
â”‚   â”‚     â•‘    URL: https://${N8N\_DOMAIN:-n8n.localhost}         â•‘  
â”‚   â”‚     â•‘    (Set password on first login)                     â•‘  
â”‚   â”‚     â•‘                                                      â•‘  
â”‚   â”‚     â•‘  Open WebUI:                                         â•‘  
â”‚   â”‚     â•‘    URL: https://${WEBUI\_DOMAIN:-webui.localhost}     â•‘  
â”‚   â”‚     â•‘    (Set password on first login)                     â•‘  
â”‚   â”‚     â•‘                                                      â•‘  
â”‚   â”‚     â•‘  Flowise:                                            â•‘  
â”‚   â”‚     â•‘    User:     admin                                   â•‘  
â”‚   â”‚     â•‘    Password: ${FLOWISE\_PASSWORD}                     â•‘  
â”‚   â”‚     â•‘    URL: https://${FLOWISE\_DOMAIN:-flowise.localhost} â•‘  
â”‚   â”‚     â•‘                                                      â•‘  
â”‚   â”‚     â•‘  Grafana:                                            â•‘  
â”‚   â”‚     â•‘    User:     admin                                   â•‘  
â”‚   â”‚     â•‘    Password: ${GRAFANA\_ADMIN\_PASSWORD}               â•‘  
â”‚   â”‚     â•‘    URL: https://${GRAFANA\_DOMAIN:-grafana.localhost} â•‘  
â”‚   â”‚     â•‘                                                      â•‘  
â”‚   â”‚     â•‘  Portainer:                                          â•‘  
â”‚   â”‚     â•‘    Password: ${PORTAINER\_ADMIN\_PASSWORD}             â•‘  
â”‚   â”‚     â•‘    URL: https://${PORTAINER\_DOMAIN:-portainer.localhost}â•‘  
â”‚   â”‚     â•‘                                                      â•‘  
â”‚   â”‚     â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
â”‚   â”‚       
â”‚   â”‚     Generated: $(date \-u \+"%Y-%m-%dT%H:%M:%SZ")  
â”‚   â”‚       
â”‚   â”‚     âš   BACK UP THIS FILE SECURELY AND DELETE FROM SERVER  
â”‚   â”‚     EOF  
â”‚   â”‚     chmod 600 $ENV\_DIR/credentials.txt  
â”‚   â”‚  
â”‚   â””â”€â”€ echo "Credentials saved to $ENV\_DIR/credentials.txt"  
â”‚       echo "âš   Copy this file to a secure location, then delete it from the server"  
â”‚  
â”œâ”€â”€ PHASE 10: Create PostgreSQL Init Script  
â”‚   â”œâ”€â”€ Generate database initialization SQL:  
â”‚   â”‚     cat \> $CONFIG\_DIR/postgres/init-databases.sql \<\< 'EOF'  
â”‚   â”‚     \-- Create databases for each service  
â”‚   â”‚     CREATE DATABASE dify OWNER aiplatform;  
â”‚   â”‚     CREATE DATABASE n8n OWNER aiplatform;  
â”‚   â”‚     CREATE DATABASE litellm OWNER aiplatform;  
â”‚   â”‚     CREATE DATABASE flowise OWNER aiplatform;  
â”‚   â”‚     CREATE DATABASE supertokens OWNER aiplatform;  
â”‚   â”‚     CREATE DATABASE anythingllm OWNER aiplatform;  
â”‚   â”‚     CREATE DATABASE openclaw OWNER aiplatform;  
â”‚   â”‚       
â”‚   â”‚     \-- Enable extensions  
â”‚   â”‚     \\c dify  
â”‚   â”‚     CREATE EXTENSION IF NOT EXISTS "uuid-ossp";  
â”‚   â”‚     CREATE EXTENSION IF NOT EXISTS "vector";  
â”‚   â”‚       
â”‚   â”‚     \\c litellm  
â”‚   â”‚     CREATE EXTENSION IF NOT EXISTS "uuid-ossp";  
â”‚   â”‚       
â”‚   â”‚     \\c n8n  
â”‚   â”‚     CREATE EXTENSION IF NOT EXISTS "uuid-ossp";  
â”‚   â”‚     EOF  
â”‚   â”‚  
â”‚   â””â”€â”€ Note: Only creates DBs for enabled services â€” Script 2 filters  
â”‚  
â”œâ”€â”€ PHASE 11: Set Script 2 Ready Flag  
â”‚   â”œâ”€â”€ echo "SCRIPT\_1\_COMPLETED=$(date \-u \+%Y-%m-%dT%H:%M:%SZ)" \>\> $ENV\_DIR/master.env  
â”‚   â”œâ”€â”€ echo "SCRIPT\_1\_VERSION=1.0.0" \>\> $ENV\_DIR/master.env  
â”‚   â”‚  
â”‚   â”œâ”€â”€ Calculate duration:  
â”‚   â”‚     END\_TIME=$(date \+%s)  
â”‚   â”‚     DURATION=$((END\_TIME \- START\_TIME))  
â”‚   â”‚     MINUTES=$((DURATION / 60))  
â”‚   â”‚     SECONDS=$((DURATION % 60))  
â”‚   â”‚  
â”‚   â””â”€â”€ Final output:  
â”‚         echo ""  
â”‚         echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚         echo "  âœ“ SCRIPT 1 COMPLETE  (${MINUTES}m ${SECONDS}s)"  
â”‚         echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚         echo ""  
â”‚         echo "  System packages:     âœ“ Installed"  
â”‚         echo "  Docker:              âœ“ $(docker \--version | awk '{print $3}')"  
â”‚         echo "  Docker Compose:      âœ“ $(docker compose version \--short)"  
â”‚         echo "  GPU:                 $(if \[\[ $HAS\_GPU \== true \]\]; then echo "âœ“ $GPU\_NAME ($GPU\_MEMORY)"; else echo "âœ— Not detected (CPU mode)"; fi)"  
â”‚         echo "  Ollama:              âœ“ Running on :11434"  
â”‚         echo "  Data volume:         âœ“ $DATA\_DIR ($(df \-BG $DATA\_DIR | awk 'NR==2{print $4}') free)"  
â”‚         echo "  Configuration:       âœ“ $ENV\_DIR/master.env"  
â”‚         echo "  Credentials:         âœ“ $ENV\_DIR/credentials.txt"  
â”‚         echo ""  
â”‚         echo "  NEXT STEP:"  
â”‚         echo "    sudo bash /opt/ai-platform/scripts/script-2-deploy.sh"  
â”‚         echo ""  
â”‚         echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚  
â””â”€â”€ PHASE 12: Reboot Check  
    â”œâ”€â”€ \# Reboot needed if NVIDIA drivers were freshly installed  
    â”œâ”€â”€ if \[\[ "$NVIDIA\_FRESH\_INSTALL" \== "true" \]\]; then  
    â”‚     echo ""  
    â”‚     echo "âš   NVIDIA drivers were installed â€” REBOOT REQUIRED"  
    â”‚     echo ""  
    â”‚     read \-p "Reboot now? (y/n) \[y\]: " REBOOT  
    â”‚     if \[\[ "${REBOOT:-y}" \=\~ ^\[Yy\] \]\]; then  
    â”‚       echo "Rebooting in 5 seconds... After reboot, run Script 2."  
    â”‚       sleep 5  
    â”‚       reboot  
    â”‚     else  
    â”‚       echo "Please reboot manually before running Script 2:"  
    â”‚       echo "  sudo reboot"  
    â”‚     fi  
    â””â”€â”€ fi

### **Final master.env After Script 1 (Example)**

\# AI Platform Master Configuration  
\# Generated: 2025-01-15T10:30:00Z  
\# Script 1 version: 1.0.0

\# Paths  
BASE\_DIR=/opt/ai-platform  
DATA\_DIR=/mnt/data  
CONFIG\_DIR=/opt/ai-platform/config  
ENV\_DIR=/opt/ai-platform/env  
LOG\_DIR=/var/log/ai-platform

\# Instance Info  
INSTANCE\_TYPE=g4dn.2xlarge  
INSTANCE\_ID=i-0abc123def456  
PUBLIC\_IP=54.123.45.67  
REGION=us-east-1

\# Hardware Detection  
HAS\_GPU=true  
NVIDIA\_DRIVER\_VERSION=535.104.12  
GPU\_NAME=Tesla T4  
GPU\_MEMORY=15360 MiB  
DOCKER\_VERSION=24.0.7

\# Ollama  
OLLAMA\_BASE\_URL=http://host.docker.internal:11434  
RCLONE\_INSTALLED=true

\# Service Selection  
ENABLED\_SERVICES=postgres,redis,litellm,caddy,portainer,dify,n8n,open-webui,flowise,monitoring

\# Vector DB  
VECTOR\_DB=qdrant  
VECTOR\_DB\_HOST=qdrant  
VECTOR\_DB\_PORT=6333

\# Model Tier  
MODEL\_TIER=standard

\# Domain  
BASE\_DOMAIN=ai.example.com  
ACME\_EMAIL=admin@example.com  
DOMAIN\_MODE=production  
DIFY\_DOMAIN=dify.ai.example.com  
N8N\_DOMAIN=n8n.ai.example.com  
WEBUI\_DOMAIN=webui.ai.example.com  
FLOWISE\_DOMAIN=flowise.ai.example.com  
LITELLM\_DOMAIN=llm.ai.example.com  
GRAFANA\_DOMAIN=grafana.ai.example.com  
PORTAINER\_DOMAIN=portainer.ai.example.com

\# Google Drive  
GDRIVE\_ENABLED=true  
GDRIVE\_CLIENT\_ID=xxxxx.apps.googleusercontent.com  
GDRIVE\_CLIENT\_SECRET=GOCSPX-xxxxx  
GDRIVE\_FOLDER=root  
GDRIVE\_INTERVAL=60  
GDRIVE\_SYNC\_DIR=/mnt/data/gdrive

\# Backups  
BACKUP\_ENABLED=true  
S3\_BUCKET=ai-platform-backups-abc123  
BACKUP\_FREQUENCY=daily  
BACKUP\_RETENTION\_DAYS=30

\# External API Keys  
OPENAI\_API\_KEY=sk-xxxxx  
ANTHROPIC\_API\_KEY=sk-ant-xxxxx  
GOOGLE\_AI\_API\_KEY=AIzaSyxxxxx

\# â”€â”€ Generated Credentials â”€â”€  
POSTGRES\_USER=aiplatform  
POSTGRES\_PASSWORD=aB3xK9mP2qR7sT4vW6yZ8nL1cD5fG0h  
POSTGRES\_HOST=postgres  
POSTGRES\_PORT=5432  
REDIS\_PASSWORD=jH2kM4nP6qS8tV0wX3yA5bC7dF9gI1lE  
REDIS\_HOST=redis  
REDIS\_PORT=6379  
LITELLM\_MASTER\_KEY=sk-a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6  
LITELLM\_SALT\_KEY=f6e5d4c3b2a1f6e5d4c3b2a1f6e5d4c3b2a1f6e5d4c3b2a1  
N8N\_ENCRYPTION\_KEY=0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef  
DIFY\_SECRET\_KEY=fedcba9876543210fedcba9876543210fedcba9876543210fedcba9876543210  
DIFY\_INIT\_PASSWORD=xK9mP2qR7sT4vW6y  
FLOWISE\_USERNAME=admin  
FLOWISE\_PASSWORD=aB3xK9mP2qR7sT4v  
GRAFANA\_ADMIN\_USER=admin  
GRAFANA\_ADMIN\_PASSWORD=W6yZ8nL1cD5fG0hJ  
PORTAINER\_ADMIN\_PASSWORD=2kM4nP6qS8tV0wX3  
SUPERTOKENS\_API\_KEY=a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6  
ANYTHINGLLM\_API\_KEY=b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6a1  
QDRANT\_API\_KEY=c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6a1b2  
JWT\_SECRET=d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4e5  
WEBHOOK\_SECRET=e5f6a1b2c3d4e5f6a1b2c3d4e5f6a1b2

\# Script Tracking  
SCRIPT\_1\_COMPLETED=2025-01-15T10:45:23Z  
SCRIPT\_1\_VERSION=1.0.0  
\---

\#\# 16\. Script 2 â€” Docker Compose Generation & Deployment

\#\#\# Script Overview

script-2-deploy.sh â”‚ â”œâ”€â”€ Pre-flight Checks â”œâ”€â”€ PHASE 1: Validate Script 1 completion â”œâ”€â”€ PHASE 2: Source master.env â”œâ”€â”€ PHASE 3: Generate per-service environment files â”œâ”€â”€ PHASE 4: Generate LiteLLM config (litellm\_config.yaml) â”œâ”€â”€ PHASE 5: Generate Caddyfile â”œâ”€â”€ PHASE 6: Generate docker-compose.yml (dynamic, based on ENABLED\_SERVICES) â”œâ”€â”€ PHASE 7: Generate monitoring configs (Prometheus, Grafana) â”œâ”€â”€ PHASE 8: Pull Docker images â”œâ”€â”€ PHASE 9: Start core infrastructure (postgres, redis) â”œâ”€â”€ PHASE 10: Wait for DB readiness, run init SQL â”œâ”€â”€ PHASE 11: Start remaining services in dependency order â”œâ”€â”€ PHASE 12: Pull Ollama models â”œâ”€â”€ PHASE 13: Configure Google Drive (rclone OAuth \+ systemd timers) â”œâ”€â”€ PHASE 14: Configure backup cron â”œâ”€â”€ PHASE 15: Health check all services â”œâ”€â”€ PHASE 16: Generate convenience scripts â””â”€â”€ PHASE 17: Final summary

\#\#\# Pre-flight & Phases 1â€“2

\`\`\`bash  
\#\!/bin/bash  
set \-euo pipefail

\# â”€â”€ Constants â”€â”€  
BASE\_DIR=/opt/ai-platform  
ENV\_DIR= $ BASE\_DIR/env  
CONFIG\_DIR= $ BASE\_DIR/config  
DATA\_DIR=$(grep DATA\_DIR  $ ENV\_DIR/master.env | cut \-d= \-f2)  
LOG\_FILE=/var/log/ai-platform/script-2.log  
COMPOSE\_DIR= $ BASE\_DIR/docker

START\_TIME=$(date \+%s)

\# â”€â”€ Logging â”€â”€  
mkdir \-p /var/log/ai-platform  
exec \> \>(tee \-a $LOG\_FILE) 2\>&1  
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
echo "  Script 2 â€” Deploy AI Platform Services"  
echo "  Started: $(date)"  
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

\# â”€â”€ PHASE 1: Validate â”€â”€  
if \[\[ \! \-f $ENV\_DIR/master.env \]\]; then  
  echo "ERROR: master.env not found. Run Script 1 first."  
  exit 1  
fi  
if \! grep \-q "SCRIPT\_1\_COMPLETED" $ENV\_DIR/master.env; then  
  echo "ERROR: Script 1 did not complete. Re-run Script 1."  
  exit 1  
fi

\# â”€â”€ PHASE 2: Source config â”€â”€  
set \-a  
source $ENV\_DIR/master.env  
set \+a

echo "Configuration loaded. Deploying services: $ENABLED\_SERVICES"

### **PHASE 3: Generate Per-Service Environment Files**

generate\_env\_files()  
â”‚  
â”œâ”€â”€ postgres.env:  
â”‚     POSTGRES\_USER=${POSTGRES\_USER}  
â”‚     POSTGRES\_PASSWORD=${POSTGRES\_PASSWORD}  
â”‚     POSTGRES\_DB=aiplatform  
â”‚     PGDATA=/var/lib/postgresql/data/pgdata  
â”‚  
â”œâ”€â”€ redis.env:  
â”‚     REDIS\_PASSWORD=${REDIS\_PASSWORD}  
â”‚  
â”œâ”€â”€ litellm.env:  
â”‚     LITELLM\_MASTER\_KEY=${LITELLM\_MASTER\_KEY}  
â”‚     LITELLM\_SALT\_KEY=${LITELLM\_SALT\_KEY}  
â”‚     DATABASE\_URL=postgresql://${POSTGRES\_USER}:${POSTGRES\_PASSWORD}@postgres:5432/litellm  
â”‚     REDIS\_HOST=redis  
â”‚     REDIS\_PORT=6379  
â”‚     REDIS\_PASSWORD=${REDIS\_PASSWORD}  
â”‚     STORE\_MODEL\_IN\_DB=True  
â”‚     OPENAI\_API\_KEY=${OPENAI\_API\_KEY:-}  
â”‚     ANTHROPIC\_API\_KEY=${ANTHROPIC\_API\_KEY:-}  
â”‚     GOOGLE\_AI\_API\_KEY=${GOOGLE\_AI\_API\_KEY:-}  
â”‚     AZURE\_API\_KEY=${AZURE\_API\_KEY:-}  
â”‚     AZURE\_API\_BASE=${AZURE\_API\_BASE:-}  
â”‚     AWS\_ACCESS\_KEY\_ID=${AWS\_ACCESS\_KEY\_ID:-}  
â”‚     AWS\_SECRET\_ACCESS\_KEY=${AWS\_SECRET\_ACCESS\_KEY:-}  
â”‚     AWS\_REGION\_NAME=${AWS\_REGION\_NAME:-}  
â”‚  
â”œâ”€â”€ dify.env (if dify in ENABLED\_SERVICES):  
â”‚     \# Core  
â”‚     MODE=api  
â”‚     LOG\_LEVEL=INFO  
â”‚     SECRET\_KEY=${DIFY\_SECRET\_KEY}  
â”‚     INIT\_PASSWORD=${DIFY\_INIT\_PASSWORD}  
â”‚     CONSOLE\_WEB\_URL=https://${DIFY\_DOMAIN:-dify.localhost}  
â”‚     CONSOLE\_API\_URL=https://${DIFY\_DOMAIN:-dify.localhost}  
â”‚     SERVICE\_API\_URL=https://${DIFY\_DOMAIN:-dify.localhost}  
â”‚     APP\_WEB\_URL=https://${DIFY\_DOMAIN:-dify.localhost}  
â”‚     \# Database  
â”‚     DB\_USERNAME=${POSTGRES\_USER}  
â”‚     DB\_PASSWORD=${POSTGRES\_PASSWORD}  
â”‚     DB\_HOST=postgres  
â”‚     DB\_PORT=5432  
â”‚     DB\_DATABASE=dify  
â”‚     \# Redis  
â”‚     REDIS\_HOST=redis  
â”‚     REDIS\_PORT=6379  
â”‚     REDIS\_PASSWORD=${REDIS\_PASSWORD}  
â”‚     REDIS\_USE\_SSL=false  
â”‚     REDIS\_DB=1  
â”‚     \# Celery (Dify uses separate Redis DBs)  
â”‚     CELERY\_BROKER\_URL=redis://:${REDIS\_PASSWORD}@redis:6379/2  
â”‚     \# Storage  
â”‚     STORAGE\_TYPE=local  
â”‚     STORAGE\_LOCAL\_PATH=/app/api/storage  
â”‚     \# Vector Store â€” route through selected vector DB  
â”‚     VECTOR\_STORE=${VECTOR\_DB}  
â”‚     $(case  $ VECTOR\_DB in  
â”‚       qdrant)  
â”‚         echo "QDRANT\_URL=http://qdrant:6333"  
â”‚         echo "QDRANT\_API\_KEY= $ {QDRANT\_API\_KEY}"  
â”‚         ;;  
â”‚       weaviate)  
â”‚         echo "WEAVIATE\_ENDPOINT=http://weaviate:8080"  
â”‚         echo "WEAVIATE\_API\_KEY=${WEAVIATE\_API\_KEY}"  
â”‚         ;;  
â”‚       milvus)  
â”‚         echo "MILVUS\_HOST=milvus"  
â”‚         echo "MILVUS\_PORT=19530"  
â”‚         echo "MILVUS\_TOKEN=${MILVUS\_TOKEN}"  
â”‚         ;;  
â”‚     esac)  
â”‚     \# LLM â€” point Dify at LiteLLM  
â”‚     LITELLM\_API\_BASE=http://litellm:4000  
â”‚     LITELLM\_API\_KEY=${LITELLM\_MASTER\_KEY}  
â”‚  
â”œâ”€â”€ n8n.env (if n8n in ENABLED\_SERVICES):  
â”‚     DB\_TYPE=postgresdb  
â”‚     DB\_POSTGRESDB\_HOST=postgres  
â”‚     DB\_POSTGRESDB\_PORT=5432  
â”‚     DB\_POSTGRESDB\_DATABASE=n8n  
â”‚     DB\_POSTGRESDB\_USER=${POSTGRES\_USER}  
â”‚     DB\_POSTGRESDB\_PASSWORD=${POSTGRES\_PASSWORD}  
â”‚     N8N\_ENCRYPTION\_KEY=${N8N\_ENCRYPTION\_KEY}  
â”‚     N8N\_HOST=0.0.0.0  
â”‚     N8N\_PORT=5678  
â”‚     N8N\_PROTOCOL=https  
â”‚     WEBHOOK\_URL=https://${N8N\_DOMAIN:-n8n.localhost}  
â”‚     N8N\_EDITOR\_BASE\_URL=https://${N8N\_DOMAIN:-n8n.localhost}  
â”‚     GENERIC\_TIMEZONE=UTC  
â”‚     N8N\_METRICS=true  
â”‚     N8N\_DIAGNOSTICS\_ENABLED=false  
â”‚     \# Community nodes  
â”‚     N8N\_COMMUNITY\_PACKAGES\_ENABLED=true  
â”‚     NODE\_FUNCTION\_ALLOW\_EXTERNAL=\*  
â”‚  
â”œâ”€â”€ openwebui.env (if open-webui in ENABLED\_SERVICES):  
â”‚     OLLAMA\_BASE\_URL=${OLLAMA\_BASE\_URL}  
â”‚     OPENAI\_API\_BASE\_URL=http://litellm:4000/v1  
â”‚     OPENAI\_API\_KEY=${LITELLM\_MASTER\_KEY}  
â”‚     WEBUI\_SECRET\_KEY=${JWT\_SECRET}  
â”‚     ENABLE\_SIGNUP=true  
â”‚     DEFAULT\_MODELS=mistral  
â”‚     ENABLE\_RAG\_WEB\_SEARCH=true  
â”‚     RAG\_EMBEDDING\_ENGINE=openai  
â”‚     RAG\_EMBEDDING\_MODEL=nomic-embed-text  
â”‚     RAG\_OPENAI\_API\_BASE\_URL=http://litellm:4000/v1  
â”‚     RAG\_OPENAI\_API\_KEY=${LITELLM\_MASTER\_KEY}  
â”‚     DATA\_DIR=/app/backend/data  
â”‚  
â”œâ”€â”€ flowise.env (if flowise in ENABLED\_SERVICES):  
â”‚     DATABASE\_TYPE=postgres  
â”‚     DATABASE\_HOST=postgres  
â”‚     DATABASE\_PORT=5432  
â”‚     DATABASE\_NAME=flowise  
â”‚     DATABASE\_USER=${POSTGRES\_USER}  
â”‚     DATABASE\_PASSWORD=${POSTGRES\_PASSWORD}  
â”‚     FLOWISE\_USERNAME=${FLOWISE\_USERNAME}  
â”‚     FLOWISE\_PASSWORD=${FLOWISE\_PASSWORD}  
â”‚     APIKEY\_PATH=/root/.flowise  
â”‚     LOG\_LEVEL=info  
â”‚     EXECUTION\_MODE=main  
â”‚  
â”œâ”€â”€ anythingllm.env (if anythingllm in ENABLED\_SERVICES):  
â”‚     STORAGE\_DIR=/app/server/storage  
â”‚     LLM\_PROVIDER=litellm  
â”‚     LITELLM\_BASE\_PATH=http://litellm:4000  
â”‚     LITELLM\_API\_KEY=${LITELLM\_MASTER\_KEY}  
â”‚     EMBEDDING\_ENGINE=native  
â”‚     VECTOR\_DB=${VECTOR\_DB}  
â”‚     $(case  $ VECTOR\_DB in  
â”‚       qdrant) echo "QDRANT\_ENDPOINT=http://qdrant:6333" && echo "QDRANT\_API\_KEY= $ {QDRANT\_API\_KEY}" ;;  
â”‚       weaviate) echo "WEAVIATE\_ENDPOINT=http://weaviate:8080" && echo "WEAVIATE\_API\_KEY=${WEAVIATE\_API\_KEY}" ;;  
â”‚       milvus) echo "MILVUS\_ADDRESS=milvus:19530" && echo "MILVUS\_TOKEN=${MILVUS\_TOKEN}" ;;  
â”‚     esac)  
â”‚     AUTH\_TOKEN=${ANYTHINGLLM\_API\_KEY}  
â”‚     JWT\_SECRET=${JWT\_SECRET}  
â”‚  
â”œâ”€â”€ supertokens.env (if supertokens in ENABLED\_SERVICES):  
â”‚     POSTGRESQL\_CONNECTION\_URI=postgresql://${POSTGRES\_USER}:${POSTGRES\_PASSWORD}@postgres:5432/supertokens  
â”‚     API\_KEYS=${SUPERTOKENS\_API\_KEY}  
â”‚  
â””â”€â”€ Write all env files to $ENV\_DIR/\<service\>.env with chmod 600

### **PHASE 4: Generate LiteLLM Config**

generate\_litellm\_config()  
â”‚  
â”œâ”€â”€ cat \> $CONFIG\_DIR/litellm/litellm\_config.yaml \<\< 'YAML'  
â”‚   model\_list:  
â”‚     \# â”€â”€ Local Ollama Models â”€â”€  
â”‚     \# (populated dynamically based on MODEL\_TIER)  
â”‚     
â”‚   YAML  
â”‚  
â”œâ”€â”€ \# Add Ollama models based on tier  
â”‚   cat \>\> $CONFIG\_DIR/litellm/litellm\_config.yaml \<\< YAML  
â”‚     
â”‚     \- model\_name: tinyllama  
â”‚       litellm\_params:  
â”‚         model: ollama/tinyllama  
â”‚         api\_base: ${OLLAMA\_BASE\_URL}  
â”‚     
â”‚     \- model\_name: nomic-embed-text  
â”‚       litellm\_params:  
â”‚         model: ollama/nomic-embed-text  
â”‚         api\_base: ${OLLAMA\_BASE\_URL}  
â”‚   YAML  
â”‚  
â”‚   if \[\[ " $ MODEL\_TIER" \=\~ ^(standard|full|custom) $  \]\]; then  
â”‚     cat \>\> $CONFIG\_DIR/litellm/litellm\_config.yaml \<\< YAML  
â”‚     
â”‚     \- model\_name: mistral  
â”‚       litellm\_params:  
â”‚         model: ollama/mistral  
â”‚         api\_base: ${OLLAMA\_BASE\_URL}  
â”‚     
â”‚     \- model\_name: llama3.1  
â”‚       litellm\_params:  
â”‚         model: ollama/llama3.1:8b  
â”‚         api\_base: ${OLLAMA\_BASE\_URL}  
â”‚     
â”‚     \- model\_name: codellama  
â”‚       litellm\_params:  
â”‚         model: ollama/codellama:7b  
â”‚         api\_base: ${OLLAMA\_BASE\_URL}  
â”‚     YAML  
â”‚   fi  
â”‚  
â”‚   if \[\[ "$MODEL\_TIER" \== "full" \]\]; then  
â”‚     cat \>\> $CONFIG\_DIR/litellm/litellm\_config.yaml \<\< YAML  
â”‚     
â”‚     \- model\_name: llama3.1-70b  
â”‚       litellm\_params:  
â”‚         model: ollama/llama3.1:70b  
â”‚         api\_base: ${OLLAMA\_BASE\_URL}  
â”‚     
â”‚     \- model\_name: mixtral  
â”‚       litellm\_params:  
â”‚         model: ollama/mixtral:8x7b  
â”‚         api\_base: ${OLLAMA\_BASE\_URL}  
â”‚     
â”‚     \- model\_name: deepseek-coder  
â”‚       litellm\_params:  
â”‚         model: ollama/deepseek-coder-v2  
â”‚         api\_base: ${OLLAMA\_BASE\_URL}  
â”‚     YAML  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# Add cloud provider models if API keys present  
â”‚   if \[\[ \-n "${OPENAI\_API\_KEY:-}" \]\]; then  
â”‚     cat \>\>  $ CONFIG\_DIR/litellm/litellm\_config.yaml \<\< YAML  
â”‚     
â”‚     \# â”€â”€ OpenAI Models â”€â”€  
â”‚     \- model\_name: gpt-4o  
â”‚       litellm\_params:  
â”‚         model: openai/gpt-4o  
â”‚         api\_key: os.environ/OPENAI\_API\_KEY  
â”‚     
â”‚     \- model\_name: gpt-4o-mini  
â”‚       litellm\_params:  
â”‚         model: openai/gpt-4o-mini  
â”‚         api\_key: os.environ/OPENAI\_API\_KEY  
â”‚     
â”‚     \- model\_name: gpt-4-turbo  
â”‚       litellm\_params:  
â”‚         model: openai/gpt-4-turbo  
â”‚         api\_key: os.environ/OPENAI\_API\_KEY  
â”‚     
â”‚     \- model\_name: gpt-3.5-turbo  
â”‚       litellm\_params:  
â”‚         model: openai/gpt-3.5-turbo  
â”‚         api\_key: os.environ/OPENAI\_API\_KEY  
â”‚     
â”‚     \- model\_name: text-embedding-3-small  
â”‚       litellm\_params:  
â”‚         model: openai/text-embedding-3-small  
â”‚         api\_key: os.environ/OPENAI\_API\_KEY  
â”‚     
â”‚     \- model\_name: text-embedding-3-large  
â”‚       litellm\_params:  
â”‚         model: openai/text-embedding-3-large  
â”‚         api\_key: os.environ/OPENAI\_API\_KEY  
â”‚     YAML  
â”‚   fi  
â”‚  
â”‚   if \[\[ \-n " $ {ANTHROPIC\_API\_KEY:-}" \]\]; then  
â”‚     cat \>\>  $ CONFIG\_DIR/litellm/litellm\_config.yaml \<\< YAML  
â”‚     
â”‚     \# â”€â”€ Anthropic Models â”€â”€  
â”‚     \- model\_name: claude-3.5-sonnet  
â”‚       litellm\_params:  
â”‚         model: anthropic/claude-3-5-sonnet-20241022  
â”‚         api\_key: os.environ/ANTHROPIC\_API\_KEY  
â”‚     
â”‚     \- model\_name: claude-3-opus  
â”‚       litellm\_params:  
â”‚         model: anthropic/claude-3-opus-20240229  
â”‚         api\_key: os.environ/ANTHROPIC\_API\_KEY  
â”‚     
â”‚     \- model\_name: claude-3-haiku  
â”‚       litellm\_params:  
â”‚         model: anthropic/claude-3-haiku-20240307  
â”‚         api\_key: os.environ/ANTHROPIC\_API\_KEY  
â”‚     YAML  
â”‚   fi  
â”‚  
â”‚   if \[\[ \-n " $ {GOOGLE\_AI\_API\_KEY:-}" \]\]; then  
â”‚     cat \>\>  $ CONFIG\_DIR/litellm/litellm\_config.yaml \<\< YAML  
â”‚     
â”‚     \# â”€â”€ Google AI Models â”€â”€  
â”‚     \- model\_name: gemini-pro  
â”‚       litellm\_params:  
â”‚         model: gemini/gemini-1.5-pro  
â”‚         api\_key: os.environ/GOOGLE\_AI\_API\_KEY  
â”‚     
â”‚     \- model\_name: gemini-flash  
â”‚       litellm\_params:  
â”‚         model: gemini/gemini-1.5-flash  
â”‚         api\_key: os.environ/GOOGLE\_AI\_API\_KEY  
â”‚     YAML  
â”‚   fi  
â”‚  
â”‚   if \[\[ \-n " $ {AWS\_ACCESS\_KEY\_ID:-}" \]\]; then  
â”‚     cat \>\> $CONFIG\_DIR/litellm/litellm\_config.yaml \<\< YAML  
â”‚     
â”‚     \# â”€â”€ AWS Bedrock Models â”€â”€  
â”‚     \- model\_name: bedrock-claude-3.5-sonnet  
â”‚       litellm\_params:  
â”‚         model: bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0  
â”‚     
â”‚     \- model\_name: bedrock-claude-3-haiku  
â”‚       litellm\_params:  
â”‚         model: bedrock/anthropic.claude-3-haiku-20240307-v1:0  
â”‚     YAML  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# Add router settings and general config  
â”‚   cat \>\> $CONFIG\_DIR/litellm/litellm\_config.yaml \<\< YAML  
â”‚     
â”‚   litellm\_settings:  
â”‚     drop\_params: true  
â”‚     set\_verbose: false  
â”‚     cache: true  
â”‚     cache\_params:  
â”‚       type: redis  
â”‚       host: redis  
â”‚       port: 6379  
â”‚       password: ${REDIS\_PASSWORD}  
â”‚     success\_callback: \["prometheus"\]  
â”‚     failure\_callback: \["prometheus"\]  
â”‚     max\_budget: 1000  
â”‚     budget\_duration: monthly  
â”‚     
â”‚   router\_settings:  
â”‚     routing\_strategy: simple-shuffle  
â”‚     num\_retries: 3  
â”‚     timeout: 120  
â”‚     retry\_after: 5  
â”‚     allowed\_fails: 3  
â”‚     cooldown\_time: 60  
â”‚     
â”‚   general\_settings:  
â”‚     master\_key: ${LITELLM\_MASTER\_KEY}  
â”‚     database\_url: postgresql://${POSTGRES\_USER}:${POSTGRES\_PASSWORD}@postgres:5432/litellm  
â”‚     store\_model\_in\_db: true  
â”‚   YAML  
â”‚  
â””â”€â”€ echo "LiteLLM config generated with $(grep 'model\_name:' $CONFIG\_DIR/litellm/litellm\_config.yaml | wc \-l) models"

### **PHASE 5: Generate Caddyfile**

generate\_caddyfile()  
â”‚  
â”œâ”€â”€ If DOMAIN\_MODE \== "production":  
â”‚     cat \> $CONFIG\_DIR/caddy/Caddyfile \<\< CADDYFILE  
â”‚     {  
â”‚       email ${ACME\_EMAIL}  
â”‚       acme\_ca https://acme-v02.api.letsencrypt.org/directory  
â”‚     }  
â”‚     CADDYFILE  
â”‚  
â”œâ”€â”€ If DOMAIN\_MODE \== "local":  
â”‚     cat \> $CONFIG\_DIR/caddy/Caddyfile \<\< CADDYFILE  
â”‚     {  
â”‚       auto\_https off  
â”‚     }  
â”‚     CADDYFILE  
â”‚  
â”œâ”€â”€ \# Common snippet for security headers  
â”‚   cat \>\>  $ CONFIG\_DIR/caddy/Caddyfile \<\< 'CADDYFILE'  
â”‚     
â”‚   (security\_headers) {  
â”‚     header {  
â”‚       X-Content-Type-Options nosniff  
â”‚       X-Frame-Options SAMEORIGIN  
â”‚       Referrer-Policy strict-origin-when-cross-origin  
â”‚       X-XSS-Protection "1; mode=block"  
â”‚       \-Server  
â”‚     }  
â”‚   }  
â”‚     
â”‚   (proxy\_defaults) {  
â”‚     header\_up X-Real-IP {remote\_host}  
â”‚     header\_up X-Forwarded-For {remote\_host}  
â”‚     header\_up X-Forwarded-Proto {scheme}  
â”‚   }  
â”‚   CADDYFILE  
â”‚  
â”œâ”€â”€ \# Generate service blocks based on ENABLED\_SERVICES  
â”‚   IFS=',' read \-ra SERVICES \<\<\< " $ ENABLED\_SERVICES"  
â”‚  
â”‚   \# Helper function â€” add\_service\_block(domain, upstream, options)  
â”‚   add\_service\_block() {  
â”‚     local domain= $ 1  
â”‚     local upstream= $ 2  
â”‚     local websocket=${3:-false}  
â”‚       
â”‚     if \[\[ " $ DOMAIN\_MODE" \== "local" \]\]; then  
â”‚       domain=": $ {4:-80}"  \# Fallback port for local mode  
â”‚     fi  
â”‚       
â”‚     cat \>\> $CONFIG\_DIR/caddy/Caddyfile \<\< CADDYFILE  
â”‚     
â”‚   ${domain} {  
â”‚     import security\_headers  
â”‚     reverse\_proxy ${upstream} {  
â”‚       import proxy\_defaults  
â”‚    $ (if \[\[ " $ websocket" \== "true" \]\]; then echo "    transport http {  
â”‚         keepalive 30s  
â”‚       }"; fi)  
â”‚     }  
â”‚   }  
â”‚   CADDYFILE  
â”‚   }  
â”‚  
â”‚   for service in "${SERVICES\[@\]}"; do  
â”‚     case  $ service in  
â”‚       dify)  
â”‚         add\_service\_block " $ {DIFY\_DOMAIN}" "dify-nginx:80" "false"  
â”‚         ;;  
â”‚       n8n)  
â”‚         add\_service\_block "${N8N\_DOMAIN}" "n8n:5678" "true"  
â”‚         ;;  
â”‚       open-webui)  
â”‚         add\_service\_block "${WEBUI\_DOMAIN}" "open-webui:8080" "true"  
â”‚         ;;  
â”‚       flowise)  
â”‚         add\_service\_block "${FLOWISE\_DOMAIN}" "flowise:3000" "true"  
â”‚         ;;  
â”‚       litellm)  
â”‚         add\_service\_block "${LITELLM\_DOMAIN}" "litellm:4000" "false"  
â”‚         ;;  
â”‚       monitoring)  
â”‚         add\_service\_block "${GRAFANA\_DOMAIN}" "grafana:3000" "true"  
â”‚         ;;  
â”‚       portainer)  
â”‚         add\_service\_block "${PORTAINER\_DOMAIN}" "portainer:9000" "true"  
â”‚         ;;  
â”‚       anythingllm)  
â”‚         add\_service\_block "${ANYTHINGLLM\_DOMAIN}" "anythingllm:3001" "true"  
â”‚         ;;  
â”‚       supertokens)  
â”‚         add\_service\_block "${SUPERTOKENS\_DOMAIN}" "supertokens:3567" "false"  
â”‚         ;;  
â”‚     esac  
â”‚   done  
â”‚  
â””â”€â”€ echo "Caddyfile generated for $(grep \-c 'reverse\_proxy' $CONFIG\_DIR/caddy/Caddyfile) services"

### **PHASE 6: Generate docker-compose.yml (Dynamic)**

This is the largest phase. The compose file is **generated dynamically** based on ENABLED\_SERVICES.

generate\_docker\_compose()  
â”‚  
â”œâ”€â”€ \# â”€â”€ Header â”€â”€  
â”‚   cat \> $COMPOSE\_DIR/docker-compose.yml \<\< 'YAML'  
â”‚   \# AI Platform â€” Auto-generated by Script 2  
â”‚   \# Do not edit manually â€” regenerate with:  
â”‚   \#   sudo bash /opt/ai-platform/scripts/script-2-deploy.sh \--regenerate  
â”‚     
â”‚   version: "3.8"  
â”‚     
â”‚   x-common-env: \&common-env  
â”‚     TZ: UTC  
â”‚     PUID: 1000  
â”‚     PGID: 1000  
â”‚   YAML  
â”‚  
â”œâ”€â”€ \# â”€â”€ Networks Section â”€â”€  
â”‚   cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< 'YAML'  
â”‚     
â”‚   networks:  
â”‚     ai-platform:  
â”‚       driver: bridge  
â”‚       name: ai-platform  
â”‚     monitoring:  
â”‚       driver: bridge  
â”‚       name: monitoring  
â”‚   YAML  
â”‚  
â”œâ”€â”€ \# â”€â”€ Volumes Section â”€â”€  
â”‚   cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚   volumes:  
â”‚     postgres-data:  
â”‚       driver: local  
â”‚       driver\_opts:  
â”‚         type: none  
â”‚         o: bind  
â”‚         device: ${DATA\_DIR}/postgres  
â”‚     redis-data:  
â”‚       driver: local  
â”‚       driver\_opts:  
â”‚         type: none  
â”‚         o: bind  
â”‚         device: ${DATA\_DIR}/redis  
â”‚     caddy-data:  
â”‚       driver: local  
â”‚       driver\_opts:  
â”‚         type: none  
â”‚         o: bind  
â”‚         device: ${DATA\_DIR}/caddy/data  
â”‚     caddy-config:  
â”‚       driver: local  
â”‚       driver\_opts:  
â”‚         type: none  
â”‚         o: bind  
â”‚         device: ${DATA\_DIR}/caddy/config  
â”‚     portainer-data:  
â”‚       driver: local  
â”‚       driver\_opts:  
â”‚         type: none  
â”‚         o: bind  
â”‚         device: ${DATA\_DIR}/portainer  
â”‚   YAML  
â”‚  
â”‚   \# Add service-specific volumes  
â”‚   if service\_enabled "dify"; then  
â”‚     cat \>\> ... dify-storage volume  
â”‚   fi  
â”‚   if service\_enabled "n8n"; then  
â”‚     cat \>\> ... n8n-data volume  
â”‚   fi  
â”‚   \# ... etc for each service  
â”‚  
â”œâ”€â”€ \# â”€â”€ Services Section â”€â”€  
â”‚   echo "" \>\> $COMPOSE\_DIR/docker-compose.yml  
â”‚   echo "services:" \>\> $COMPOSE\_DIR/docker-compose.yml  
â”‚  
â”œâ”€â”€ \# â”€â”€ PostgreSQL (always) â”€â”€  
â”‚   cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     postgres:  
â”‚       image: pgvector/pgvector:pg16  
â”‚       container\_name: postgres  
â”‚       restart: unless-stopped  
â”‚       env\_file: ${ENV\_DIR}/postgres.env  
â”‚       volumes:  
â”‚         \- postgres-data:/var/lib/postgresql/data  
â”‚         \- ${CONFIG\_DIR}/postgres/init-databases.sql:/docker-entrypoint-initdb.d/init.sql:ro  
â”‚       ports:  
â”‚         \- "127.0.0.1:5432:5432"  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       healthcheck:  
â”‚         test: \["CMD-SHELL", "pg\_isready \-U ${POSTGRES\_USER}"\]  
â”‚         interval: 10s  
â”‚         timeout: 5s  
â”‚         retries: 5  
â”‚         start\_period: 30s  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 2G  
â”‚           reservations:  
â”‚             memory: 512M  
â”‚   YAML  
â”‚  
â”œâ”€â”€ \# â”€â”€ Redis (always) â”€â”€  
â”‚   cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     redis:  
â”‚       image: redis:7-alpine  
â”‚       container\_name: redis  
â”‚       restart: unless-stopped  
â”‚       command: \>  
â”‚         redis-server  
â”‚         \--requirepass ${REDIS\_PASSWORD}  
â”‚         \--maxmemory 1gb  
â”‚         \--maxmemory-policy allkeys-lru  
â”‚         \--appendonly yes  
â”‚         \--appendfsync everysec  
â”‚       volumes:  
â”‚         \- redis-data:/data  
â”‚       ports:  
â”‚         \- "127.0.0.1:6379:6379"  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       healthcheck:  
â”‚         test: \["CMD", "redis-cli", "-a", "${REDIS\_PASSWORD}", "ping"\]  
â”‚         interval: 10s  
â”‚         timeout: 5s  
â”‚         retries: 5  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 1G  
â”‚   YAML  
â”‚  
â”œâ”€â”€ \# â”€â”€ LiteLLM (always) â”€â”€  
â”‚   cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     litellm:  
â”‚       image: ghcr.io/berriai/litellm:main-latest  
â”‚       container\_name: litellm  
â”‚       restart: unless-stopped  
â”‚       env\_file: ${ENV\_DIR}/litellm.env  
â”‚       volumes:  
â”‚         \- ${CONFIG\_DIR}/litellm/litellm\_config.yaml:/app/config.yaml:ro  
â”‚       command: \["--config", "/app/config.yaml", "--port", "4000"\]  
â”‚       ports:  
â”‚         \- "127.0.0.1:4000:4000"  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚         \- monitoring  
â”‚       depends\_on:  
â”‚         postgres:  
â”‚           condition: service\_healthy  
â”‚         redis:  
â”‚           condition: service\_healthy  
â”‚       extra\_hosts:  
â”‚         \- "host.docker.internal:host-gateway"  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:4000/health"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 5  
â”‚         start\_period: 45s  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 2G  
â”‚   YAML  
â”‚  
â”œâ”€â”€ \# â”€â”€ Vector DB (based on VECTOR\_DB selection) â”€â”€  
â”‚   case $VECTOR\_DB in  
â”‚     qdrant)  
â”‚       cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     qdrant:  
â”‚       image: qdrant/qdrant:latest  
â”‚       container\_name: qdrant  
â”‚       restart: unless-stopped  
â”‚       environment:  
â”‚         QDRANT\_\_SERVICE\_\_API\_KEY: ${QDRANT\_API\_KEY}  
â”‚         QDRANT\_\_STORAGE\_\_STORAGE\_PATH: /qdrant/storage  
â”‚         QDRANT\_\_SERVICE\_\_GRPC\_PORT: 6334  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/qdrant/storage:/qdrant/storage  
â”‚         \- ${DATA\_DIR}/qdrant/snapshots:/qdrant/snapshots  
â”‚       ports:  
â”‚         \- "127.0.0.1:6333:6333"  
â”‚         \- "127.0.0.1:6334:6334"  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:6333/healthz"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 3  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 4G  
â”‚   YAML  
â”‚       ;;  
â”‚     weaviate)  
â”‚       cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     weaviate:  
â”‚       image: semitechnologies/weaviate:latest  
â”‚       container\_name: weaviate  
â”‚       restart: unless-stopped  
â”‚       environment:  
â”‚         QUERY\_DEFAULTS\_LIMIT: 25  
â”‚         AUTHENTICATION\_APIKEY\_ENABLED: "true"  
â”‚         AUTHENTICATION\_APIKEY\_ALLOWED\_KEYS: ${WEAVIATE\_API\_KEY}  
â”‚         AUTHENTICATION\_APIKEY\_USERS: admin  
â”‚         PERSISTENCE\_DATA\_PATH: /var/lib/weaviate  
â”‚         DEFAULT\_VECTORIZER\_MODULE: none  
â”‚         CLUSTER\_HOSTNAME: weaviate-node1  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/weaviate:/var/lib/weaviate  
â”‚       ports:  
â”‚         \- "127.0.0.1:8080:8080"  
â”‚         \- "127.0.0.1:50051:50051"  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:8080/v1/.well-known/ready"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 3  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 4G  
â”‚   YAML  
â”‚       ;;  
â”‚     milvus)  
â”‚       cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     etcd:  
â”‚       image: quay.io/coreos/etcd:v3.5.11  
â”‚       container\_name: milvus-etcd  
â”‚       restart: unless-stopped  
â”‚       environment:  
â”‚         ETCD\_AUTO\_COMPACTION\_MODE: revision  
â”‚         ETCD\_AUTO\_COMPACTION\_RETENTION: "1000"  
â”‚         ETCD\_QUOTA\_BACKEND\_BYTES: "4294967296"  
â”‚         ETCD\_SNAPSHOT\_COUNT: "50000"  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/milvus/etcd:/etcd  
â”‚       command: \>  
â”‚         etcd  
â”‚         \-advertise-client-urls=http://127.0.0.1:2379  
â”‚         \-listen-client-urls=http://0.0.0.0:2379  
â”‚         \--data-dir /etcd  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       healthcheck:  
â”‚         test: \["CMD", "etcdctl", "endpoint", "health"\]  
â”‚         interval: 30s  
â”‚         timeout: 20s  
â”‚         retries: 3  
â”‚     
â”‚     minio-milvus:  
â”‚       image: minio/minio:latest  
â”‚       container\_name: milvus-minio  
â”‚       restart: unless-stopped  
â”‚       environment:  
â”‚         MINIO\_ACCESS\_KEY: minioadmin  
â”‚         MINIO\_SECRET\_KEY: minioadmin  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/milvus/minio:/minio\_data  
â”‚       command: minio server /minio\_data \--console-address ":9001"  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"\]  
â”‚         interval: 30s  
â”‚         timeout: 20s  
â”‚         retries: 3  
â”‚     
â”‚     milvus:  
â”‚       image: milvusdb/milvus:v2.4-latest  
â”‚       container\_name: milvus  
â”‚       restart: unless-stopped  
â”‚       environment:  
â”‚         ETCD\_ENDPOINTS: etcd:2379  
â”‚         MINIO\_ADDRESS: minio-milvus:9000  
â”‚         COMMON\_SECURITY\_AUTHORIZATIONENABLED: "true"  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/milvus/data:/var/lib/milvus  
â”‚       ports:  
â”‚         \- "127.0.0.1:19530:19530"  
â”‚         \- "127.0.0.1:9091:9091"  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       depends\_on:  
â”‚         etcd:  
â”‚           condition: service\_healthy  
â”‚         minio-milvus:  
â”‚           condition: service\_healthy  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:9091/healthz"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 3  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 4G  
â”‚   YAML  
â”‚       ;;  
â”‚   esac  
â”‚  
â”œâ”€â”€ \# â”€â”€ Dify (if enabled) â”€â”€  
â”‚   if service\_enabled "dify"; then  
â”‚     cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     dify-api:  
â”‚       image: langgenius/dify-api:latest  
â”‚       container\_name: dify-api  
â”‚       restart: unless-stopped  
â”‚       env\_file: ${ENV\_DIR}/dify.env  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/dify/storage:/app/api/storage  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       depends\_on:  
â”‚         postgres:  
â”‚           condition: service\_healthy  
â”‚         redis:  
â”‚           condition: service\_healthy  
â”‚       extra\_hosts:  
â”‚         \- "host.docker.internal:host-gateway"  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:5001/health"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 5  
â”‚         start\_period: 60s  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 2G  
â”‚     
â”‚     dify-worker:  
â”‚       image: langgenius/dify-api:latest  
â”‚       container\_name: dify-worker  
â”‚       restart: unless-stopped  
â”‚       env\_file: ${ENV\_DIR}/dify.env  
â”‚       environment:  
â”‚         MODE: worker  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/dify/storage:/app/api/storage  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       depends\_on:  
â”‚         postgres:  
â”‚           condition: service\_healthy  
â”‚         redis:  
â”‚           condition: service\_healthy  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 2G  
â”‚     
â”‚     dify-web:  
â”‚       image: langgenius/dify-web:latest  
â”‚       container\_name: dify-web  
â”‚       restart: unless-stopped  
â”‚       environment:  
â”‚         CONSOLE\_API\_URL: https://${DIFY\_DOMAIN}  
â”‚         APP\_API\_URL: https://${DIFY\_DOMAIN}  
â”‚         SENTRY\_DSN: ""  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚     
â”‚     dify-nginx:  
â”‚       image: nginx:alpine  
â”‚       container\_name: dify-nginx  
â”‚       restart: unless-stopped  
â”‚       volumes:  
â”‚         \- ${CONFIG\_DIR}/dify/nginx.conf:/etc/nginx/nginx.conf:ro  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       depends\_on:  
â”‚         \- dify-api  
â”‚         \- dify-web  
â”‚   YAML  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# â”€â”€ n8n (if enabled) â”€â”€  
â”‚   if service\_enabled "n8n"; then  
â”‚     cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     n8n:  
â”‚       image: n8nio/n8n:latest  
â”‚       container\_name: n8n  
â”‚       restart: unless-stopped  
â”‚       env\_file: ${ENV\_DIR}/n8n.env  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/n8n:/home/node/.n8n  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       depends\_on:  
â”‚         postgres:  
â”‚           condition: service\_healthy  
â”‚         redis:  
â”‚           condition: service\_healthy  
â”‚       extra\_hosts:  
â”‚         \- "host.docker.internal:host-gateway"  
â”‚       healthcheck:  
â”‚         test: \["CMD-SHELL", "wget \-qO- http://localhost:5678/healthz || exit 1"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 5  
â”‚         start\_period: 30s  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 2G  
â”‚   YAML  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# â”€â”€ Open WebUI (if enabled) â”€â”€  
â”‚   if service\_enabled "open-webui"; then  
â”‚     cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     open-webui:  
â”‚       image: ghcr.io/open-webui/open-webui:main  
â”‚       container\_name: open-webui  
â”‚       restart: unless-stopped  
â”‚       env\_file: ${ENV\_DIR}/openwebui.env  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/open-webui:/app/backend/data  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       extra\_hosts:  
â”‚         \- "host.docker.internal:host-gateway"  
â”‚       depends\_on:  
â”‚         litellm:  
â”‚           condition: service\_healthy  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:8080/health"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 5  
â”‚         start\_period: 30s  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 2G  
â”‚   YAML  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# â”€â”€ Flowise (if enabled) â”€â”€  
â”‚   if service\_enabled "flowise"; then  
â”‚     cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     flowise:  
â”‚       image: flowiseai/flowise:latest  
â”‚       container\_name: flowise  
â”‚       restart: unless-stopped  
â”‚       env\_file: ${ENV\_DIR}/flowise.env  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/flowise:/root/.flowise  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       depends\_on:  
â”‚         postgres:  
â”‚           condition: service\_healthy  
â”‚       extra\_hosts:  
â”‚         \- "host.docker.internal:host-gateway"  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:3000"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 5  
â”‚         start\_period: 30s  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 2G  
â”‚   YAML  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# â”€â”€ AnythingLLM (if enabled) â”€â”€  
â”‚   if service\_enabled "anythingllm"; then  
â”‚     cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     anythingllm:  
â”‚       image: mintplexlabs/anythingllm:latest  
â”‚       container\_name: anythingllm  
â”‚       restart: unless-stopped  
â”‚       env\_file: ${ENV\_DIR}/anythingllm.env  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/anythingllm:/app/server/storage  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       extra\_hosts:  
â”‚         \- "host.docker.internal:host-gateway"  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:3001/api/ping"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 5  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 2G  
â”‚   YAML  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# â”€â”€ SuperTokens (if enabled) â”€â”€  
â”‚   if service\_enabled "supertokens"; then  
â”‚     cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     supertokens:  
â”‚       image: registry.supertokens.io/supertokens/supertokens-postgresql:latest  
â”‚       container\_name: supertokens  
â”‚       restart: unless-stopped  
â”‚       env\_file: ${ENV\_DIR}/supertokens.env  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       depends\_on:  
â”‚         postgres:  
â”‚           condition: service\_healthy  
â”‚       ports:  
â”‚         \- "127.0.0.1:3567:3567"  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:3567/hello"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 3  
â”‚   YAML  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# â”€â”€ Caddy (always) â”€â”€  
â”‚   cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     caddy:  
â”‚       image: caddy:2-alpine  
â”‚       container\_name: caddy  
â”‚       restart: unless-stopped  
â”‚       ports:  
â”‚         \- "80:80"  
â”‚         \- "443:443"  
â”‚         \- "443:443/udp"  
â”‚       volumes:  
â”‚         \- ${CONFIG\_DIR}/caddy/Caddyfile:/etc/caddy/Caddyfile:ro  
â”‚         \- caddy-data:/data  
â”‚         \- caddy-config:/config  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       healthcheck:  
â”‚         test: \["CMD", "caddy", "validate", "--config", "/etc/caddy/Caddyfile"\]  
â”‚         interval: 60s  
â”‚         timeout: 10s  
â”‚         retries: 3  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 512M  
â”‚   YAML  
â”‚  
â”œâ”€â”€ \# â”€â”€ Portainer (always) â”€â”€  
â”‚   cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     portainer:  
â”‚       image: portainer/portainer-ce:latest  
â”‚       container\_name: portainer  
â”‚       restart: unless-stopped  
â”‚       volumes:  
â”‚         \- /var/run/docker.sock:/var/run/docker.sock:ro  
â”‚         \- portainer-data:/data  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚       healthcheck:  
â”‚         test: \["CMD", "wget", "-qO-", "http://localhost:9000/api/system/status"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 3  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 512M  
â”‚   YAML  
â”‚  
â”œâ”€â”€ \# â”€â”€ Monitoring Stack (if enabled) â”€â”€  
â”‚   if service\_enabled "monitoring"; then  
â”‚     cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     prometheus:  
â”‚       image: prom/prometheus:latest  
â”‚       container\_name: prometheus  
â”‚       restart: unless-stopped  
â”‚       command:  
â”‚         \- '--config.file=/etc/prometheus/prometheus.yml'  
â”‚         \- '--storage.tsdb.path=/prometheus'  
â”‚         \- '--storage.tsdb.retention.time=30d'  
â”‚         \- '--web.console.libraries=/etc/prometheus/console\_libraries'  
â”‚         \- '--web.console.templates=/etc/prometheus/consoles'  
â”‚       volumes:  
â”‚         \- ${CONFIG\_DIR}/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro  
â”‚         \- ${DATA\_DIR}/prometheus:/prometheus  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚         \- monitoring  
â”‚       healthcheck:  
â”‚         test: \["CMD", "wget", "-qO-", "http://localhost:9090/-/healthy"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 3  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 1G  
â”‚     
â”‚     grafana:  
â”‚       image: grafana/grafana:latest  
â”‚       container\_name: grafana  
â”‚       restart: unless-stopped  
â”‚       environment:  
â”‚         GF\_SECURITY\_ADMIN\_USER: ${GRAFANA\_ADMIN\_USER}  
â”‚         GF\_SECURITY\_ADMIN\_PASSWORD: ${GRAFANA\_ADMIN\_PASSWORD}  
â”‚         GF\_USERS\_ALLOW\_SIGN\_UP: "false"  
â”‚         GF\_SERVER\_ROOT\_URL: https://${GRAFANA\_DOMAIN:-grafana.localhost}  
â”‚       volumes:  
â”‚         \- ${DATA\_DIR}/grafana:/var/lib/grafana  
â”‚         \- ${CONFIG\_DIR}/grafana/provisioning:/etc/grafana/provisioning:ro  
â”‚         \- ${CONFIG\_DIR}/grafana/dashboards:/var/lib/grafana/dashboards:ro  
â”‚       networks:  
â”‚         \- ai-platform  
â”‚         \- monitoring  
â”‚       depends\_on:  
â”‚         \- prometheus  
â”‚       healthcheck:  
â”‚         test: \["CMD", "curl", "-f", "http://localhost:3000/api/health"\]  
â”‚         interval: 30s  
â”‚         timeout: 10s  
â”‚         retries: 3  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 512M  
â”‚     
â”‚     node-exporter:  
â”‚       image: prom/node-exporter:latest  
â”‚       container\_name: node-exporter  
â”‚       restart: unless-stopped  
â”‚       command:  
â”‚         \- '--path.procfs=/host/proc'  
â”‚         \- '--path.rootfs=/rootfs'  
â”‚         \- '--path.sysfs=/host/sys'  
â”‚         \- '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'  
â”‚       volumes:  
â”‚         \- /proc:/host/proc:ro  
â”‚         \- /sys:/host/sys:ro  
â”‚         \- /:/rootfs:ro  
â”‚       networks:  
â”‚         \- monitoring  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 128M  
â”‚     
â”‚     cadvisor:  
â”‚       image: gcr.io/cadvisor/cadvisor:latest  
â”‚       container\_name: cadvisor  
â”‚       restart: unless-stopped  
â”‚       privileged: true  
â”‚       devices:  
â”‚         \- /dev/kmsg:/dev/kmsg  
â”‚       volumes:  
â”‚         \- /:/rootfs:ro  
â”‚         \- /var/run:/var/run:ro  
â”‚         \- /sys:/sys:ro  
â”‚         \- /var/lib/docker/:/var/lib/docker:ro  
â”‚       networks:  
â”‚         \- monitoring  
â”‚       deploy:  
â”‚         resources:  
â”‚           limits:  
â”‚             memory: 256M  
â”‚   YAML  
â”‚  
â”‚     \# Add nvidia-smi-exporter if GPU present  
â”‚     if \[\[ "$HAS\_GPU" \== "true" \]\]; then  
â”‚       cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< YAML  
â”‚     
â”‚     nvidia-smi-exporter:  
â”‚       image: nvcr.io/nvidia/k8s/dcgm-exporter:latest  
â”‚       container\_name: nvidia-exporter  
â”‚       restart: unless-stopped  
â”‚       runtime: nvidia  
â”‚       environment:  
â”‚         NVIDIA\_VISIBLE\_DEVICES: all  
â”‚       networks:  
â”‚         \- monitoring  
â”‚       deploy:  
â”‚         resources:  
â”‚           reservations:  
â”‚             devices:  
â”‚               \- driver: nvidia  
â”‚                 count: all  
â”‚                 capabilities: \[gpu\]  
â”‚   YAML  
â”‚     fi  
â”‚   fi  
â”‚  
â””â”€â”€ echo "docker-compose.yml generated: $(grep 'container\_name:' $COMPOSE\_DIR/docker-compose.yml | wc \-l) containers"

### **PHASE 7: Generate Monitoring Configs**

generate\_monitoring\_configs()  
â”‚  
â”œâ”€â”€ \# Prometheus config  
â”‚   cat \>  $ CONFIG\_DIR/prometheus/prometheus.yml \<\< YAML  
â”‚   global:  
â”‚     scrape\_interval: 15s  
â”‚     evaluation\_interval: 15s  
â”‚     
â”‚   scrape\_configs:  
â”‚     \- job\_name: 'prometheus'  
â”‚       static\_configs:  
â”‚         \- targets: \['localhost:9090'\]  
â”‚     
â”‚     \- job\_name: 'node-exporter'  
â”‚       static\_configs:  
â”‚         \- targets: \['node-exporter:9100'\]  
â”‚     
â”‚     \- job\_name: 'cadvisor'  
â”‚       static\_configs:  
â”‚         \- targets: \['cadvisor:8080'\]  
â”‚     
â”‚     \- job\_name: 'litellm'  
â”‚       metrics\_path: /metrics  
â”‚       static\_configs:  
â”‚         \- targets: \['litellm:4000'\]  
â”‚     
â”‚     \- job\_name: 'ollama'  
â”‚       metrics\_path: /metrics  
â”‚       static\_configs:  
â”‚         \- targets: \['host.docker.internal:11434'\]  
â”‚   YAML  
â”‚  
â”‚   if \[\[ " $ HAS\_GPU" \== "true" \]\]; then  
â”‚     cat \>\> $CONFIG\_DIR/prometheus/prometheus.yml \<\< YAML  
â”‚     
â”‚     \- job\_name: 'nvidia-gpu'  
â”‚       static\_configs:  
â”‚         \- targets: \['nvidia-exporter:9400'\]  
â”‚   YAML  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# Grafana datasource provisioning  
â”‚   mkdir \-p $CONFIG\_DIR/grafana/provisioning/datasources  
â”‚   cat \> $CONFIG\_DIR/grafana/provisioning/datasources/prometheus.yml \<\< YAML  
â”‚   apiVersion: 1  
â”‚   datasources:  
â”‚     \- name: Prometheus  
â”‚       type: prometheus  
â”‚       access: proxy  
â”‚       url: http://prometheus:9090  
â”‚       isDefault: true  
â”‚       editable: false  
â”‚   YAML  
â”‚  
â”œâ”€â”€ \# Grafana dashboard provisioning  
â”‚   mkdir \-p $CONFIG\_DIR/grafana/provisioning/dashboards  
â”‚   mkdir \-p $CONFIG\_DIR/grafana/dashboards  
â”‚   cat \> $CONFIG\_DIR/grafana/provisioning/dashboards/default.yml \<\< YAML  
â”‚   apiVersion: 1  
â”‚   providers:  
â”‚     \- name: 'default'  
â”‚       orgId: 1  
â”‚       folder: 'AI Platform'  
â”‚       type: file  
â”‚       disableDeletion: false  
â”‚       editable: true  
â”‚       options:  
â”‚         path: /var/lib/grafana/dashboards  
â”‚         foldersFromFilesStructure: false  
â”‚   YAML  
â”‚  
â””â”€â”€ \# Generate AI Platform dashboard JSON (LiteLLM metrics, GPU, system)  
    \# (Large JSON dashboard definition â€” stored as file)  
    \# Includes panels for:  
    \#   \- LLM requests/sec, tokens/sec, latency P50/P95/P99  
    \#   \- Cost per model (from LiteLLM Prometheus metrics)  
    \#   \- GPU utilization, GPU memory, GPU temperature  
    \#   \- System CPU, RAM, disk I/O  
    \#   \- Container resource usage (from cAdvisor)  
    \#   \- Ollama model loading status  
    cp $BASE\_DIR/assets/grafana-dashboard-ai-platform.json \\  
       $CONFIG\_DIR/grafana/dashboards/ 2\>/dev/null || \\  
    generate\_dashboard\_json \> $CONFIG\_DIR/grafana/dashboards/ai-platform.json

### **PHASE 8: Pull Docker Images**

pull\_docker\_images()  
â”‚  
â”œâ”€â”€ echo "Pulling Docker images (this may take 5â€“15 minutes)..."  
â”‚  
â”œâ”€â”€ \# Parse compose file for image list  
â”‚   IMAGES=$(grep 'image:' $COMPOSE\_DIR/docker-compose.yml | awk '{print  $ 2}' | sort \-u)  
â”‚  
â”œâ”€â”€ TOTAL= $ (echo "$IMAGES" | wc \-l)  
â”‚   COUNT=0  
â”‚   FAILED=()  
â”‚  
â”œâ”€â”€ for img in  $ IMAGES; do  
â”‚     COUNT= $ ((COUNT \+ 1))  
â”‚     echo "  \[ $ COUNT/ $ TOTAL\] Pulling  $ img..."  
â”‚     if \! docker pull " $ img" 2\>\>$LOG\_FILE; then  
â”‚       echo "    âš  Failed to pull  $ img â€” will retry"  
â”‚       FAILED+=(" $ img")  
â”‚     fi  
â”‚   done  
â”‚  
â”œâ”€â”€ \# Retry failed pulls once  
â”‚   for img in "${FAILED\[@\]}"; do  
â”‚     echo "  Retrying  $ img..."  
â”‚     docker pull " $ img" || echo "  âœ— FAILED: $img â€” check network/registry"  
â”‚   done  
â”‚  
â””â”€â”€ echo "Docker images ready."

### **PHASES 9â€“11: Start Services in Order**

deploy\_services()  
â”‚  
â”œâ”€â”€ cd $COMPOSE\_DIR  
â”‚  
â”œâ”€â”€ \# PHASE 9: Core infrastructure  
â”‚   echo "Starting core infrastructure..."  
â”‚   docker compose up \-d postgres redis  
â”‚  
â”œâ”€â”€ \# PHASE 10: Wait for DB  
â”‚   echo "Waiting for PostgreSQL..."  
â”‚   RETRIES=0  
â”‚   MAX\_RETRIES=30  
â”‚   until docker exec postgres pg\_isready \-U ${POSTGRES\_USER} 2\>/dev/null; do  
â”‚     RETRIES=$((RETRIES \+ 1))  
â”‚     if \[\[ $RETRIES \-ge $MAX\_RETRIES \]\]; then  
â”‚       echo "ERROR: PostgreSQL failed to start after ${MAX\_RETRIES} attempts"  
â”‚       docker logs postgres \--tail 50  
â”‚       exit 1  
â”‚     fi  
â”‚     echo "  Waiting for PostgreSQL... ( $ RETRIES/ $ MAX\_RETRIES)"  
â”‚     sleep 2  
â”‚   done  
â”‚   echo "  âœ“ PostgreSQL ready"  
â”‚  
â”‚   echo "Waiting for Redis..."  
â”‚   until docker exec redis redis-cli \-a ${REDIS\_PASSWORD} ping 2\>/dev/null | grep \-q PONG; do  
â”‚     sleep 2  
â”‚   done  
â”‚   echo "  âœ“ Redis ready"  
â”‚  
â”œâ”€â”€ \# PHASE 11: Start remaining services in waves  
â”‚   echo ""  
â”‚   echo "Starting services..."  
â”‚     
â”‚   \# Wave 1: Vector DB \+ LiteLLM (foundation layer)  
â”‚   echo "  Wave 1: Vector DB \+ LiteLLM..."  
â”‚   WAVE1="litellm"  
â”‚   case  $ VECTOR\_DB in  
â”‚     qdrant)  WAVE1=" $ WAVE1 qdrant" ;;  
â”‚     weaviate) WAVE1=" $ WAVE1 weaviate" ;;  
â”‚     milvus)  WAVE1=" $ WAVE1 etcd minio-milvus milvus" ;;  
â”‚   esac  
â”‚   docker compose up \-d  $ WAVE1  
â”‚     
â”‚   \# Wait for LiteLLM health  
â”‚   echo "  Waiting for LiteLLM..."  
â”‚   RETRIES=0  
â”‚   until curl \-sf http://localhost:4000/health \>/dev/null 2\>&1; do  
â”‚     RETRIES= $ ((RETRIES \+ 1))  
â”‚     \[\[  $ RETRIES \-ge 60 \]\] && { echo "ERROR: LiteLLM failed"; docker logs litellm \--tail 30; exit 1; }  
â”‚     sleep 3  
â”‚   done  
â”‚   echo "  âœ“ LiteLLM ready"  
â”‚  
â”‚   \# Wave 2: Application services  
â”‚   echo "  Wave 2: Application services..."  
â”‚   WAVE2=""  
â”‚   service\_enabled "dify" && WAVE2=" $ WAVE2 dify-api dify-worker dify-web dify-nginx"  
â”‚   service\_enabled "n8n" && WAVE2=" $ WAVE2 n8n"  
â”‚   service\_enabled "open-webui" && WAVE2=" $ WAVE2 open-webui"  
â”‚   service\_enabled "flowise" && WAVE2=" $ WAVE2 flowise"  
â”‚   service\_enabled "anythingllm" && WAVE2=" $ WAVE2 anythingllm"  
â”‚   service\_enabled "supertokens" && WAVE2=" $ WAVE2 supertokens"  
â”‚   \[\[ \-n " $ WAVE2" \]\] && docker compose up \-d  $ WAVE2  
â”‚     
â”‚   \# Wave 3: Infrastructure services  
â”‚   echo "  Wave 3: Infrastructure (Caddy, Portainer, Monitoring)..."  
â”‚   WAVE3="caddy portainer"  
â”‚   service\_enabled "monitoring" && WAVE3=" $ WAVE3 prometheus grafana node-exporter cadvisor"  
â”‚   \[\[ " $ HAS\_GPU" \== "true" \]\] && service\_enabled "monitoring" && WAVE3=" $ WAVE3 nvidia-smi-exporter"  
â”‚   docker compose up \-d $WAVE3  
â”‚     
â”‚   echo ""  
â”‚   echo "All services started. Waiting 30s for stabilization..."  
â”‚   sleep 30

### **PHASE 12: Pull Ollama Models**

pull\_ollama\_models()  
â”‚  
â”œâ”€â”€ echo "Pulling Ollama models (tier:  $ MODEL\_TIER)..."  
â”‚  
â”œâ”€â”€ declare \-A TIER\_MODELS  
â”‚   TIER\_MODELS\[minimal\]="tinyllama nomic-embed-text"  
â”‚   TIER\_MODELS\[standard\]=" $ {TIER\_MODELS\[minimal\]} mistral llama3.1:8b codellama:7b"  
â”‚   TIER\_MODELS\[full\]="${TIER\_MODELS\[standard\]} llama3.1:70b mixtral:8x7b deepseek-coder-v2"  
â”‚  
â”œâ”€â”€ MODELS=${TIER\_MODELS\[ $ MODEL\_TIER\]:- $ {TIER\_MODELS\[minimal\]}}  
â”‚  
â”œâ”€â”€ for model in  $ MODELS; do  
â”‚     if ollama list 2\>/dev/null | grep \-q "^ $ model"; then  
â”‚       echo "  âœ“ $model (already present)"  
â”‚     else  
â”‚       echo "  â†“ Pulling  $ model..."  
â”‚       DISK\_FREE= $ (df \-BG /mnt/data | awk 'NR==2{gsub("G",""); print $4}')  
â”‚       if \[\[  $ DISK\_FREE \-lt 10 \]\]; then  
â”‚         echo "    âš  Low disk space ( $ {DISK\_FREE}GB free) â€” skipping remaining models"  
â”‚         break  
â”‚       fi  
â”‚       if ollama pull " $ model" 2\>\> $ LOG\_FILE; then  
â”‚         echo "    âœ“ $model pulled successfully"  
â”‚       else  
â”‚         echo "    âœ— Failed to pull $model"  
â”‚       fi  
â”‚     fi  
â”‚   done  
â”‚  
â””â”€â”€ echo "Ollama models: $(ollama list 2\>/dev/null | tail \-n \+2 | wc \-l) installed"

### **PHASE 13: Configure Google Drive**

configure\_gdrive()  
â”‚  
â”œâ”€â”€ if \[\[ "${GDRIVE\_ENABLED}" \!= "true" \]\]; then  
â”‚     echo "Google Drive sync: disabled"  
â”‚     return  
â”‚   fi  
â”‚  
â”œâ”€â”€ echo "Configuring Google Drive sync..."  
â”‚  
â”œâ”€â”€ \# Configure rclone  
â”‚   mkdir \-p /root/.config/rclone  
â”‚   cat \> /root/.config/rclone/rclone.conf \<\< CONF  
â”‚   \[gdrive\]  
â”‚   type \= drive  
â”‚   client\_id \= ${GDRIVE\_CLIENT\_ID}  
â”‚   client\_secret \= ${GDRIVE\_CLIENT\_SECRET}  
â”‚   scope \= drive.readonly  
â”‚   root\_folder\_id \= ${GDRIVE\_FOLDER}  
â”‚   CONF  
â”‚  
â”œâ”€â”€ \# OAuth token â€” needs interactive auth  
â”‚   echo ""  
â”‚   echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚   echo "  Google Drive OAuth Authorization Required"  
â”‚   echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚   echo ""  
â”‚   echo "Option 1: If you have a browser on this machine:"  
â”‚   echo "  rclone config reconnect gdrive:"  
â”‚   echo ""  
â”‚   echo "Option 2: Remote (headless) authorization:"  
â”‚   echo "  On a machine with a browser, run:"  
â”‚   echo "    rclone authorize drive client\_id=${GDRIVE\_CLIENT\_ID} client\_secret=${GDRIVE\_CLIENT\_SECRET}"  
â”‚   echo "  Then paste the token here."  
â”‚   echo ""  
â”‚   read \-p "Paste OAuth token (or 'skip' to configure later): " GDRIVE  
â”‚   read \-p "Paste OAuth token (or 'skip' to configure later): " GDRIVE\_TOKEN  
â”‚  
â”œâ”€â”€ if \[\[ "$GDRIVE\_TOKEN" \!= "skip" \]\] && \[\[ \-n "$GDRIVE\_TOKEN" \]\]; then  
â”‚     \# Append token to rclone config  
â”‚     echo "token \= ${GDRIVE\_TOKEN}" \>\> /root/.config/rclone/rclone.conf  
â”‚       
â”‚     \# Verify connection  
â”‚     if rclone lsd gdrive: \--max-depth 0 \>/dev/null 2\>&1; then  
â”‚       echo "  âœ“ Google Drive connection verified"  
â”‚     else  
â”‚       echo "  âš  Could not verify connection â€” check token and try:"  
â”‚       echo "    rclone config reconnect gdrive:"  
â”‚     fi  
â”‚   else  
â”‚     echo "  âš  Skipped â€” run later: rclone config reconnect gdrive:"  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# Create sync directory  
â”‚   mkdir \-p ${GDRIVE\_SYNC\_DIR}  
â”‚  
â”œâ”€â”€ \# Create sync script  
â”‚   cat \> $BASE\_DIR/scripts/gdrive-sync.sh \<\< 'SCRIPT'  
â”‚   \#\!/bin/bash  
â”‚   set \-euo pipefail  
â”‚   LOG=/var/log/ai-platform/gdrive-sync.log  
â”‚   LOCK=/tmp/gdrive-sync.lock  
â”‚     
â”‚   \# Prevent concurrent runs  
â”‚   exec 200\>"$LOCK"  
â”‚   flock \-n 200 || { echo "$(date) â€” sync already running" \>\> $LOG; exit 0; }  
â”‚     
â”‚   echo "$(date) â€” Starting Google Drive sync" \>\> $LOG  
â”‚     
â”‚   SYNC\_DIR=$(grep GDRIVE\_SYNC\_DIR /opt/ai-platform/env/master.env | cut \-d= \-f2)  
â”‚     
â”‚   rclone sync gdrive: "$SYNC\_DIR" \\  
â”‚     \--transfers 4 \\  
â”‚     \--checkers 8 \\  
â”‚     \--contimeout 60s \\  
â”‚     \--timeout 300s \\  
â”‚     \--retries 3 \\  
â”‚     \--low-level-retries 10 \\  
â”‚     \--stats-one-line \\  
â”‚     \--log-file "$LOG" \\  
â”‚     \--log-level INFO \\  
â”‚     \--exclude ".trash/\*\*" \\  
â”‚     \--exclude ".\~\*" \\  
â”‚     \--exclude "\~$\*"  
â”‚     
â”‚   SYNC\_STATUS=$?  
â”‚     
â”‚   if \[\[ $SYNC\_STATUS \-eq 0 \]\]; then  
â”‚     echo "$(date) â€” Sync completed successfully" \>\> $LOG  
â”‚     FILE\_COUNT=$(find "$SYNC\_DIR" \-type f | wc \-l)  
â”‚     TOTAL\_SIZE=$(du \-sh "$SYNC\_DIR" | awk '{print $1}')  
â”‚     echo "$(date) â€” Files: $FILE\_COUNT, Size: $TOTAL\_SIZE" \>\> $LOG  
â”‚   else  
â”‚     echo "$(date) â€” Sync failed with code $SYNC\_STATUS" \>\> $LOG  
â”‚   fi  
â”‚   SCRIPT  
â”‚   chmod \+x $BASE\_DIR/scripts/gdrive-sync.sh  
â”‚  
â”œâ”€â”€ \# Create systemd timer for periodic sync  
â”‚   cat \> /etc/systemd/system/gdrive-sync.service \<\< SERVICE  
â”‚   \[Unit\]  
â”‚   Description=Google Drive Sync for AI Platform  
â”‚   After=network-online.target  
â”‚   Wants=network-online.target  
â”‚     
â”‚   \[Service\]  
â”‚   Type=oneshot  
â”‚   ExecStart=$BASE\_DIR/scripts/gdrive-sync.sh  
â”‚   User=root  
â”‚   StandardOutput=journal  
â”‚   StandardError=journal  
â”‚   SERVICE  
â”‚  
â”‚   cat \> /etc/systemd/system/gdrive-sync.timer \<\< TIMER  
â”‚   \[Unit\]  
â”‚   Description=Run Google Drive Sync every ${GDRIVE\_INTERVAL} minutes  
â”‚     
â”‚   \[Timer\]  
â”‚   OnBootSec=5min  
â”‚   OnUnitActiveSec=${GDRIVE\_INTERVAL}min  
â”‚   Persistent=true  
â”‚   RandomizedDelaySec=60  
â”‚     
â”‚   \[Install\]  
â”‚   WantedBy=timers.target  
â”‚   TIMER  
â”‚  
â”‚   systemctl daemon-reload  
â”‚   systemctl enable \--now gdrive-sync.timer  
â”‚   echo "  âœ“ Google Drive sync scheduled every ${GDRIVE\_INTERVAL} minutes"  
â”‚  
â”œâ”€â”€ \# Run initial sync  
â”‚   echo "  Running initial sync..."  
â”‚   bash $BASE\_DIR/scripts/gdrive-sync.sh &  
â”‚   echo "  âœ“ Initial sync started in background"  
â”‚  
â””â”€â”€ echo "Google Drive configuration complete"

### **PHASE 14: Configure Backups**

configure\_backups()  
â”‚  
â”œâ”€â”€ if \[\[ "${BACKUP\_ENABLED}" \!= "true" \]\]; then  
â”‚     echo "Automated backups: disabled"  
â”‚     return  
â”‚   fi  
â”‚  
â”œâ”€â”€ echo "Configuring automated backups..."  
â”‚  
â”œâ”€â”€ \# Create backup script  
â”‚   cat \> $BASE\_DIR/scripts/backup.sh \<\< 'SCRIPT'  
â”‚   \#\!/bin/bash  
â”‚   set \-euo pipefail  
â”‚     
â”‚   \# â”€â”€ Config â”€â”€  
â”‚   source /opt/ai-platform/env/master.env  
â”‚   BACKUP\_DIR=/tmp/ai-platform-backup  
â”‚   TIMESTAMP=$(date \+%Y%m%d-%H%M%S)  
â”‚   BACKUP\_NAME="ai-platform-${TIMESTAMP}"  
â”‚   LOG=/var/log/ai-platform/backup.log  
â”‚   LOCK=/tmp/ai-platform-backup.lock  
â”‚     
â”‚   exec 200\>"$LOCK"  
â”‚   flock \-n 200 || { echo "$(date) â€” backup already running" \>\> $LOG; exit 0; }  
â”‚     
â”‚   log() { echo "$(date '+%Y-%m-%d %H:%M:%S') â€” $1" | tee \-a $LOG; }  
â”‚     
â”‚   log "Starting backup: ${BACKUP\_NAME}"  
â”‚     
â”‚   \# Clean previous temp  
â”‚   rm \-rf $BACKUP\_DIR  
â”‚   mkdir \-p $BACKUP\_DIR/${BACKUP\_NAME}  
â”‚     
â”‚   \# â”€â”€ 1\. PostgreSQL dump â”€â”€  
â”‚   log "Dumping PostgreSQL databases..."  
â”‚   DATABASES=$(docker exec postgres psql \-U ${POSTGRES\_USER} \-t \-c \\  
â”‚     "SELECT datname FROM pg\_database WHERE datistemplate \= false AND datname \!= 'postgres';" | tr \-d ' ')  
â”‚     
â”‚   mkdir \-p $BACKUP\_DIR/${BACKUP\_NAME}/postgres  
â”‚   for db in $DATABASES; do  
â”‚     \[\[ \-z "$db" \]\] && continue  
â”‚     log "  Dumping database: $db"  
â”‚     docker exec postgres pg\_dump \-U ${POSTGRES\_USER} \-Fc "$db" \\  
â”‚       \> $BACKUP\_DIR/${BACKUP\_NAME}/postgres/${db}.dump  
â”‚   done  
â”‚     
â”‚   \# â”€â”€ 2\. Redis snapshot â”€â”€  
â”‚   log "Creating Redis snapshot..."  
â”‚   docker exec redis redis-cli \-a ${REDIS\_PASSWORD} BGSAVE \>/dev/null 2\>&1  
â”‚   sleep 5  
â”‚   mkdir \-p $BACKUP\_DIR/${BACKUP\_NAME}/redis  
â”‚   docker cp redis:/data/appendonly.aof $BACKUP\_DIR/${BACKUP\_NAME}/redis/ 2\>/dev/null || true  
â”‚   docker cp redis:/data/dump.rdb $BACKUP\_DIR/${BACKUP\_NAME}/redis/ 2\>/dev/null || true  
â”‚     
â”‚   \# â”€â”€ 3\. Environment & config (no secrets in backup â€” can be regenerated) â”€â”€  
â”‚   log "Backing up configuration..."  
â”‚   mkdir \-p $BACKUP\_DIR/${BACKUP\_NAME}/config  
â”‚   cp \-r /opt/ai-platform/config/ $BACKUP\_DIR/${BACKUP\_NAME}/config/  
â”‚   cp \-r /opt/ai-platform/docker/ $BACKUP\_DIR/${BACKUP\_NAME}/config/  
â”‚   \# Exclude master.env (contains secrets) â€” store encrypted separately  
â”‚   openssl enc \-aes-256-cbc \-salt \-pbkdf2 \\  
â”‚     \-in /opt/ai-platform/env/master.env \\  
â”‚     \-out $BACKUP\_DIR/${BACKUP\_NAME}/config/master.env.enc \\  
â”‚     \-pass pass:${LITELLM\_SALT\_KEY} 2\>/dev/null || \\  
â”‚     cp /opt/ai-platform/env/master.env $BACKUP\_DIR/${BACKUP\_NAME}/config/master.env  
â”‚     
â”‚   \# â”€â”€ 4\. Vector DB data â”€â”€  
â”‚   log "Backing up vector database..."  
â”‚   mkdir \-p $BACKUP\_DIR/${BACKUP\_NAME}/vectordb  
â”‚   case ${VECTOR\_DB} in  
â”‚     qdrant)  
â”‚       \# Qdrant snapshot via API  
â”‚       curl \-sf \-X POST http://localhost:6333/snapshots \-o \\  
â”‚         $BACKUP\_DIR/${BACKUP\_NAME}/vectordb/qdrant-snapshot.tar 2\>/dev/null || \\  
â”‚         log "  âš  Qdrant snapshot failed â€” backing up data dir"  
â”‚       ;;  
â”‚     weaviate)  
â”‚       curl \-sf \-X POST http://localhost:8080/v1/backups/filesystem \\  
â”‚         \-H 'Content-Type: application/json' \\  
â”‚         \-d "{\\"id\\": \\"${TIMESTAMP}\\"}" 2\>/dev/null || \\  
â”‚         log "  âš  Weaviate backup failed"  
â”‚       ;;  
â”‚     milvus)  
â”‚       log "  Milvus backup requires milvus-backup tool â€” skipping data dir copy"  
â”‚       ;;  
â”‚   esac  
â”‚     
â”‚   \# â”€â”€ 5\. Service-specific data â”€â”€  
â”‚   log "Backing up service data..."  
â”‚     
â”‚   \# n8n workflows export  
â”‚   if docker ps \--format '{{.Names}}' | grep \-q '^n8n$'; then  
â”‚     mkdir \-p $BACKUP\_DIR/${BACKUP\_NAME}/n8n  
â”‚     curl \-sf http://localhost:5678/api/v1/workflows \\  
â”‚       \-H "Accept: application/json" \\  
â”‚       \> $BACKUP\_DIR/${BACKUP\_NAME}/n8n/workflows.json 2\>/dev/null || true  
â”‚     curl \-sf http://localhost:5678/api/v1/credentials \\  
â”‚       \-H "Accept: application/json" \\  
â”‚       \> $BACKUP\_DIR/${BACKUP\_NAME}/n8n/credentials.json 2\>/dev/null || true  
â”‚   fi  
â”‚     
â”‚   \# Dify storage (uploaded files)  
â”‚   if \[\[ \-d "${DATA\_DIR}/dify/storage" \]\]; then  
â”‚     log "  Backing up Dify storage..."  
â”‚     mkdir \-p $BACKUP\_DIR/${BACKUP\_NAME}/dify  
â”‚     tar \-czf $BACKUP\_DIR/${BACKUP\_NAME}/dify/storage.tar.gz \\  
â”‚       \-C ${DATA\_DIR}/dify storage 2\>/dev/null || true  
â”‚   fi  
â”‚     
â”‚   \# Flowise flows  
â”‚   if \[\[ \-d "${DATA\_DIR}/flowise" \]\]; then  
â”‚     mkdir \-p $BACKUP\_DIR/${BACKUP\_NAME}/flowise  
â”‚     cp \-r ${DATA\_DIR}/flowise/ $BACKUP\_DIR/${BACKUP\_NAME}/flowise/ 2\>/dev/null || true  
â”‚   fi  
â”‚     
â”‚   \# â”€â”€ 6\. Compress â”€â”€  
â”‚   log "Compressing backup..."  
â”‚   cd $BACKUP\_DIR  
â”‚   tar \-czf ${BACKUP\_NAME}.tar.gz ${BACKUP\_NAME}/  
â”‚   BACKUP\_SIZE=$(du \-sh ${BACKUP\_NAME}.tar.gz | awk '{print $1}')  
â”‚   log "Backup size: ${BACKUP\_SIZE}"  
â”‚     
â”‚   \# â”€â”€ 7\. Upload to S3 â”€â”€  
â”‚   log "Uploading to S3..."  
â”‚   aws s3 cp ${BACKUP\_NAME}.tar.gz \\  
â”‚     s3://${S3\_BUCKET}/backups/${BACKUP\_NAME}.tar.gz \\  
â”‚     \--storage-class STANDARD\_IA \\  
â”‚     \--only-show-errors  
â”‚     
â”‚   if \[\[ $? \-eq 0 \]\]; then  
â”‚     log "  âœ“ Uploaded to s3://${S3\_BUCKET}/backups/${BACKUP\_NAME}.tar.gz"  
â”‚   else  
â”‚     log "  âœ— S3 upload failed"  
â”‚     \# Keep local copy if S3 fails  
â”‚     mkdir \-p ${DATA\_DIR}/backups  
â”‚     mv ${BACKUP\_NAME}.tar.gz ${DATA\_DIR}/backups/  
â”‚     log "  â†’ Saved locally to ${DATA\_DIR}/backups/${BACKUP\_NAME}.tar.gz"  
â”‚   fi  
â”‚     
â”‚   \# â”€â”€ 8\. Cleanup old backups â”€â”€  
â”‚   log "Cleaning up old backups..."  
â”‚   \# Remote: lifecycle policy handles S3 â€” but also prune manually  
â”‚   RETENTION\_DAYS=${BACKUP\_RETENTION\_DAYS:-30}  
â”‚   aws s3 ls s3://${S3\_BUCKET}/backups/ 2\>/dev/null | while read \-r line; do  
â”‚     BACKUP\_DATE=$(echo "$line" | awk '{print $1}')  
â”‚     BACKUP\_FILE=$(echo "$line" | awk '{print $4}')  
â”‚     if \[\[ \-n "$BACKUP\_DATE" \]\] && \[\[ \-n "$BACKUP\_FILE" \]\]; then  
â”‚       AGE\_DAYS=$(( ($(date \+%s) \- $(date \-d "$BACKUP\_DATE" \+%s)) / 86400 ))  
â”‚       if \[\[ $AGE\_DAYS \-gt $RETENTION\_DAYS \]\]; then  
â”‚         log "  Removing old backup: $BACKUP\_FILE (${AGE\_DAYS} days old)"  
â”‚         aws s3 rm "s3://${S3\_BUCKET}/backups/${BACKUP\_FILE}" \--only-show-errors  
â”‚       fi  
â”‚     fi  
â”‚   done  
â”‚     
â”‚   \# Local cleanup  
â”‚   find ${DATA\_DIR}/backups/ \-name "ai-platform-\*.tar.gz" \-mtime \+7 \-delete 2\>/dev/null || true  
â”‚     
â”‚   \# Temp cleanup  
â”‚   rm \-rf $BACKUP\_DIR  
â”‚     
â”‚   log "Backup complete: ${BACKUP\_NAME}"  
â”‚   SCRIPT  
â”‚   chmod \+x $BASE\_DIR/scripts/backup.sh  
â”‚  
â”œâ”€â”€ \# Create restore script  
â”‚   cat \> $BASE\_DIR/scripts/restore.sh \<\< 'SCRIPT'  
â”‚   \#\!/bin/bash  
â”‚   set \-euo pipefail  
â”‚     
â”‚   BACKUP\_FILE=${1:-}  
â”‚   if \[\[ \-z "$BACKUP\_FILE" \]\]; then  
â”‚     echo "Usage: $0 \<backup-file-or-s3-key\>"  
â”‚     echo ""  
â”‚     echo "Examples:"  
â”‚     echo "  $0 /path/to/ai-platform-20250115-104523.tar.gz"  
â”‚     echo "  $0 s3://bucket/backups/ai-platform-20250115-104523.tar.gz"  
â”‚     echo ""  
â”‚     echo "Available S3 backups:"  
â”‚     source /opt/ai-platform/env/master.env  
â”‚     aws s3 ls s3://${S3\_BUCKET}/backups/ 2\>/dev/null | tail \-10  
â”‚     exit 1  
â”‚   fi  
â”‚     
â”‚   source /opt/ai-platform/env/master.env  
â”‚   RESTORE\_DIR=/tmp/ai-platform-restore  
â”‚     
â”‚   echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚   echo "  AI Platform Restore"  
â”‚   echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚   echo ""  
â”‚   echo "WARNING: This will overwrite current databases and configs."  
â”‚   read \-p "Continue? (yes/no): " CONFIRM  
â”‚   \[\[ "$CONFIRM" \!= "yes" \]\] && exit 0  
â”‚     
â”‚   rm \-rf $RESTORE\_DIR  
â”‚   mkdir \-p $RESTORE\_DIR  
â”‚     
â”‚   \# Download if S3  
â”‚   if \[\[ "$BACKUP\_FILE" \== s3://\* \]\]; then  
â”‚     echo "Downloading from S3..."  
â”‚     aws s3 cp "$BACKUP\_FILE" $RESTORE\_DIR/backup.tar.gz  
â”‚     BACKUP\_FILE="$RESTORE\_DIR/backup.tar.gz"  
â”‚   fi  
â”‚     
â”‚   echo "Extracting..."  
â”‚   tar \-xzf "$BACKUP\_FILE" \-C $RESTORE\_DIR  
â”‚   BACKUP\_DIR=$(ls \-d $RESTORE\_DIR/ai-platform-\* | head \-1)  
â”‚     
â”‚   echo "Stopping application services..."  
â”‚   cd /opt/ai-platform/docker  
â”‚   docker compose stop dify-api dify-worker dify-web n8n open-webui flowise anythingllm litellm 2\>/dev/null || true  
â”‚     
â”‚   \# Restore PostgreSQL  
â”‚   if \[\[ \-d "$BACKUP\_DIR/postgres" \]\]; then  
â”‚     echo "Restoring PostgreSQL databases..."  
â”‚     for dump in $BACKUP\_DIR/postgres/\*.dump; do  
â”‚       DB\_NAME=$(basename "$dump" .dump)  
â”‚       echo "  Restoring database: $DB\_NAME"  
â”‚       docker exec postgres dropdb \-U ${POSTGRES\_USER} \--if-exists "$DB\_NAME" 2\>/dev/null || true  
â”‚       docker exec postgres createdb \-U ${POSTGRES\_USER} "$DB\_NAME" 2\>/dev/null || true  
â”‚       cat "$dump" | docker exec \-i postgres pg\_restore \-U ${POSTGRES\_USER} \-d "$DB\_NAME" \--clean \--if-exists 2\>/dev/null || true  
â”‚     done  
â”‚   fi  
â”‚     
â”‚   \# Restore Redis  
â”‚   if \[\[ \-d "$BACKUP\_DIR/redis" \]\]; then  
â”‚     echo "Restoring Redis data..."  
â”‚     docker compose stop redis  
â”‚     cp $BACKUP\_DIR/redis/\* ${DATA\_DIR}/redis/ 2\>/dev/null || true  
â”‚     docker compose start redis  
â”‚     sleep 5  
â”‚   fi  
â”‚     
â”‚   \# Restore service data  
â”‚   if \[\[ \-f "$BACKUP\_DIR/dify/storage.tar.gz" \]\]; then  
â”‚     echo "Restoring Dify storage..."  
â”‚     tar \-xzf "$BACKUP\_DIR/dify/storage.tar.gz" \-C ${DATA\_DIR}/dify/  
â”‚   fi  
â”‚     
â”‚   echo "Restarting all services..."  
â”‚   docker compose up \-d  
â”‚     
â”‚   echo ""  
â”‚   echo "âœ“ Restore complete. Services restarting..."  
â”‚   echo "  Monitor with: docker compose logs \-f"  
â”‚     
â”‚   rm \-rf $RESTORE\_DIR  
â”‚   SCRIPT  
â”‚   chmod \+x $BASE\_DIR/scripts/restore.sh  
â”‚  
â”œâ”€â”€ \# Schedule backup via systemd timer  
â”‚   cat \> /etc/systemd/system/ai-platform-backup.service \<\< SERVICE  
â”‚   \[Unit\]  
â”‚   Description=AI Platform Backup  
â”‚   After=network-online.target docker.service  
â”‚   Wants=network-online.target  
â”‚     
â”‚   \[Service\]  
â”‚   Type=oneshot  
â”‚   ExecStart=/opt/ai-platform/scripts/backup.sh  
â”‚   User=root  
â”‚   StandardOutput=journal  
â”‚   StandardError=journal  
â”‚   TimeoutStartSec=3600  
â”‚   SERVICE  
â”‚  
â”‚   if \[\[ "$BACKUP\_FREQUENCY" \== "daily" \]\]; then  
â”‚     TIMER\_SCHEDULE="OnCalendar=\*-\*-\* 02:00:00"  
â”‚   else  
â”‚     TIMER\_SCHEDULE="OnCalendar=Sun \*-\*-\* 02:00:00"  
â”‚   fi  
â”‚  
â”‚   cat \> /etc/systemd/system/ai-platform-backup.timer \<\< TIMER  
â”‚   \[Unit\]  
â”‚   Description=AI Platform Backup Timer (${BACKUP\_FREQUENCY})  
â”‚     
â”‚   \[Timer\]  
â”‚   ${TIMER\_SCHEDULE}  
â”‚   Persistent=true  
â”‚   RandomizedDelaySec=1800  
â”‚     
â”‚   \[Install\]  
â”‚   WantedBy=timers.target  
â”‚   TIMER  
â”‚  
â”‚   systemctl daemon-reload  
â”‚   systemctl enable \--now ai-platform-backup.timer  
â”‚   echo "  âœ“ Backup scheduled: ${BACKUP\_FREQUENCY} at 2:00 AM"  
â”‚   echo "  âœ“ Retention: ${BACKUP\_RETENTION\_DAYS} days"  
â”‚   echo "  âœ“ Destination: s3://${S3\_BUCKET}/backups/"  
â”‚  
â””â”€â”€ echo "Backup configuration complete"

### **PHASE 15: Health Check All Services**

health\_check\_all()  
â”‚  
â”œâ”€â”€ echo ""  
â”‚   echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚   echo "  Service Health Check"  
â”‚   echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚   echo ""  
â”‚  
â”œâ”€â”€ PASS=0  
â”‚   FAIL=0  
â”‚   WARN=0  
â”‚  
â”œâ”€â”€ check\_service() {  
â”‚     local name=$1  
â”‚     local url=$2  
â”‚     local expected=${3:-200}  
â”‚       
â”‚     STATUS=$(curl \-sf \-o /dev/null \-w "%{http\_code}" "$url" 2\>/dev/null || echo "000")  
â”‚       
â”‚     if \[\[ "$STATUS" \== "$expected" \]\] || \[\[ "$STATUS" \== "200" \]\]; then  
â”‚       echo "  âœ“ ${name} â€” healthy (HTTP ${STATUS})"  
â”‚       PASS=$((PASS \+ 1))  
â”‚     elif \[\[ "$STATUS" \== "000" \]\]; then  
â”‚       echo "  âœ— ${name} â€” unreachable"  
â”‚       FAIL=$((FAIL \+ 1))  
â”‚     else  
â”‚       echo "  âš  ${name} â€” unexpected status (HTTP ${STATUS})"  
â”‚       WARN=$((WARN \+ 1))  
â”‚     fi  
â”‚   }  
â”‚  
â”œâ”€â”€ \# Core  
â”‚   check\_service "PostgreSQL" "skip" "skip"  
â”‚   \# PostgreSQL check via docker  
â”‚   if docker exec postgres pg\_isready \-U ${POSTGRES\_USER} \>/dev/null 2\>&1; then  
â”‚     echo "  âœ“ PostgreSQL â€” healthy"  
â”‚     PASS=$((PASS \+ 1))  
â”‚   else  
â”‚     echo "  âœ— PostgreSQL â€” not ready"  
â”‚     FAIL=$((FAIL \+ 1))  
â”‚   fi  
â”‚     
â”‚   if docker exec redis redis-cli \-a ${REDIS\_PASSWORD} ping 2\>/dev/null | grep \-q PONG; then  
â”‚     echo "  âœ“ Redis â€” healthy"  
â”‚     PASS=$((PASS \+ 1))  
â”‚   else  
â”‚     echo "  âœ— Redis â€” not responding"  
â”‚     FAIL=$((FAIL \+ 1))  
â”‚   fi  
â”‚  
â”œâ”€â”€ \# LiteLLM  
â”‚   check\_service "LiteLLM" "http://localhost:4000/health"  
â”‚  
â”œâ”€â”€ \# Vector DB  
â”‚   case $VECTOR\_DB in  
â”‚     qdrant)   check\_service "Qdrant" "http://localhost:6333/healthz" ;;  
â”‚     weaviate) check\_service "Weaviate" "http://localhost:8080/v1/.well-known/ready" ;;  
â”‚     milvus)   check\_service "Milvus" "http://localhost:9091/healthz" ;;  
â”‚   esac  
â”‚  
â”œâ”€â”€ \# Application services  
â”‚   service\_enabled "dify" && check\_service "Dify" "http://localhost:5001/health" "200"  
â”‚   service\_enabled "n8n" && check\_service "n8n" "http://localhost:5678/healthz"  
â”‚   service\_enabled "open-webui" && check\_service "Open WebUI" "http://localhost:8080/health"  
â”‚   service\_enabled "flowise" && check\_service "Flowise" "http://localhost:3000"  
â”‚   service\_enabled "anythingllm" && check\_service "AnythingLLM" "http://localhost:3001/api/ping"  
â”‚   service\_enabled "supertokens" && check\_service "SuperTokens" "http://localhost:3567/hello"  
â”‚  
â”œâ”€â”€ \# Infrastructure  
â”‚   check\_service "Caddy" "http://localhost:80"  
â”‚   check\_service "Portainer" "http://localhost:9000/api/system/status"  
â”‚   service\_enabled "monitoring" && check\_service "Prometheus" "http://localhost:9090/-/healthy"  
â”‚   service\_enabled "monitoring" && check\_service "Grafana" "http://localhost:3000/api/health"  
â”‚  
â”œâ”€â”€ \# Ollama (host)  
â”‚   if curl \-sf http://localhost:11434/api/tags \>/dev/null 2\>&1; then  
â”‚     MODEL\_COUNT=$(curl \-sf http://localhost:11434/api/tags | python3 \-c "import sys,json; print(len(json.load(sys.stdin).get('models',\[\])))" 2\>/dev/null || echo "?")  
â”‚     echo "  âœ“ Ollama â€” healthy (${MODEL\_COUNT} models loaded)"  
â”‚     PASS=$((PASS \+ 1))  
â”‚   else  
â”‚     echo "  âš  Ollama â€” not responding on port 11434"  
â”‚     WARN=$((WARN \+ 1))  
â”‚   fi  
â”‚  
â”œâ”€â”€ echo ""  
â”‚   echo "  Results: ${PASS} passed, ${WARN} warnings, ${FAIL} failed"  
â”‚   echo ""  
â”‚  
â””â”€â”€ if \[\[ $FAIL \-gt 0 \]\]; then  
â”‚     echo "  âš  Some services failed. Check logs:"  
â”‚     echo "    docker compose \-f $COMPOSE\_DIR/docker-compose.yml logs \<service\>"  
â”‚     echo ""  
â”‚   fi  
â”‚   return $FAIL

### **PHASE 16: Generate Convenience Scripts**

generate\_convenience\_scripts()  
â”‚  
â”œâ”€â”€ \# â”€â”€ ai-status â€” quick dashboard â”€â”€  
â”‚   cat \> /usr/local/bin/ai-status \<\< 'SCRIPT'  
â”‚   \#\!/bin/bash  
â”‚   source /opt/ai-platform/env/master.env 2\>/dev/null  
â”‚   echo ""  
â”‚   echo "â•â•â• AI Platform Status â•â•â•"  
â”‚   echo ""  
â”‚   echo "Services:"  
â”‚   docker ps \--format "table {{.Names}}\\t{{.Status}}\\t{{.Ports}}" | \\  
â”‚     grep \-E "(postgres|redis|litellm|dify|n8n|open-webui|flowise|caddy|grafana|portainer|qdrant|weaviate|milvus|anythingllm|supertokens|prometheus)" | \\  
â”‚     sort  
â”‚   echo ""  
â”‚   echo "Ollama Models:"  
â”‚   ollama list 2\>/dev/null || echo "  Ollama not running"  
â”‚   echo ""  
â”‚   echo "Disk Usage:"  
â”‚   df \-h /mnt/data 2\>/dev/null || df \-h /  
â”‚   echo ""  
â”‚   echo "GPU:"  
â”‚   nvidia-smi \--query-gpu=name,utilization.gpu,memory.used,memory.total,temperature.gpu \\  
â”‚     \--format=csv,noheader 2\>/dev/null || echo "  No GPU detected"  
â”‚   echo ""  
â”‚   if \[\[ \-n "${BASE\_DOMAIN:-}" \]\]; then  
â”‚     echo "URLs:"  
â”‚     echo "  Dify:        https://${DIFY\_DOMAIN:-N/A}"  
â”‚     echo "  n8n:         https://${N8N\_DOMAIN:-N/A}"  
â”‚     echo "  Open WebUI:  https://${WEBUI\_DOMAIN:-N/A}"  
â”‚     echo "  Flowise:     https://${FLOWISE\_DOMAIN:-N/A}"  
â”‚     echo "  LiteLLM:     https://${LITELLM\_DOMAIN:-N/A}"  
â”‚     echo "  Grafana:     https://${GRAFANA\_DOMAIN:-N/A}"  
â”‚     echo "  Portainer:   https://${PORTAINER\_DOMAIN:-N/A}"  
â”‚   fi  
â”‚   echo ""  
â”‚   SCRIPT  
â”‚   chmod \+x /usr/local/bin/ai-status  
â”‚  
â”œâ”€â”€ \# â”€â”€ ai-logs â€” tail logs â”€â”€  
â”‚   cat \> /usr/local/bin/ai-logs \<\< 'SCRIPT'  
â”‚   \#\!/bin/bash  
â”‚   SERVICE=${1:-}  
â”‚   if \[\[ \-z "$SERVICE" \]\]; then  
â”‚     echo "Usage: ai-logs \<service|all\>"  
â”‚     echo "Services: postgres redis litellm dify n8n open-webui flowise caddy grafana portainer"  
â”‚     exit 1  
â”‚   fi  
â”‚   if \[\[ "$SERVICE" \== "all" \]\]; then  
â”‚     docker compose \-f /opt/ai-platform/docker/docker-compose.yml logs \-f \--tail 50  
â”‚   else  
â”‚     docker compose \-f /opt/ai-platform/docker/docker-compose.yml logs \-f \--tail 100 "$SERVICE"  
â”‚   fi  
â”‚   SCRIPT  
â”‚   chmod \+x /usr/local/bin/ai-logs  
â”‚  
â”œâ”€â”€ \# â”€â”€ ai-restart â€” restart service(s) â”€â”€  
â”‚   cat \> /usr/local/bin/ai-restart \<\< 'SCRIPT'  
â”‚   \#\!/bin/bash  
â”‚   SERVICE=${1:-}  
â”‚   if \[\[ \-z "$SERVICE" \]\]; then  
â”‚     echo "Usage: ai-restart \<service|all\>"  
â”‚     exit 1  
â”‚   fi  
â”‚   if \[\[ "$SERVICE" \== "all" \]\]; then  
â”‚     docker compose \-f /opt/ai-platform/docker/docker-compose.yml restart  
â”‚   else  
â”‚     docker compose \-f /opt/ai-platform/docker/docker-compose.yml restart "$SERVICE"  
â”‚   fi  
â”‚   SCRIPT  
â”‚   chmod \+x /usr/local/bin/ai-restart  
â”‚  
â”œâ”€â”€ \# â”€â”€ ai-backup â€” trigger backup â”€â”€  
â”‚   cat \> /usr/local/bin/ai-backup \<\< 'SCRIPT'  
â”‚   \#\!/bin/bash  
â”‚   echo "Starting backup..."  
â”‚   bash /opt/ai-platform/scripts/backup.sh  
â”‚   SCRIPT  
â”‚   chmod \+x /usr/local/bin/ai-backup  
â”‚  
â”œâ”€â”€ \# â”€â”€ ai-update â€” update all containers â”€â”€  
â”‚   cat \> /usr/local/bin/ai-update \<\< 'SCRIPT'  
â”‚   \#\!/bin/bash  
â”‚   echo "â•â•â• AI Platform Update â•â•â•"  
â”‚   echo ""  
â”‚   echo "1. Creating pre-update backup..."  
â”‚   bash /opt/ai-platform/scripts/backup.sh  
â”‚   echo ""  
â”‚   echo "2. Pulling latest images..."  
â”‚   docker compose \-f /opt/ai-platform/docker/docker-compose.yml pull  
â”‚   echo ""  
â”‚   echo "3. Recreating containers..."  
â”‚   docker compose \-f /opt/ai-platform/docker/docker-compose.yml up \-d \--remove-orphans  
â”‚   echo ""  
â”‚   echo "4. Cleaning old images..."  
â”‚   docker image prune \-f  
â”‚   echo ""  
â”‚   echo "5. Health check..."  
â”‚   sleep 30  
â”‚   ai-status  
â”‚   SCRIPT  
â”‚   chmod \+x /usr/local/bin/ai-update  
â”‚  
â”œâ”€â”€ \# â”€â”€ ai-models â€” manage Ollama models â”€â”€  
â”‚   cat \> /usr/local/bin/ai-models \<\< 'SCRIPT'  
â”‚   \#\!/bin/bash  
â”‚   ACTION=${1:-list}  
â”‚   case $ACTION in  
â”‚     list)   ollama list ;;  
â”‚     pull)   ollama pull "${2:?Specify model name}" ;;  
â”‚     remove) ollama rm "${2:?Specify model name}" ;;  
â”‚     run)    ollama run "${2:?Specify model name}" ;;  
â”‚     \*)      echo "Usage: ai-models \<list|pull|remove|run\> \[model\]" ;;  
â”‚   esac  
â”‚   SCRIPT  
â”‚   chmod \+x /usr/local/bin/ai-models  
â”‚  
â””â”€â”€ echo "Convenience commands installed: ai-status, ai-logs, ai-restart, ai-backup, ai-update, ai-models"

### **PHASE 17: Final Summary**

print\_final\_summary()  
â”‚  
â”œâ”€â”€ ELAPSED=$(( $(date \+%s) \- START\_TIME ))  
â”‚   MINUTES=$(( ELAPSED / 60 ))  
â”‚   SECONDS=$(( ELAPSED % 60 ))  
â”‚  
â”œâ”€â”€ echo ""  
â”‚   echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•‘   âœ“  AI Platform Deployment Complete                       â•‘"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•‘   Deployment time: ${MINUTES}m ${SECONDS}s                 â•‘"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•‘   SERVICE URLS                                             â•‘"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚  
â”‚   if \[\[ "$DOMAIN\_MODE" \== "production" \]\]; then  
â”‚     service\_enabled "dify" && \\  
â”‚       echo "â•‘   Dify:         https://${DIFY\_DOMAIN}                  â•‘"  
â”‚     service\_enabled "n8n" && \\  
â”‚       echo "â•‘   n8n:          https://${N8N\_DOMAIN}                   â•‘"  
â”‚     service\_enabled "open-webui" && \\  
â”‚       echo "â•‘   Open WebUI:   https://${WEBUI\_DOMAIN}                 â•‘"  
â”‚     service\_enabled "flowise" && \\  
â”‚       echo "â•‘   Flowise:      https://${FLOWISE\_DOMAIN}               â•‘"  
â”‚     echo "â•‘   LiteLLM:      https://${LITELLM\_DOMAIN}                â•‘"  
â”‚     service\_enabled "monitoring" && \\  
â”‚       echo "â•‘   Grafana:      https://${GRAFANA\_DOMAIN}               â•‘"  
â”‚     echo "â•‘   Portainer:    https://${PORTAINER\_DOMAIN}              â•‘"  
â”‚   else  
â”‚     echo "â•‘   Access via: http://\<server-ip\>:\<port\>                  â•‘"  
â”‚     echo "â•‘   Ports: Dify(80) n8n(5678) WebUI(8080) Flowise(3000)   â•‘"  
â”‚     echo "â•‘          LiteLLM(4000) Grafana(3001) Portainer(9000)    â•‘"  
â”‚   fi  
â”‚  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•‘   DEFAULT CREDENTIALS                                     â•‘"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•‘   Dify:        admin / ${DIFY\_INIT\_PASSWORD}               â•‘"  
â”‚   echo "â•‘   Flowise:     ${FLOWISE\_USERNAME} / ${FLOWISE\_PASSWORD}   â•‘"  
â”‚   echo "â•‘   Grafana:     ${GRAFANA\_ADMIN\_USER} / ${GRAFANA\_ADMIN\_PASSWORD} â•‘"  
â”‚   echo "â•‘   Portainer:   Set on first login                         â•‘"  
â”‚   echo "â•‘   n8n:         Set on first login                         â•‘"  
â”‚   echo "â•‘   Open WebUI:  Set on first login                         â•‘"  
â”‚   echo "â•‘   LiteLLM key: ${LITELLM\_MASTER\_KEY:0:20}...             â•‘"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•‘   USEFUL COMMANDS                                         â•‘"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•‘   ai-status           â€” Service dashboard                 â•‘"  
â”‚   echo "â•‘   ai-logs \<service\>   â€” View logs                         â•‘"  
â”‚   echo "â•‘   ai-restart \<service\>â€” Restart service                   â•‘"  
â”‚   echo "â•‘   ai-backup           â€” Trigger backup                    â•‘"  
â”‚   echo "â•‘   ai-update           â€” Update all services               â•‘"  
â”‚   echo "â•‘   ai-models list      â€” List Ollama models                â•‘"  
â”‚   echo "â•‘   ai-models pull \<m\>  â€” Pull new model                    â•‘"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•‘   CREDENTIALS FILE                                        â•‘"  
â”‚   echo "â•‘   /opt/ai-platform/env/master.env                         â•‘"  
â”‚   echo "â•‘   (chmod 600 â€” root only)                                 â•‘"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•‘   NEXT STEPS                                              â•‘"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•‘   1\. Access each service URL and complete initial setup    â•‘"  
â”‚   echo "â•‘   2\. Configure Dify to use LiteLLM as model provider      â•‘"  
â”‚   echo "â•‘      (API base: http://litellm:4000/v1)                   â•‘"  
â”‚   echo "â•‘   3\. Import n8n workflow templates from:                   â•‘"  
â”‚   echo "â•‘      /opt/ai-platform/templates/n8n/                      â•‘"  
â”‚   echo "â•‘   4\. Set up Grafana alerts for cost/usage thresholds       â•‘"  
â”‚   echo "â•‘   5\. Upload documents to Google Drive for RAG sync         â•‘"  
â”‚   echo "â•‘   6\. Run 'ai-status' to verify everything is healthy      â•‘"  
â”‚   echo "â•‘                                                            â•‘"  
â”‚   echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"  
â”‚   echo ""  
â”‚  
â”œâ”€â”€ \# Mark completion  
â”‚   echo "SCRIPT\_2\_COMPLETED=$(date \-u \+%Y-%m-%dT%H:%M:%SZ)" \>\> $ENV\_DIR/master.env  
â”‚   echo "SCRIPT\_2\_VERSION=1.0.0" \>\> $ENV\_DIR/master.env  
â”‚  
â””â”€â”€ echo "Deployment log: $LOG\_FILE"

### **Complete Script 2 Flow (main)**

main() {  
  \# Phases 1-2: Validate & source  
  validate\_prerequisites  
  source\_config  
    
  \# Phase 3: Environment files  
  generate\_env\_files  
    
  \# Phase 4: LiteLLM config  
  generate\_litellm\_config  
    
  \# Phase 5: Caddyfile  
  generate\_caddyfile  
    
  \# Phase 6: Docker compose  
  generate\_docker\_compose  
    
  \# Phase 7: Monitoring configs  
  if service\_enabled "monitoring"; then  
    generate\_monitoring\_configs  
  fi  
    
  \# Phase 8: Pull images  
  pull\_docker\_images  
    
  \# Phases 9-11: Deploy  
  deploy\_services  
    
  \# Phase 12: Ollama models  
  pull\_ollama\_models  
    
  \# Phase 13: Google Drive  
  configure\_gdrive  
    
  \# Phase 14: Backups  
  configure\_backups  
    
  \# Phase 15: Health check  
  health\_check\_all || true  
    
  \# Phase 16: Convenience scripts  
  generate\_convenience\_scripts  
    
  \# Phase 17: Summary  
  print\_final\_summary  
}

\# Support \--regenerate flag for config updates  
if \[\[ "${1:-}" \== "--regenerate" \]\]; then  
  echo "Regenerating configs without redeploying..."  
  source\_config  
  generate\_env\_files  
  generate\_litellm\_config  
  generate\_caddyfile  
  generate\_docker\_compose  
  generate\_monitoring\_configs  
  echo "Done. Restart with: docker compose \-f $COMPOSE\_DIR/docker-compose.yml up \-d"  
  exit 0  
fi

main "$@"  
\---

\#\# 17\. PostgreSQL Initialization SQL

\#\#\# Generated by Script 2, Phase 10

generate\_postgres\_init() â”‚ â”œâ”€â”€ File: $CONFIG\_DIR/postgres/init.sql â”‚ â”œâ”€â”€ \-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚ \-- AI Platform â€” PostgreSQL Initialization â”‚ \-- Generated by Script 2 â”‚ \-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚ â”œâ”€â”€ \-- â”€â”€ Extensions â”€â”€ â”‚ CREATE EXTENSION IF NOT EXISTS "uuid-ossp"; â”‚ CREATE EXTENSION IF NOT EXISTS "pgcrypto"; â”‚ CREATE EXTENSION IF NOT EXISTS "pg\_trgm"; â”‚ CREATE EXTENSION IF NOT EXISTS "vector"; \-- pgvector for embeddings â”‚ CREATE EXTENSION IF NOT EXISTS "hstore"; â”‚ â”œâ”€â”€ \-- â”€â”€ Database: LiteLLM â”€â”€ â”‚ CREATE DATABASE litellm OWNER ${POSTGRES\_USER}; â”‚ \\c litellm â”‚ CREATE EXTENSION IF NOT EXISTS "uuid-ossp"; â”‚ CREATE EXTENSION IF NOT EXISTS "pgcrypto"; â”‚  
â”‚ \-- LiteLLM manages its own schema via migrations â”‚ \-- but we pre-create the spend tracking view â”‚ \-- (LiteLLM will create tables on first start) â”‚ â”œâ”€â”€ \-- â”€â”€ Database: Dify â”€â”€ â”‚ CREATE DATABASE dify OWNER ${POSTGRES\_USER}; â”‚ \\c dify â”‚ CREATE EXTENSION IF NOT EXISTS "uuid-ossp"; â”‚ CREATE EXTENSION IF NOT EXISTS "pgcrypto"; â”‚ CREATE EXTENSION IF NOT EXISTS "vector"; â”‚ CREATE EXTENSION IF NOT EXISTS "pg\_trgm"; â”‚ â”œâ”€â”€ \-- â”€â”€ Database: n8n â”€â”€ â”‚ CREATE DATABASE n8n OWNER ${POSTGRES\_USER}; â”‚ \\c n8n â”‚ CREATE EXTENSION IF NOT EXISTS "uuid-ossp"; â”‚ CREATE EXTENSION IF NOT EXISTS "pgcrypto"; â”‚ â”œâ”€â”€ \-- â”€â”€ Database: SuperTokens â”€â”€ â”‚ CREATE DATABASE supertokens OWNER ${POSTGRES\_USER}; â”‚ \\c supertokens â”‚ CREATE EXTENSION IF NOT EXISTS "uuid-ossp"; â”‚ â”œâ”€â”€ \-- â”€â”€ Database: Grafana â”€â”€ â”‚ CREATE DATABASE grafana OWNER ${POSTGRES\_USER}; â”‚ â”œâ”€â”€ \-- â”€â”€ Database: Platform (shared/custom tables) â”€â”€ â”‚ CREATE DATABASE platform OWNER ${POSTGRES\_USER}; â”‚ \\c platform â”‚ CREATE EXTENSION IF NOT EXISTS "uuid-ossp"; â”‚ CREATE EXTENSION IF NOT EXISTS "pgcrypto"; â”‚ CREATE EXTENSION IF NOT EXISTS "vector"; â”‚ CREATE EXTENSION IF NOT EXISTS "hstore"; â”‚  
â”‚ \-- â”€â”€ API Key Management â”€â”€ â”‚ CREATE TABLE IF NOT EXISTS api\_keys ( â”‚ id UUID PRIMARY KEY DEFAULT uuid\_generate\_v4(), â”‚ key\_hash VARCHAR(128) NOT NULL UNIQUE, â”‚ key\_prefix VARCHAR(10) NOT NULL, \-- e.g., "sk-plat-" â”‚ name VARCHAR(255) NOT NULL, â”‚ description TEXT, â”‚ user\_id VARCHAR(255), \-- links to SuperTokens user â”‚ permissions JSONB DEFAULT '\["read"\]'::jsonb, â”‚ rate\_limit\_rpm INTEGER DEFAULT 60, â”‚ rate\_limit\_rpd INTEGER DEFAULT 1000, â”‚ monthly\_budget\_usd DECIMAL(10,2), â”‚ total\_spend\_usd DECIMAL(10,2) DEFAULT 0, â”‚ allowed\_models TEXT\[\], \-- NULL \= all models â”‚ allowed\_ips INET\[\], \-- NULL \= any IP â”‚ metadata JSONB DEFAULT '{}'::jsonb, â”‚ is\_active BOOLEAN DEFAULT true, â”‚ expires\_at TIMESTAMP WITH TIME ZONE, â”‚ last\_used\_at TIMESTAMP WITH TIME ZONE, â”‚ created\_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(), â”‚ updated\_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() â”‚ ); â”‚  
â”‚ CREATE INDEX idx\_api\_keys\_hash ON api\_keys(key\_hash); â”‚ CREATE INDEX idx\_api\_keys\_user ON api\_keys(user\_id); â”‚ CREATE INDEX idx\_api\_keys\_active ON api\_keys(is\_active) WHERE is\_active \= true; â”‚  
â”‚ \-- â”€â”€ Usage Tracking â”€â”€ â”‚ CREATE TABLE IF NOT EXISTS usage\_logs ( â”‚ id UUID PRIMARY KEY DEFAULT uuid\_generate\_v4(), â”‚ api\_key\_id UUID REFERENCES api\_keys(id), â”‚ user\_id VARCHAR(255), â”‚ model VARCHAR(255) NOT NULL, â”‚ provider VARCHAR(100), â”‚ request\_type VARCHAR(50) DEFAULT 'chat', \-- chat, completion, embedding, image â”‚ input\_tokens INTEGER DEFAULT 0, â”‚ output\_tokens INTEGER DEFAULT 0, â”‚ total\_tokens INTEGER DEFAULT 0, â”‚ input\_cost\_usd DECIMAL(10,6) DEFAULT 0, â”‚ output\_cost\_usd DECIMAL(10,6) DEFAULT 0, â”‚ total\_cost\_usd DECIMAL(10,6) DEFAULT 0, â”‚ latency\_ms INTEGER, â”‚ status\_code INTEGER, â”‚ error\_message TEXT, â”‚ cache\_hit BOOLEAN DEFAULT false, â”‚ metadata JSONB DEFAULT '{}'::jsonb, â”‚ requested\_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() â”‚ ); â”‚  
â”‚ \-- Partitioned by month for performance â”‚ CREATE INDEX idx\_usage\_requested ON usage\_logs(requested\_at); â”‚ CREATE INDEX idx\_usage\_user ON usage\_logs(user\_id); â”‚ CREATE INDEX idx\_usage\_model ON usage\_logs(model); â”‚ CREATE INDEX idx\_usage\_api\_key ON usage\_logs(api\_key\_id); â”‚ CREATE INDEX idx\_usage\_cost ON usage\_logs(total\_cost\_usd) WHERE total\_cost\_usd \> 0; â”‚  
â”‚ \-- â”€â”€ Budget Alerts â”€â”€ â”‚ CREATE TABLE IF NOT EXISTS budget\_alerts ( â”‚ id UUID PRIMARY KEY DEFAULT uuid\_generate\_v4(), â”‚ scope VARCHAR(50) NOT NULL, \-- 'global', 'user', 'api\_key', 'model' â”‚ scope\_id VARCHAR(255), \-- NULL for global, user\_id, key\_id, model name â”‚ period VARCHAR(20) DEFAULT 'monthly', \-- 'daily', 'weekly', 'monthly' â”‚ budget\_usd DECIMAL(10,2) NOT NULL, â”‚ alert\_threshold\_pct INTEGER DEFAULT 80, \-- alert at 80% of budget â”‚ hard\_limit BOOLEAN DEFAULT false, \-- if true, block requests over budget â”‚ notification\_channels TEXT\[\] DEFAULT ARRAY\['log'\], \-- 'log', 'webhook', 'email' â”‚ webhook\_url TEXT, â”‚ current\_spend\_usd DECIMAL(10,2) DEFAULT 0, â”‚ last\_alert\_at TIMESTAMP WITH TIME ZONE, â”‚ is\_active BOOLEAN DEFAULT true, â”‚ created\_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() â”‚ ); â”‚  
â”‚ \-- â”€â”€ RAG Document Registry â”€â”€ â”‚ CREATE TABLE IF NOT EXISTS rag\_documents ( â”‚ id UUID PRIMARY KEY DEFAULT uuid\_generate\_v4(), â”‚ source VARCHAR(50) NOT NULL, \-- 'upload', 'gdrive', 'url', 'api' â”‚ source\_path TEXT, â”‚ filename VARCHAR(500) NOT NULL, â”‚ file\_type VARCHAR(50), â”‚ file\_size\_bytes BIGINT, â”‚ checksum\_sha256 VARCHAR(64), â”‚ chunk\_count INTEGER DEFAULT 0, â”‚ embedding\_model VARCHAR(255), â”‚ vector\_db VARCHAR(50), \-- 'qdrant', 'weaviate', 'milvus', 'pgvector' â”‚ collection\_name VARCHAR(255), â”‚ processing\_status VARCHAR(50) DEFAULT 'pending', \-- pending, processing, completed, failed â”‚ processing\_error TEXT, â”‚ last\_synced\_at TIMESTAMP WITH TIME ZONE, â”‚ metadata JSONB DEFAULT '{}'::jsonb, â”‚ created\_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(), â”‚ updated\_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() â”‚ ); â”‚  
â”‚ CREATE INDEX idx\_rag\_docs\_source ON rag\_documents(source); â”‚ CREATE INDEX idx\_rag\_docs\_status ON rag\_documents(processing\_status); â”‚ CREATE INDEX idx\_rag\_docs\_checksum ON rag\_documents(checksum\_sha256); â”‚  
â”‚ \-- â”€â”€ Audit Log â”€â”€ â”‚ CREATE TABLE IF NOT EXISTS audit\_log ( â”‚ id BIGSERIAL PRIMARY KEY, â”‚ event\_type VARCHAR(100) NOT NULL, \-- 'api\_key.created', 'model.added', 'budget.exceeded' â”‚ actor\_type VARCHAR(50), \-- 'user', 'system', 'api\_key' â”‚ actor\_id VARCHAR(255), â”‚ resource\_type VARCHAR(100), â”‚ resource\_id VARCHAR(255), â”‚ details JSONB DEFAULT '{}'::jsonb, â”‚ ip\_address INET, â”‚ user\_agent TEXT, â”‚ created\_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() â”‚ ); â”‚  
â”‚ CREATE INDEX idx\_audit\_event ON audit\_log(event\_type); â”‚ CREATE INDEX idx\_audit\_actor ON audit\_log(actor\_id); â”‚ CREATE INDEX idx\_audit\_time ON audit\_log(created\_at); â”‚  
â”‚ \-- â”€â”€ Materialized View: Daily Cost Summary â”€â”€ â”‚ CREATE MATERIALIZED VIEW IF NOT EXISTS daily\_cost\_summary AS â”‚ SELECT â”‚ DATE(requested\_at) AS date, â”‚ model, â”‚ provider, â”‚ user\_id, â”‚ COUNT(\*) AS request\_count, â”‚ SUM(input\_tokens) AS total\_input\_tokens, â”‚ SUM(output\_tokens) AS total\_output\_tokens, â”‚ SUM(total\_tokens) AS total\_tokens, â”‚ SUM(total\_cost\_usd) AS total\_cost\_usd, â”‚ AVG(latency\_ms) AS avg\_latency\_ms, â”‚ PERCENTILE\_CONT(0.95) WITHIN GROUP (ORDER BY latency\_ms) AS p95\_latency\_ms, â”‚ SUM(CASE WHEN cache\_hit THEN 1 ELSE 0 END) AS cache\_hits, â”‚ SUM(CASE WHEN status\_code \>= 400 THEN 1 ELSE 0 END) AS error\_count â”‚ FROM usage\_logs â”‚ GROUP BY DATE(requested\_at), model, provider, user\_id; â”‚  
â”‚ CREATE UNIQUE INDEX idx\_daily\_cost\_unique â”‚ ON daily\_cost\_summary(date, model, provider, user\_id); â”‚  
â”‚ \-- â”€â”€ Function: Refresh daily summary (called by cron) â”€â”€ â”‚ CREATE OR REPLACE FUNCTION refresh\_daily\_cost\_summary() â”‚ RETURNS void AS $$ â”‚ BEGIN â”‚ REFRESH MATERIALIZED VIEW CONCURRENTLY daily\_cost\_summary; â”‚ END; â”‚ $$ LANGUAGE plpgsql; â”‚  
â”‚ \-- â”€â”€ Function: Check budget and return status â”€â”€ â”‚ CREATE OR REPLACE FUNCTION check\_budget( â”‚ p\_scope VARCHAR, â”‚ p\_scope\_id VARCHAR DEFAULT NULL â”‚ ) RETURNS TABLE( â”‚ budget\_usd DECIMAL, â”‚ current\_spend DECIMAL, â”‚ remaining DECIMAL, â”‚ pct\_used DECIMAL, â”‚ is\_exceeded BOOLEAN, â”‚ hard\_limit BOOLEAN â”‚ ) AS $$ â”‚ BEGIN â”‚ RETURN QUERY â”‚ SELECT â”‚ ba.budget\_usd, â”‚ COALESCE(SUM(ul.total\_cost\_usd), 0\) AS current\_spend, â”‚ ba.budget\_usd \- COALESCE(SUM(ul.total\_cost\_usd), 0\) AS remaining, â”‚ CASE WHEN ba.budget\_usd \> 0 â”‚ THEN ROUND(COALESCE(SUM(ul.total\_cost\_usd), 0\) / ba.budget\_usd \* 100, 2\) â”‚ ELSE 0 â”‚ END AS pct\_used, â”‚ COALESCE(SUM(ul.total\_cost\_usd), 0\) \>= ba.budget\_usd AS is\_exceeded, â”‚ ba.hard\_limit â”‚ FROM budget\_alerts ba â”‚ LEFT JOIN usage\_logs ul ON ( â”‚ CASE â”‚ WHEN ba.scope \= 'global' THEN true â”‚ WHEN ba.scope \= 'user' THEN ul.user\_id \= ba.scope\_id â”‚ WHEN ba.scope \= 'model' THEN ul.model \= ba.scope\_id â”‚ WHEN ba.scope \= 'api\_key' THEN ul.api\_key\_id::text \= ba.scope\_id â”‚ END â”‚ AND ul.requested\_at \>= DATE\_TRUNC( â”‚ CASE ba.period â”‚ WHEN 'daily' THEN 'day' â”‚ WHEN 'weekly' THEN 'week' â”‚ WHEN 'monthly' THEN 'month' â”‚ END, â”‚ NOW() â”‚ ) â”‚ ) â”‚ WHERE ba.scope \= p\_scope â”‚ AND (ba.scope\_id \= p\_scope\_id OR (p\_scope\_id IS NULL AND ba.scope \= 'global')) â”‚ AND ba.is\_active \= true â”‚ GROUP BY ba.id, ba.budget\_usd, ba.hard\_limit; â”‚ END; â”‚ $$ LANGUAGE plpgsql; â”‚  
â”‚ \-- â”€â”€ Insert default global budget â”€â”€ â”‚ INSERT INTO budget\_alerts (scope, period, budget\_usd, alert\_threshold\_pct, hard\_limit) â”‚ VALUES ('global', 'monthly', 500.00, 80, false) â”‚ ON CONFLICT DO NOTHING; â”‚  
â”‚ \-- â”€â”€ Grant permissions â”€â”€ â”‚ \\c aiplatform â”‚ GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO ${POSTGRES\_USER}; â”‚ GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO ${POSTGRES\_USER};

\---

\#\# 18\. Dify Reverse Proxy Configuration

\#\#\# Dify Internal Architecture (requires nginx between Caddy and Dify components)

generate\_dify\_nginx\_conf() â”‚ â”œâ”€â”€ File: $CONFIG\_DIR/dify/nginx.conf â”‚ â”œâ”€â”€ upstream dify\_api { â”‚ server dify-api:5001; â”‚ } â”‚  
â”‚ upstream dify\_web { â”‚ server dify-web:3000; â”‚ } â”‚  
â”‚ server { â”‚ listen 80; â”‚ server\_name \_; â”‚  
â”‚ client\_max\_body\_size 100M; â”‚  
â”‚ \# â”€â”€ API routes â”€â”€ â”‚ location /v1 { â”‚ proxy\_pass [http://dify\_api](http://dify_api); â”‚ proxy\_set\_header Host $host; â”‚ proxy\_set\_header X-Real-IP $remote\_addr; â”‚ proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for; â”‚ proxy\_set\_header X-Forwarded-Proto $scheme; â”‚ proxy\_read\_timeout 300s; â”‚ proxy\_send\_timeout 300s; â”‚ } â”‚  
â”‚ location /console/api { â”‚ proxy\_pass [http://dify\_api](http://dify_api); â”‚ proxy\_set\_header Host $host; â”‚ proxy\_set\_header X-Real-IP $remote\_addr; â”‚ proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for; â”‚ proxy\_set\_header X-Forwarded-Proto $scheme; â”‚ proxy\_read\_timeout 300s; â”‚ } â”‚  
â”‚ location /api { â”‚ proxy\_pass [http://dify\_api](http://dify_api); â”‚ proxy\_set\_header Host $host; â”‚ proxy\_set\_header X-Real-IP $remote\_addr; â”‚ proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for; â”‚ proxy\_set\_header X-Forwarded-Proto $scheme; â”‚ proxy\_read\_timeout 300s; â”‚ } â”‚  
â”‚ location /files { â”‚ proxy\_pass [http://dify\_api](http://dify_api); â”‚ proxy\_set\_header Host $host; â”‚ proxy\_set\_header X-Real-IP $remote\_addr; â”‚ proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for; â”‚ proxy\_set\_header X-Forwarded-Proto $scheme; â”‚ } â”‚  
â”‚ \# â”€â”€ SSE (Server-Sent Events) for streaming â”€â”€ â”‚ location /v1/chat-messages { â”‚ proxy\_pass [http://dify\_api](http://dify_api); â”‚ proxy\_set\_header Host $host; â”‚ proxy\_set\_header X-Real-IP $remote\_addr; â”‚ proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for; â”‚ proxy\_set\_header X-Forwarded-Proto $scheme; â”‚ proxy\_set\_header Connection ''; â”‚ proxy\_http\_version 1.1; â”‚ chunked\_transfer\_encoding off; â”‚ proxy\_buffering off; â”‚ proxy\_cache off; â”‚ proxy\_read\_timeout 600s; â”‚ } â”‚  
â”‚ location /v1/completion-messages { â”‚ proxy\_pass [http://dify\_api](http://dify_api); â”‚ proxy\_set\_header Host $host; â”‚ proxy\_set\_header X-Real-IP $remote\_addr; â”‚ proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for; â”‚ proxy\_set\_header X-Forwarded-Proto $scheme; â”‚ proxy\_set\_header Connection ''; â”‚ proxy\_http\_version 1.1; â”‚ chunked\_transfer\_encoding off; â”‚ proxy\_buffering off; â”‚ proxy\_cache off; â”‚ proxy\_read\_timeout 600s; â”‚ } â”‚  
â”‚ \# â”€â”€ Web UI (frontend) â”€â”€ â”‚ location / { â”‚ proxy\_pass [http://dify\_web](http://dify_web); â”‚ proxy\_set\_header Host $host; â”‚ proxy\_set\_header X-Real-IP $remote\_addr; â”‚ proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for; â”‚ proxy\_set\_header X-Forwarded-Proto $scheme; â”‚ } â”‚  
â”‚ \# â”€â”€ WebSocket support â”€â”€ â”‚ location /ws { â”‚ proxy\_pass [http://dify\_api](http://dify_api); â”‚ proxy\_http\_version 1.1; â”‚ proxy\_set\_header Upgrade $http\_upgrade; â”‚ proxy\_set\_header Connection "upgrade"; â”‚ proxy\_set\_header Host $host; â”‚ proxy\_read\_timeout 86400; â”‚ } â”‚ } â”‚ â””â”€â”€ Dify container "dify-nginx" uses this config and Caddy proxies to dify-nginx:80

\---

\#\# 19\. OpenClaw / Open WebUI Integration Details

\#\#\# Open WebUI Configuration for LiteLLM Backend

configure\_open\_webui() â”‚ â”œâ”€â”€ Open WebUI environment variables (in docker-compose): â”‚  
â”‚ OPENAI\_API\_BASE\_URL: [http://litellm:4000/v1](http://litellm:4000/v1) â”‚ OPENAI\_API\_KEY: ${LITELLM\_MASTER\_KEY} â”‚ OLLAMA\_BASE\_URL: [http://host.docker.internal:11434](http://host.docker.internal:11434) â”‚ WEBUI\_AUTH: true â”‚ WEBUI\_NAME: "AI Platform" â”‚ ENABLE\_SIGNUP: true â”‚ ENABLE\_RAG\_WEB\_SEARCH: true â”‚ RAG\_EMBEDDING\_ENGINE: openai â”‚ RAG\_OPENAI\_API\_BASE\_URL: [http://litellm:4000/v1](http://litellm:4000/v1) â”‚ RAG\_OPENAI\_API\_KEY: ${LITELLM\_MASTER\_KEY} â”‚ RAG\_EMBEDDING\_MODEL: nomic-embed-text â”‚ ENABLE\_IMAGE\_GENERATION: false â”‚ DEFAULT\_MODELS: "mistral,llama3.1:8b,gpt-4o-mini" â”‚ ENABLE\_ADMIN\_EXPORT: true â”‚ ENABLE\_COMMUNITY\_SHARING: false â”‚ â”œâ”€â”€ Volume mounts: â”‚ \- ${DATA\_DIR}/open-webui:/app/backend/data â”‚ â”œâ”€â”€ Open WebUI sees ALL models via LiteLLM: â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ Open WebUI Model Selector â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ LOCAL (Ollama via LiteLLM): â”‚ â”‚ â”‚ â€¢ tinyllama â”‚ â”‚ â”‚ â€¢ mistral â”‚ â”‚ â”‚ â€¢ llama3.1:8b â”‚ â”‚ â”‚ â€¢ codellama:7b â”‚ â”‚ â”‚ â€¢ nomic-embed-text â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ CLOUD (via LiteLLM): â”‚ â”‚ â”‚ â€¢ gpt-4o â”‚ â”‚ â”‚ â€¢ gpt-4o-mini â”‚ â”‚ â”‚ â€¢ claude-3.5-sonnet â”‚ â”‚ â”‚ â€¢ gemini-1.5-pro â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ All requests â†’ LiteLLM â†’ provider â”‚ â”‚ â”‚ Cost tracking unified across all models â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”œâ”€â”€ RAG Integration: â”‚ Open WebUI has built-in RAG that can use: â”‚ 1\. Local documents uploaded via UI â”‚ 2\. Web search results (via SearXNG or Google) â”‚ 3\. External knowledge bases â”‚  
â”‚ For Google Drive documents: â”‚ \- gdrive-sync.sh syncs to ${GDRIVE\_SYNC\_DIR} â”‚ \- Configure Open WebUI to scan that directory â”‚ \- Or use Dify's knowledge base (more advanced) â”‚ â””â”€â”€ Authentication: If SuperTokens is enabled, Open WebUI delegates auth via WEBUI\_AUTH\_TRUSTED\_EMAIL\_HEADER pattern (configured in Caddy as X-Auth-Email from SuperTokens middleware)

\#\#\# Open WebUI â†” Dify Integration

Dual UI Strategy: â”‚ â”œâ”€â”€ Open WebUI \= Simple chat interface â”‚ \- End users who just want to chat â”‚ \- Model switching on the fly â”‚ \- Simple RAG (upload \+ ask) â”‚ \- Ollama model management â”‚ â”œâ”€â”€ Dify \= Advanced AI application builder â”‚ \- Workflow automation â”‚ \- Multi-step agents â”‚ \- Complex RAG pipelines â”‚ \- API endpoint generation â”‚ \- Prompt engineering studio â”‚ â”œâ”€â”€ Both share: â”‚ \- LiteLLM as unified model gateway â”‚ \- Same Ollama models â”‚ \- Same cloud API keys â”‚ \- Same PostgreSQL (different databases) â”‚ \- Same Redis â”‚ \- Same vector DB â”‚ â””â”€â”€ Cross-Integration: \- Dify APIs can be called from Open WebUI "Tools" \- n8n can orchestrate between both \- Flowise agents can use Dify knowledge bases

\---

\#\# 20\. Cost Management & Budget Controls

\#\#\# Architecture

Cost Management Flow: â”‚ â”œâ”€â”€ Layer 1: LiteLLM (Real-time tracking) â”‚ â”‚ â”‚ â”‚ Every API request â†’ LiteLLM logs: â”‚ â”‚ \- model, tokens (in/out), cost, latency â”‚ â”‚ \- stored in litellm PostgreSQL DB â”‚ â”‚ \- callbacks to custom webhook â”‚ â”‚ â”‚ â”‚ LiteLLM config (litellm\_config.yaml): â”‚ â”‚ litellm\_settings: â”‚ â”‚ success\_callback: \["postgres", "custom\_callback\_api"\] â”‚ â”‚ max\_budget: 500 \# global monthly limit USD â”‚ â”‚ budget\_duration: "monthly" â”‚ â”‚  
â”‚ â”‚ general\_settings: â”‚ â”‚ max\_parallel\_requests: 100 â”‚ â”‚ global\_max\_parallel\_requests: 200 â”‚ â”‚ â”‚ â””â”€â”€ Per-key budgets via LiteLLM admin API: â”‚ curl \-X POST [http://litellm:4000/key/generate](http://litellm:4000/key/generate)  
 â”‚ \-H "Authorization: Bearer ${LITELLM\_MASTER\_KEY}"  
 â”‚ \-d '{ â”‚ "max\_budget": 50.0, â”‚ "budget\_duration": "monthly", â”‚ "models": \["gpt-4o-mini", "mistral"\], â”‚ "max\_parallel\_requests": 10, â”‚ "tpm\_limit": 100000, â”‚ "rpm\_limit": 60, â”‚ "metadata": {"user": "team-a"} â”‚ }' â”‚ â”œâ”€â”€ Layer 2: Platform DB (Aggregated analytics) â”‚ â”‚ â”‚ â”‚ Materialized view: daily\_cost\_summary â”‚ â”‚ \- Refreshed every 15 minutes via pg\_cron or systemd â”‚ â”‚ \- Powers Grafana dashboards â”‚ â”‚ â”‚ â”‚ Budget alerts table checked by: â”‚ â”‚ \- Prometheus alerting rules â”‚ â”‚ \- n8n scheduled workflow â”‚ â”‚ \- Custom webhook from LiteLLM â”‚ â”‚ â”‚ â””â”€â”€ check\_budget() function returns: â”‚ \- current spend vs budget â”‚ \- percentage used â”‚ \- whether hard limit is exceeded â”‚ â”œâ”€â”€ Layer 3: Grafana Dashboards â”‚ â”‚ â”‚ â”‚ Dashboard: "AI Platform â€” Cost Overview" â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Monthly Spend â”‚ Budget Remaining â”‚ â”‚ â”‚ â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ $347.22 â”‚ â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ $152.78 â”‚ â”‚ â”‚ â”‚ of $500.00 (69.4%) â”‚ 69.4% used â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ â”‚ Cost by Model (30d) â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ gpt-4o â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ $180 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ claude-3.5 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ $98 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ gpt-4o-mini â–ˆâ–ˆâ–ˆ $42 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ mistral(local)â”‚ $0 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ llama3.1(local)â”‚ $0 â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ â”‚ Cost by User (30d) â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ team-dev â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ $210 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ team-support â–ˆâ–ˆâ–ˆâ–ˆ $87 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ team-sales â–ˆâ–ˆ $50 â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ â”‚ Daily Spend Trend â”‚ â”‚ â”‚ â”‚ $25 â”¤ â•­â”€â•® â”‚ â”‚ â”‚ â”‚ $20 â”¤ â•­â”€â•® â”‚ â”‚ â•­â•® â”‚ â”‚ â”‚ â”‚ $15 â”¤ â•­â”€â•¯ â•°â”€â”€â•¯ â•°â”€â•¯â•°â•® â”‚ â”‚ â”‚ â”‚ $10 â”¤â•­â”€â•¯ â•°â”€â”€â•® â”‚ â”‚ â”‚ â”‚ $5 â”¤â•¯ â•°â”€â”€ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”‚ â”‚ â”‚ 1 5 10 15 20 25 â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ â”‚ Cache Savings â”‚ â”‚ â”‚ â”‚ Requests cached: 12,847 (34.2%) â”‚ â”‚ â”‚ â”‚ Estimated savings: $89.50 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Local vs Cloud â”‚ â”‚ â”‚ â”‚ Local model requests: 45,230 (68%) â”‚ â”‚ â”‚ â”‚ Cloud model requests: 21,340 (32%) â”‚ â”‚ â”‚ â”‚ Estimated local savings: $412.00 â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€ Dashboard provisioned automatically by Script 2 â”‚ (JSON model in $CONFIG\_DIR/grafana/dashboards/) â”‚ â”œâ”€â”€ Layer 4: Alert Rules â”‚ â”‚ â”‚ â”‚ Prometheus alerting rules: â”‚ â”‚  
â”‚ â”‚ groups: â”‚ â”‚ \- name: cost\_alerts â”‚ â”‚ rules: â”‚ â”‚ \- alert: BudgetWarning â”‚ â”‚ expr: ai\_platform\_monthly\_spend\_usd / ai\_platform\_monthly\_budget\_usd \> 0.8 â”‚ â”‚ for: 5m â”‚ â”‚ labels: â”‚ â”‚ severity: warning â”‚ â”‚ annotations: â”‚ â”‚ summary: "AI Platform spend at {{ $value | humanizePercentage }} of budget" â”‚ â”‚ â”‚ â”‚ \- alert: BudgetCritical â”‚ â”‚ expr: ai\_platform\_monthly\_spend\_usd / ai\_platform\_monthly\_budget\_usd \> 0.95 â”‚ â”‚ for: 5m â”‚ â”‚ labels: â”‚ â”‚ severity: critical â”‚ â”‚ annotations: â”‚ â”‚ summary: "AI Platform spend at {{ $ value | humanizePercentage }} â€” approaching limit" â”‚ â”‚ â”‚ â”‚ \- alert: UnusualSpendSpike â”‚ â”‚ expr: \> â”‚ â”‚ rate(ai\_platform\_daily\_spend\_usd\[1h\]) \> â”‚ â”‚ 2 \* avg\_over\_time(ai\_platform\_daily\_spend\_usd\[7d\]) â”‚ â”‚ for: 15m â”‚ â”‚ labels: â”‚ â”‚ severity: warning â”‚ â”‚ annotations: â”‚ â”‚ summary: "Unusual spending spike detected" â”‚ â”‚ â”‚ â”‚ \- alert: HighErrorRate â”‚ â”‚ expr: \> â”‚ â”‚ rate(litellm\_request\_errors\_total\[5m\]) / â”‚ â”‚ rate(litellm\_requests\_total\[5m\]) \> 0.05 â”‚ â”‚ for: 5m â”‚ â”‚ labels: â”‚ â”‚ severity: warning â”‚ â”‚ annotations: â”‚ â”‚ summary: "LiteLLM error rate above 5%" â”‚ â”‚ â”‚ â””â”€â”€ n8n workflow for alerts: â”‚ Trigger: Webhook from Prometheus AlertManager â”‚ Actions: â”‚ 1\. Parse alert details â”‚ 2\. Check budget\_alerts table for context â”‚ 3\. Send notification: â”‚ \- Slack webhook (if configured) â”‚ \- Email (if SMTP configured) â”‚ \- Webhook (custom) â”‚ 4\. If hard\_limit exceeded: â”‚ \- Call LiteLLM API to disable expensive models â”‚ \- Log to audit\_log â”‚ â””â”€â”€ Layer 5: Smart Routing (Cost Optimization) â”‚ â”‚ LiteLLM router config for cost-aware routing: â”‚  
 â”‚ router\_settings: â”‚ routing\_strategy: "cost-based" â”‚ \# Try local model first, fall back to cloud â”‚ model\_group\_alias: â”‚ "default-chat": \["ollama/mistral", "gpt-4o-mini"\] â”‚ "code-assist": \["ollama/codellama:7b", "gpt-4o"\] â”‚  
 â”‚ \# Fallback chain: local â†’ cheap cloud â†’ expensive cloud â”‚ fallbacks: â”‚ \- model\_name: "ollama/mistral" â”‚ fallback: "gpt-4o-mini" â”‚ \- model\_name: "gpt-4o-mini" â”‚ fallback: "gpt-4o" â”‚  
 â”‚ Caching (Redis-backed): â”‚ litellm\_settings: â”‚ cache: true â”‚ cache\_params: â”‚ type: "redis" â”‚ host: "redis" â”‚ port: 6379 â”‚ password: " $ {REDIS\_PASSWORD}" â”‚ ttl: 3600 â”‚ \# Semantic caching with embeddings â”‚ supported\_call\_types: â”‚ \- "acompletion" â”‚ \- "completion" â”‚ \- "aembedding" â”‚ \- "embedding" â”‚ â””â”€â”€ Result: Automatic cost optimization \- Simple requests â†’ local Ollama (free) \- Cache hits â†’ zero cost \- Complex requests â†’ cheapest capable cloud model \- Budget exceeded â†’ block cloud, allow local only

\#\#\# Budget Management n8n Workflow

n8n Workflow: "Budget Monitor" (auto-imported) â”‚ â”œâ”€â”€ Trigger: Cron every 15 minutes â”‚ â”œâ”€â”€ Step 1: Query PostgreSQL â”‚ SELECT \* FROM check\_budget('global'); â”‚ SELECT \* FROM check\_budget('user', user\_id) â”‚ FROM (SELECT DISTINCT user\_id FROM usage\_logs â”‚ WHERE requested\_at \> NOW() \- INTERVAL '1 day'); â”‚ â”œâ”€â”€ Step 2: Evaluate thresholds â”‚ For each budget result: â”‚ if pct\_used \>= alert\_threshold\_pct AND NOT already\_alerted\_today: â”‚ â†’ trigger alert â”‚ if is\_exceeded AND hard\_limit: â”‚ â†’ trigger enforcement â”‚ â”œâ”€â”€ Step 3: Alert (if threshold crossed) â”‚ \- Log to audit\_log â”‚ \- Send webhook/email/Slack â”‚ \- Update last\_alert\_at â”‚ â”œâ”€â”€ Step 4: Enforce (if hard limit exceeded) â”‚ POST [http://litellm:4000/model/update](http://litellm:4000/model/update) â”‚ { â”‚ "model\_id": "\<cloud-models\>", â”‚ "model\_info": { "mode": "blocked" } â”‚ } â”‚ â†’ Cloud models disabled, local models still work â”‚ â”œâ”€â”€ Step 5: Refresh materialized view â”‚ SELECT refresh\_daily\_cost\_summary(); â”‚ â””â”€â”€ Step 6: Export metrics for Prometheus Write to /tmp/ai-platform-metrics.prom (node\_exporter textfile collector picks up)

\---  
\*\*End of Part 5 (Sections 17â€“20).\*\*  
\---

\#\# 21\. Docker Compose Generation (Dynamic Builder)

\#\#\# Generator Function

generate\_docker\_compose() â”‚ â”œâ”€â”€ The compose file is built dynamically based on: â”‚ \- ENABLED\_SERVICES from master.env â”‚ \- GPU\_AVAILABLE flag â”‚ \- Selected vector DB â”‚ \- DNS/SSL configuration â”‚ \- Hardware detection results â”‚ â”œâ”€â”€ File: $COMPOSE\_DIR/docker-compose.yml â”‚ â”œâ”€â”€ â”€â”€ Header â”€â”€ â”‚ cat \> $COMPOSE\_DIR/docker-compose.yml \<\< 'HEADER' â”‚ \# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚ \# AI Platform â€” Docker Compose â”‚ \# Auto-generated by Script 2 â€” DO NOT EDIT MANUALLY â”‚ \# Regenerate with: script-2-deploy.sh \--regenerate â”‚ \# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚  
â”‚ version: "3.8" â”‚  
â”‚ x-common-env: \&common-env â”‚ TZ: ${TIMEZONE} â”‚ PUID: 1000 â”‚ PGID: 1000 â”‚  
â”‚ x-restart-policy: \&restart-policy â”‚ restart: unless-stopped â”‚  
â”‚ x-logging: \&default-logging â”‚ logging: â”‚ driver: json-file â”‚ options: â”‚ max-size: "10m" â”‚ max-file: "3" â”‚  
â”‚ HEADER â”‚ â”œâ”€â”€ â”€â”€ Networks â”€â”€ â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< 'NETWORKS' â”‚ networks: â”‚ ai-platform: â”‚ driver: bridge â”‚ ipam: â”‚ config: â”‚ \- subnet: 172.28.0.0/16 â”‚ monitoring: â”‚ driver: bridge â”‚  
â”‚ NETWORKS â”‚ â”œâ”€â”€ â”€â”€ Volumes â”€â”€ â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ volumes: â”‚ postgres\_data: â”‚ driver: local â”‚ driver\_opts: â”‚ type: none â”‚ o: bind â”‚ device: ${DATA\_DIR}/postgres â”‚ redis\_data: â”‚ driver: local â”‚ driver\_opts: â”‚ type: none â”‚ o: bind â”‚ device: ${DATA\_DIR}/redis â”‚ caddy\_data: â”‚ driver: local â”‚ caddy\_config: â”‚ driver: local â”‚  
â”‚ EOF â”‚ â”œâ”€â”€ â”€â”€ Services Start â”€â”€ â”‚ echo "services:" \>\> $COMPOSE\_DIR/docker-compose.yml â”‚ â”œâ”€â”€ â”€â”€ Core: PostgreSQL â”€â”€ â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ PostgreSQL with pgvector â”€â”€ â”‚ postgres: â”‚ image: pgvector/pgvector:pg16 â”‚ container\_name: ai-postgres â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ environment: â”‚ POSTGRES\_USER: ${POSTGRES\_USER} â”‚ POSTGRES\_PASSWORD: ${POSTGRES\_PASSWORD} â”‚ POSTGRES\_DB: aiplatform â”‚ PGDATA: /var/lib/postgresql/data/pgdata â”‚ volumes: â”‚ \- postgres\_data:/var/lib/postgresql/data â”‚ \- ${CONFIG\_DIR}/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro â”‚ \- ${CONFIG\_DIR}/postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro â”‚ command: postgres \-c config\_file=/etc/postgresql/postgresql.conf â”‚ ports: â”‚ \- "127.0.0.1:5432:5432" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.10 â”‚ healthcheck: â”‚ test: \["CMD-SHELL", "pg\_isready \-U ${POSTGRES\_USER} \-d aiplatform"\] â”‚ interval: 10s â”‚ timeout: 5s â”‚ retries: 5 â”‚ start\_period: 30s â”‚ shm\_size: '256mb' â”‚ deploy: â”‚ resources: â”‚ limits: â”‚ memory: ${POSTGRES\_MEMORY\_LIMIT:-2G} â”‚  
â”‚ EOF â”‚ â”œâ”€â”€ â”€â”€ Core: Redis â”€â”€ â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ Redis with persistence â”€â”€ â”‚ redis: â”‚ image: redis:7-alpine â”‚ container\_name: ai-redis â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ command: \> â”‚ redis-server â”‚ \--requirepass ${REDIS\_PASSWORD} â”‚ \--maxmemory ${REDIS\_MAX\_MEMORY:-512mb} â”‚ \--maxmemory-policy allkeys-lru â”‚ \--appendonly yes â”‚ \--appendfsync everysec â”‚ \--save 60 1000 â”‚ \--save 300 100 â”‚ volumes: â”‚ \- redis\_data:/data â”‚ ports: â”‚ \- "127.0.0.1:6379:6379" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.11 â”‚ healthcheck: â”‚ test: \["CMD", "redis-cli", "-a", "${REDIS\_PASSWORD}", "ping"\] â”‚ interval: 10s â”‚ timeout: 5s â”‚ retries: 5 â”‚  
â”‚ EOF â”‚ â”œâ”€â”€ â”€â”€ Core: Caddy â”€â”€ â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ Caddy Reverse Proxy â”€â”€ â”‚ caddy: â”‚ image: caddy:2-alpine â”‚ container\_name: ai-caddy â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ ports: â”‚ \- "80:80" â”‚ \- "443:443" â”‚ \- "443:443/udp" â”‚ volumes: â”‚ \- ${CONFIG\_DIR}/caddy/Caddyfile:/etc/caddy/Caddyfile:ro â”‚ \- caddy\_data:/data â”‚ \- caddy\_config:/config â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.2 â”‚ depends\_on: â”‚ \- litellm â”‚ healthcheck: â”‚ test: \["CMD", "caddy", "version"\] â”‚ interval: 30s â”‚ timeout: 5s â”‚ retries: 3 â”‚  
â”‚ EOF â”‚ â”œâ”€â”€ â”€â”€ LiteLLM â”€â”€ â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ LiteLLM Proxy â”€â”€ â”‚ litellm: â”‚ image: ghcr.io/berriai/litellm:main-latest â”‚ container\_name: ai-litellm â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ env\_file: â”‚ \- ${ENV\_DIR}/litellm.env â”‚ volumes: â”‚ \- ${CONFIG\_DIR}/litellm/litellm\_config.yaml:/app/config.yaml:ro â”‚ command: \["--config", "/app/config.yaml", "--port", "4000", "--num\_workers", "4"\] â”‚ ports: â”‚ \- "127.0.0.1:4000:4000" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.20 â”‚ depends\_on: â”‚ postgres: â”‚ condition: service\_healthy â”‚ redis: â”‚ condition: service\_healthy â”‚ healthcheck: â”‚ test: \["CMD", "curl", "-f", "[http://localhost:4000/health"\]](http://localhost:4000/health) â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚ start\_period: 40s â”‚ deploy: â”‚ resources: â”‚ limits: â”‚ memory: 1G â”‚  
â”‚ EOF â”‚ â”œâ”€â”€ â”€â”€ Conditional: Dify Stack â”€â”€ â”‚ if service\_enabled "dify"; then â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ Dify API â”€â”€ â”‚ dify-api: â”‚ image: langgenius/dify-api:latest â”‚ container\_name: ai-dify-api â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ env\_file: â”‚ \- ${ENV\_DIR}/dify.env â”‚ volumes: â”‚ \- ${DATA\_DIR}/dify/storage:/app/api/storage â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.30 â”‚ depends\_on: â”‚ postgres: â”‚ condition: service\_healthy â”‚ redis: â”‚ condition: service\_healthy â”‚ deploy: â”‚ resources: â”‚ limits: â”‚ memory: 2G â”‚  
â”‚ \# â”€â”€ Dify Worker â”€â”€ â”‚ dify-worker: â”‚ image: langgenius/dify-api:latest â”‚ container\_name: ai-dify-worker â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ env\_file: â”‚ \- ${ENV\_DIR}/dify.env â”‚ command: celery \-A app.celery worker \-P gevent \-c 1 \--loglevel INFO â”‚ volumes: â”‚ \- ${DATA\_DIR}/dify/storage:/app/api/storage â”‚ networks: â”‚ \- ai-platform â”‚ depends\_on: â”‚ \- dify-api â”‚ deploy: â”‚ resources: â”‚ limits: â”‚ memory: 1G â”‚  
â”‚ \# â”€â”€ Dify Web â”€â”€ â”‚ dify-web: â”‚ image: langgenius/dify-web:latest â”‚ container\_name: ai-dify-web â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ environment: â”‚ CONSOLE\_API\_URL: https://${DIFY\_DOMAIN} â”‚ APP\_API\_URL: https://${DIFY\_DOMAIN} â”‚ SENTRY\_DSN: "" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.31 â”‚  
â”‚ \# â”€â”€ Dify Nginx â”€â”€ â”‚ dify-nginx: â”‚ image: nginx:alpine â”‚ container\_name: ai-dify-nginx â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ volumes: â”‚ \- ${CONFIG\_DIR}/dify/nginx.conf:/etc/nginx/conf.d/default.conf:ro â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.32 â”‚ depends\_on: â”‚ \- dify-api â”‚ \- dify-web â”‚  
â”‚ \# â”€â”€ Dify Sandbox â”€â”€ â”‚ dify-sandbox: â”‚ image: langgenius/dify-sandbox:latest â”‚ container\_name: ai-dify-sandbox â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ environment: â”‚ API\_KEY: ${DIFY\_SANDBOX\_KEY} â”‚ GIN\_MODE: release â”‚ WORKER\_TIMEOUT: 15 â”‚ networks: â”‚ \- ai-platform â”‚  
â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: n8n â”€â”€ â”‚ if service\_enabled "n8n"; then â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ n8n Workflow Automation â”€â”€ â”‚ n8n: â”‚ image: docker.n8n.io/n8nio/n8n:latest â”‚ container\_name: ai-n8n â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ env\_file: â”‚ \- ${ENV\_DIR}/n8n.env â”‚ environment: â”‚ \<\<: \*common-env â”‚ N8N\_HOST: ${N8N\_DOMAIN} â”‚ N8N\_PORT: 5678 â”‚ N8N\_PROTOCOL: https â”‚ WEBHOOK\_URL: https://${N8N\_DOMAIN}/ â”‚ N8N\_EDITOR\_BASE\_URL: https://${N8N\_DOMAIN}/ â”‚ DB\_TYPE: postgresdb â”‚ DB\_POSTGRESDB\_HOST: postgres â”‚ DB\_POSTGRESDB\_PORT: 5432 â”‚ DB\_POSTGRESDB\_DATABASE: n8n â”‚ DB\_POSTGRESDB\_USER: ${POSTGRES\_USER} â”‚ DB\_POSTGRESDB\_PASSWORD: ${POSTGRES\_PASSWORD} â”‚ N8N\_ENCRYPTION\_KEY: ${N8N\_ENCRYPTION\_KEY} â”‚ EXECUTIONS\_MODE: regular â”‚ GENERIC\_TIMEZONE: ${TIMEZONE} â”‚ N8N\_DIAGNOSTICS\_ENABLED: false â”‚ N8N\_PERSONALIZATION\_ENABLED: false â”‚ volumes: â”‚ \- ${DATA\_DIR}/n8n:/home/node/.n8n â”‚ ports: â”‚ \- "127.0.0.1:5678:5678" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.40 â”‚ depends\_on: â”‚ postgres: â”‚ condition: service\_healthy â”‚ healthcheck: â”‚ test: \["CMD-SHELL", "curl \-f [http://localhost:5678/healthz](http://localhost:5678/healthz) || exit 1"\] â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚ start\_period: 30s â”‚ deploy: â”‚ resources: â”‚ limits: â”‚ memory: 1G â”‚  
â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Open WebUI â”€â”€ â”‚ if service\_enabled "open-webui"; then â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ Open WebUI â”€â”€ â”‚ open-webui: â”‚ image: ghcr.io/open-webui/open-webui:main â”‚ container\_name: ai-open-webui â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ environment: â”‚ OPENAI\_API\_BASE\_URL: [http://litellm:4000/v1](http://litellm:4000/v1) â”‚ OPENAI\_API\_KEY: ${LITELLM\_MASTER\_KEY} â”‚ OLLAMA\_BASE\_URL: [http://host.docker.internal:11434](http://host.docker.internal:11434) â”‚ WEBUI\_AUTH: "true" â”‚ WEBUI\_NAME: "AI Platform" â”‚ WEBUI\_URL: https://${OPEN\_WEBUI\_DOMAIN} â”‚ ENABLE\_SIGNUP: "true" â”‚ ENABLE\_RAG\_WEB\_SEARCH: "true" â”‚ RAG\_EMBEDDING\_ENGINE: openai â”‚ RAG\_OPENAI\_API\_BASE\_URL: [http://litellm:4000/v1](http://litellm:4000/v1) â”‚ RAG\_OPENAI\_API\_KEY: ${LITELLM\_MASTER\_KEY} â”‚ RAG\_EMBEDDING\_MODEL: nomic-embed-text â”‚ ENABLE\_IMAGE\_GENERATION: "false" â”‚ ENABLE\_COMMUNITY\_SHARING: "false" â”‚ DEFAULT\_MODELS: "${DEFAULT\_CHAT\_MODEL:-mistral}" â”‚ volumes: â”‚ \- ${DATA\_DIR}/open-webui:/app/backend/data â”‚ ports: â”‚ \- "127.0.0.1:3000:8080" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.50 â”‚ depends\_on: â”‚ \- litellm â”‚ extra\_hosts: â”‚ \- "host.docker.internal:host-gateway" â”‚ healthcheck: â”‚ test: \["CMD", "curl", "-f", "[http://localhost:8080/"\]](http://localhost:8080/) â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚ start\_period: 30s â”‚ deploy: â”‚ resources: â”‚ limits: â”‚ memory: 1G â”‚  
â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Flowise â”€â”€ â”‚ if service\_enabled "flowise"; then â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ Flowise â”€â”€ â”‚ flowise: â”‚ image: flowiseai/flowise:latest â”‚ container\_name: ai-flowise â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ environment: â”‚ FLOWISE\_USERNAME: ${FLOWISE\_USER} â”‚ FLOWISE\_PASSWORD: ${FLOWISE\_PASSWORD} â”‚ DATABASE\_TYPE: postgres â”‚ DATABASE\_HOST: postgres â”‚ DATABASE\_PORT: 5432 â”‚ DATABASE\_NAME: aiplatform â”‚ DATABASE\_USER: ${POSTGRES\_USER} â”‚ DATABASE\_PASSWORD: ${POSTGRES\_PASSWORD} â”‚ APIKEY\_PATH: /root/.flowise â”‚ LOG\_LEVEL: info â”‚ volumes: â”‚ \- ${DATA\_DIR}/flowise:/root/.flowise â”‚ ports: â”‚ \- "127.0.0.1:3001:3000" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.55 â”‚ depends\_on: â”‚ postgres: â”‚ condition: service\_healthy â”‚ healthcheck: â”‚ test: \["CMD", "curl", "-f", "[http://localhost:3000/"\]](http://localhost:3000/) â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚  
â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Vector DB (Qdrant) â”€â”€ â”‚ if \[\[ "${VECTOR\_DB}" \== "qdrant" \]\]; then â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ Qdrant Vector Database â”€â”€ â”‚ qdrant: â”‚ image: qdrant/qdrant:latest â”‚ container\_name: ai-qdrant â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ environment: â”‚ QDRANT\_\_SERVICE\_\_API\_KEY: ${QDRANT\_API\_KEY} â”‚ QDRANT\_\_STORAGE\_\_STORAGE\_PATH: /qdrant/storage â”‚ QDRANT\_\_STORAGE\_\_SNAPSHOTS\_PATH: /qdrant/snapshots â”‚ volumes: â”‚ \- ${DATA\_DIR}/qdrant/storage:/qdrant/storage â”‚ \- ${DATA\_DIR}/qdrant/snapshots:/qdrant/snapshots â”‚ ports: â”‚ \- "127.0.0.1:6333:6333" â”‚ \- "127.0.0.1:6334:6334" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.60 â”‚ healthcheck: â”‚ test: \["CMD", "curl", "-f", "[http://localhost:6333/healthz"\]](http://localhost:6333/healthz) â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚ deploy: â”‚ resources: â”‚ limits: â”‚ memory: ${QDRANT\_MEMORY\_LIMIT:-1G} â”‚  
â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Vector DB (Weaviate) â”€â”€ â”‚ if \[\[ "${VECTOR\_DB}" \== "weaviate" \]\]; then â”‚ cat \>\> $ COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ Weaviate Vector Database â”€â”€ â”‚ weaviate: â”‚ image: semitechnologies/weaviate:latest â”‚ container\_name: ai-weaviate â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ environment: â”‚ QUERY\_DEFAULTS\_LIMIT: 25 â”‚ AUTHENTICATION\_APIKEY\_ENABLED: "true" â”‚ AUTHENTICATION\_APIKEY\_ALLOWED\_KEYS: " $ {WEAVIATE\_API\_KEY}" â”‚ AUTHENTICATION\_APIKEY\_USERS: "admin" â”‚ PERSISTENCE\_DATA\_PATH: /var/lib/weaviate â”‚ DEFAULT\_VECTORIZER\_MODULE: none â”‚ CLUSTER\_HOSTNAME: node1 â”‚ volumes: â”‚ \- ${DATA\_DIR}/weaviate:/var/lib/weaviate â”‚ ports: â”‚ \- "127.0.0.1:8080:8080" â”‚ \- "127.0.0.1:50051:50051" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.60 â”‚ healthcheck: â”‚ test: \["CMD", "curl", "-f", "[http://localhost:8080/v1/.well-known/ready"\]](http://localhost:8080/v1/.well-known/ready) â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚ deploy: â”‚ resources: â”‚ limits: â”‚ memory: ${WEAVIATE\_MEMORY\_LIMIT:-1G} â”‚  
â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Vector DB (Milvus) â”€â”€ â”‚ if \[\[ "${VECTOR\_DB}" \== "milvus" \]\]; then â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ Milvus Dependencies â”€â”€ â”‚ milvus-etcd: â”‚ image: quay.io/coreos/etcd:v3.5.5 â”‚ container\_name: ai-milvus-etcd â”‚ \<\<: \*restart-policy â”‚ environment: â”‚ ETCD\_AUTO\_COMPACTION\_MODE: revision â”‚ ETCD\_AUTO\_COMPACTION\_RETENTION: "1000" â”‚ ETCD\_QUOTA\_BACKEND\_BYTES: "4294967296" â”‚ ETCD\_SNAPSHOT\_COUNT: "50000" â”‚ volumes: â”‚ \- ${DATA\_DIR}/milvus/etcd:/etcd â”‚ command: etcd \-advertise-client-urls=[http://127.0.0.1:2379](http://127.0.0.1:2379) \-listen-client-urls [http://0.0.0.0:2379](http://0.0.0.0:2379) \--data-dir /etcd â”‚ networks: â”‚ \- ai-platform â”‚  
â”‚ milvus-minio: â”‚ image: minio/minio:RELEASE.2023-03-20T20-16-18Z â”‚ container\_name: ai-milvus-minio â”‚ \<\<: \*restart-policy â”‚ environment: â”‚ MINIO\_ACCESS\_KEY: minioadmin â”‚ MINIO\_SECRET\_KEY: minioadmin â”‚ volumes: â”‚ \- ${DATA\_DIR}/milvus/minio:/minio\_data â”‚ command: minio server /minio\_data \--console-address ":9001" â”‚ networks: â”‚ \- ai-platform â”‚ healthcheck: â”‚ test: \["CMD", "curl", "-f", "[http://localhost:9000/minio/health/live"\]](http://localhost:9000/minio/health/live) â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚  
â”‚ \# â”€â”€ Milvus Standalone â”€â”€ â”‚ milvus: â”‚ image: milvusdb/milvus:v2.3-latest â”‚ container\_name: ai-milvus â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ environment: â”‚ ETCD\_ENDPOINTS: milvus-etcd:2379 â”‚ MINIO\_ADDRESS: milvus-minio:9000 â”‚ volumes: â”‚ \- ${DATA\_DIR}/milvus/data:/var/lib/milvus â”‚ command: \["milvus", "run", "standalone"\] â”‚ ports: â”‚ \- "127.0.0.1:19530:19530" â”‚ \- "127.0.0.1:9091:9091" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.60 â”‚ depends\_on: â”‚ \- milvus-etcd â”‚ \- milvus-minio â”‚ healthcheck: â”‚ test: \["CMD", "curl", "-f", "[http://localhost:9091/healthz"\]](http://localhost:9091/healthz) â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚ deploy: â”‚ resources: â”‚ limits: â”‚ memory: ${MILVUS\_MEMORY\_LIMIT:-2G} â”‚  
â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: SuperTokens â”€â”€ â”‚ if service\_enabled "supertokens"; then â”‚ cat \>\> $ COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ SuperTokens Authentication â”€â”€ â”‚ supertokens: â”‚ image: registry.supertokens.io/supertokens/supertokens-postgresql:latest â”‚ container\_name: ai-supertokens â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ environment: â”‚ POSTGRESQL\_CONNECTION\_URI: "postgresql:// $ {POSTGRES\_USER}:${POSTGRES\_PASSWORD}@postgres:5432/supertokens" â”‚ API\_KEYS: ${SUPERTOKENS\_API\_KEY} â”‚ ports: â”‚ \- "127.0.0.1:3567:3567" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.70 â”‚ depends\_on: â”‚ postgres: â”‚ condition: service\_healthy â”‚ healthcheck: â”‚ test: \["CMD", "curl", "-f", "[http://localhost:3567/hello"\]](http://localhost:3567/hello) â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚  
â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Monitoring Stack â”€â”€ â”‚ if service\_enabled "monitoring"; then â”‚ cat \>\> $COMPOSE\_DIR/docker-compose.yml \<\< EOF â”‚ \# â”€â”€ Prometheus â”€â”€ â”‚ prometheus: â”‚ image: prom/prometheus:latest â”‚ container\_name: ai-prometheus â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ command: â”‚ \- '--config.file=/etc/prometheus/prometheus.yml' â”‚ \- '--storage.tsdb.path=/prometheus' â”‚ \- '--storage.tsdb.retention.time=30d' â”‚ \- '--storage.tsdb.retention.size=5GB' â”‚ \- '--web.enable-lifecycle' â”‚ volumes: â”‚ \- ${CONFIG\_DIR}/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro â”‚ \- ${CONFIG\_DIR}/prometheus/alert\_rules.yml:/etc/prometheus/alert\_rules.yml:ro â”‚ \- ${DATA\_DIR}/prometheus:/prometheus â”‚ ports: â”‚ \- "127.0.0.1:9090:9090" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.80 â”‚ monitoring: â”‚ healthcheck: â”‚ test: \["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "[http://localhost:9090/-/healthy"\]](http://localhost:9090/-/healthy) â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚  
â”‚ \# â”€â”€ Grafana â”€â”€ â”‚ grafana: â”‚ image: grafana/grafana:latest â”‚ container\_name: ai-grafana â”‚ \<\<: \*restart-policy â”‚ \<\<: \*default-logging â”‚ environment: â”‚ GF\_SECURITY\_ADMIN\_USER: ${GRAFANA\_USER} â”‚ GF\_SECURITY\_ADMIN\_PASSWORD: ${GRAFANA\_PASSWORD} â”‚ GF\_SERVER\_ROOT\_URL: https://${GRAFANA\_DOMAIN} â”‚ GF\_DATABASE\_TYPE: postgres â”‚ GF\_DATABASE\_HOST: postgres:5432 â”‚ GF\_DATABASE\_NAME: grafana â”‚ GF\_DATABASE\_USER: ${POSTGRES\_USER} â”‚ GF\_DATABASE\_PASSWORD: ${POSTGRES\_PASSWORD} â”‚ GF\_USERS\_ALLOW\_SIGN\_UP: "false" â”‚ GF\_AUTH\_ANONYMOUS\_ENABLED: "false" â”‚ GF\_INSTALL\_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource â”‚ volumes: â”‚ \- ${DATA\_DIR}/grafana:/var/lib/grafana â”‚ \- ${CONFIG\_DIR}/grafana/provisioning:/etc/grafana/provisioning:ro â”‚ \- ${CONFIG\_DIR}/grafana/dashboards:/var/lib/grafana/dashboards:ro â”‚ ports: â”‚ \- "127.0.0.1:3100:3000" â”‚ networks: â”‚ ai-platform: â”‚ ipv4\_address: 172.28.0.81 â”‚ monitoring: â”‚ depends\_on: â”‚ postgres: â”‚ condition: service\_healthy â”‚ healthcheck: â”‚ test: \["CMD", "curl", "-f", "[http://localhost:3000/api/health"\]](http://localhost:3000/api/health) â”‚ interval: 30s â”‚ timeout: 10s â”‚ retries: 5 â”‚  
â”‚ \# â”€â”€ Node Exporter â”€â”€ â”‚ node-exporter: â”‚ image: prom/node-exporter:latest â”‚ container\_name: ai-node-exporter â”‚ \<\<: \*restart-policy â”‚ command: â”‚ \- '--path.rootfs=/host' â”‚ \- '--collector.textfile.directory=/etc/node-exporter/textfile' â”‚ volumes: â”‚ \- /:/host:ro,rslave â”‚ \- /tmp:/etc/node-exporter/textfile:ro â”‚ ports: â”‚ \- "127.0.0.1:9100:9100" â”‚ networks: â”‚ \- monitoring â”‚ pid: host â”‚  
â”‚ \# â”€â”€ cAdvisor â”€â”€ â”‚ cadvisor: â”‚ image: gcr.io/cadvisor/cadvisor:latest â”‚ container\_name: ai-cadvisor â”‚ \<\<: \*restart-policy â”‚ volumes: â”‚ \- /:/rootfs:ro â”‚ \- /var/run:/var/run:ro â”‚ \- /sys:/sys:ro â”‚ \- /var/lib/docker/:/var/lib/docker:ro â”‚ \- /dev/disk/:/dev/disk:ro â”‚ ports: â”‚ \- "127.0.0.1:8081:8080" â”‚ networks: â”‚ \- monitoring â”‚ privileged: true â”‚ devices: â”‚ \- /dev/kmsg â”‚  
â”‚ EOF â”‚ fi â”‚ â””â”€â”€ echo " âœ“ docker-compose.yml generated" echo " Services: $(grep 'container\_name:' $COMPOSE\_DIR/docker-compose.yml | wc \-l)"

\#\#\# PostgreSQL Custom Configuration

generate\_postgres\_conf() â”‚ â”œâ”€â”€ File: $CONFIG\_DIR/postgres/postgresql.conf â”‚ â”œâ”€â”€ \# Auto-tuned based on available RAM: ${TOTAL\_RAM\_GB}GB â”‚  
â”‚ \# â”€â”€ Memory â”€â”€ â”‚ shared\_buffers \= ${PG\_SHARED\_BUFFERS} \# \~25% of RAM allocated to PG â”‚ effective\_cache\_size \= ${PG\_EFFECTIVE\_CACHE} \# \~50% of RAM allocated to PG â”‚ work\_mem \= ${PG\_WORK\_MEM} \# Per-operation sort memory â”‚ maintenance\_work\_mem \= ${PG\_MAINT\_MEM} \# For VACUUM, CREATE INDEX â”‚  
â”‚ \# â”€â”€ WAL â”€â”€ â”‚ wal\_buffers \= 16MB â”‚ max\_wal\_size \= 2GB â”‚ min\_wal\_size \= 512MB â”‚ checkpoint\_completion\_target \= 0.9 â”‚  
â”‚ \# â”€â”€ Connections â”€â”€ â”‚ max\_connections \= 200 â”‚  
â”‚ \# â”€â”€ Query Planner â”€â”€ â”‚ random\_page\_cost \= 1.1 \# SSD optimized â”‚ effective\_io\_concurrency \= 200 \# SSD optimized â”‚  
â”‚ \# â”€â”€ pgvector â”€â”€ â”‚ \# Enable parallel index builds for HNSW â”‚ max\_parallel\_workers\_per\_gather \= 4 â”‚ max\_parallel\_workers \= 8 â”‚ max\_parallel\_maintenance\_workers \= 4 â”‚  
â”‚ \# â”€â”€ Logging â”€â”€ â”‚ log\_min\_duration\_statement \= 1000 \# Log queries \> 1s â”‚ log\_checkpoints \= on â”‚ log\_lock\_waits \= on â”‚  
â”‚ \# â”€â”€ Locale â”€â”€ â”‚ timezone \= '${TIMEZONE}' â”‚ â”œâ”€â”€ Memory auto-tuning logic: â”‚ PG\_ALLOCATED\_RAM=$(( TOTAL\_RAM\_GB / 4 )) \# 25% of total RAM to PG â”‚ if \[\[ $PG\_ALLOCATED\_RAM \-lt 1 \]\]; then PG\_ALLOCATED\_RAM=1; fi â”‚ if \[\[ $ PG\_ALLOCATED\_RAM \-gt 8 \]\]; then PG\_ALLOCATED\_RAM=8; fi â”‚  
â”‚ PG\_SHARED\_BUFFERS=" $ {PG\_ALLOCATED\_RAM}GB" â”‚ PG\_EFFECTIVE\_CACHE=" $ (( PG\_ALLOCATED\_RAM \* 2 ))GB" â”‚ PG\_WORK\_MEM=" $ (( PG\_ALLOCATED\_RAM \* 64 ))MB" â”‚ PG\_MAINT\_MEM=" $ (( PG\_ALLOCATED\_RAM \* 256 ))MB" â”‚ â””â”€â”€ echo " âœ“ PostgreSQL config generated ( $ {PG\_ALLOCATED\_RAM}GB shared\_buffers)"

\---

\#\# 22\. Caddyfile Generation

generate\_caddyfile() â”‚ â”œâ”€â”€ File: $CONFIG\_DIR/caddy/Caddyfile â”‚ â”œâ”€â”€ â”€â”€ Global Options â”€â”€ â”‚ { â”‚ email ${ADMIN\_EMAIL} â”‚  
â”‚ \# If using IP-only (no domain), use internal CA â”‚ (if\[\[" {SSL\_MODE}" \== "selfsigned" \]\]; then â”‚ echo " local\_certs" â”‚ fi) â”‚  
â”‚ \# Rate limiting â”‚ order rate\_limit before basicauth â”‚  
â”‚ \# Logging â”‚ log { â”‚ output file /data/access.log { â”‚ roll\_size 100mb â”‚ roll\_keep 5 â”‚ } â”‚ } â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Snippet: Common Security Headers â”€â”€ â”‚ (security-headers) { â”‚ header { â”‚ X-Content-Type-Options "nosniff" â”‚ X-Frame-Options "SAMEORIGIN" â”‚ Referrer-Policy "strict-origin-when-cross-origin" â”‚ X-XSS-Protection "1; mode=block" â”‚ \-Server â”‚ } â”‚ } â”‚  
â”‚ (proxy-common) { â”‚ header\_up X-Real-IP {remote\_host} â”‚ header\_up X-Forwarded-For {remote\_host} â”‚ header\_up X-Forwarded-Proto {scheme} â”‚ } â”‚ â”œâ”€â”€ â”€â”€ LiteLLM (always enabled) â”€â”€ â”‚ ${LITELLM\_DOMAIN} { â”‚ import security-headers â”‚  
â”‚ \# Health check endpoint (no auth) â”‚ @health path /health /health/liveliness /health/readiness â”‚ handle @health { â”‚ reverse\_proxy litellm:4000 â”‚ } â”‚  
â”‚ \# API endpoints â”‚ handle { â”‚ reverse\_proxy litellm:4000 { â”‚ import proxy-common â”‚ transport http { â”‚ read\_timeout 300s â”‚ write\_timeout 300s â”‚ } â”‚ } â”‚ } â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Conditional: Dify â”€â”€ â”‚ if service\_enabled "dify"; then â”‚ cat \>\> Caddyfile \<\< EOF â”‚ ${DIFY\_DOMAIN} { â”‚ import security-headers â”‚  
â”‚ reverse\_proxy dify-nginx:80 { â”‚ import proxy-common â”‚ transport http { â”‚ read\_timeout 600s â”‚ write\_timeout 600s â”‚ } â”‚ \# SSE support â”‚ flush\_interval \-1 â”‚ } â”‚ } â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: n8n â”€â”€ â”‚ if service\_enabled "n8n"; then â”‚ cat \>\> Caddyfile \<\< EOF â”‚ ${N8N\_DOMAIN} { â”‚ import security-headers â”‚  
â”‚ reverse\_proxy n8n:5678 { â”‚ import proxy-common â”‚ transport http { â”‚ read\_timeout 300s â”‚ } â”‚ \# WebSocket support for n8n â”‚ flush\_interval \-1 â”‚ } â”‚ } â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Open WebUI â”€â”€ â”‚ if service\_enabled "open-webui"; then â”‚ cat \>\> Caddyfile \<\< EOF â”‚ ${OPEN\_WEBUI\_DOMAIN} { â”‚ import security-headers â”‚  
â”‚ reverse\_proxy open-webui:8080 { â”‚ import proxy-common â”‚ transport http { â”‚ read\_timeout 600s â”‚ write\_timeout 600s â”‚ } â”‚ \# SSE support for streaming â”‚ flush\_interval \-1 â”‚ } â”‚ } â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Flowise â”€â”€ â”‚ if service\_enabled "flowise"; then â”‚ cat \>\> Caddyfile \<\< EOF â”‚ ${FLOWISE\_DOMAIN} { â”‚ import security-headers â”‚  
â”‚ reverse\_proxy flowise:3000 { â”‚ import proxy-common â”‚ } â”‚ } â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Grafana â”€â”€ â”‚ if service\_enabled "monitoring"; then â”‚ cat \>\> Caddyfile \<\< EOF â”‚ ${GRAFANA\_DOMAIN} { â”‚ import security-headers â”‚  
â”‚ reverse\_proxy grafana:3000 { â”‚ import proxy-common â”‚ } â”‚ } â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ IP-only mode (no DNS) â”€â”€ â”‚ If SSL\_MODE \== "selfsigned" or no domains configured: â”‚  
â”‚ :443 { â”‚ tls internal â”‚ import security-headers â”‚  
â”‚ \# Route by path prefix â”‚ handle /litellm/\* { â”‚ uri strip\_prefix /litellm â”‚ reverse\_proxy litellm:4000 â”‚ } â”‚  
â”‚ handle /dify/\* { â”‚ uri strip\_prefix /dify â”‚ reverse\_proxy dify-nginx:80 â”‚ } â”‚  
â”‚ handle /n8n/\* { â”‚ uri strip\_prefix /n8n â”‚ reverse\_proxy n8n:5678 â”‚ } â”‚  
â”‚ handle /chat/\* { â”‚ uri strip\_prefix /chat â”‚ reverse\_proxy open-webui:8080 â”‚ } â”‚  
â”‚ handle /flowise/\* { â”‚ uri strip\_prefix /flowise â”‚ reverse\_proxy flowise:3000 â”‚ } â”‚  
â”‚ handle /grafana/\* { â”‚ uri strip\_prefix /grafana â”‚ reverse\_proxy grafana:3000 â”‚ } â”‚  
â”‚ \# Default landing page â”‚ handle { â”‚ respond "AI Platform is running. Services: /litellm /dify /n8n /chat /flowise /grafana" 200 â”‚ } â”‚ } â”‚ â””â”€â”€ echo " âœ“ Caddyfile generated" echo " Mode: (if\[\[âˆ’n" {DOMAIN\_BASE}" \]\]; then echo "subdomain (${DOMAIN\_BASE})"; else echo "IP-only path-based"; fi)"

\---

\#\# 23\. LiteLLM Configuration YAML

generate\_litellm\_config() â”‚ â”œâ”€â”€ File: $CONFIG\_DIR/litellm/litellm\_config.yaml â”‚ â”œâ”€â”€ \# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚ \# LiteLLM Configuration â€” Auto-generated by Script 2 â”‚ \# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚  
â”‚ \# â”€â”€ Model List â”€â”€ â”‚ model\_list: â”‚ â”œâ”€â”€ â”€â”€ Always: Ollama models â”€â”€ â”‚ \# â”€â”€ Local Ollama Models â”€â”€ â”‚ \- model\_name: tinyllama â”‚ litellm\_params: â”‚ model: ollama/tinyllama â”‚ api\_base: [http://host.docker.internal:11434](http://host.docker.internal:11434) â”‚ stream: true â”‚ model\_info: â”‚ mode: chat â”‚ input\_cost\_per\_token: 0 â”‚ output\_cost\_per\_token: 0 â”‚  
â”‚ \# Add each configured Ollama model â”‚ for model in ${OLLAMA\_MODELS\[@\]}; do â”‚ model\_name=$(echo $model | cut \-d: \-f1) â”‚ cat \>\> config \<\< EOF â”‚ \- model\_name: ${model\_name} â”‚ litellm\_params: â”‚ model: ollama/${model} â”‚ api\_base: [http://host.docker.internal:11434](http://host.docker.internal:11434) â”‚ stream: true â”‚ model\_info: â”‚ mode: chat â”‚ input\_cost\_per\_token: 0 â”‚ output\_cost\_per\_token: 0 â”‚ EOF â”‚ done â”‚ â”œâ”€â”€ â”€â”€ Conditional: OpenAI models â”€â”€ â”‚ if \[\[ \-n "${OPENAI\_API\_KEY}" \]\]; then â”‚ cat \>\> config \<\< EOF â”‚ \# â”€â”€ OpenAI Models â”€â”€ â”‚ \- model\_name: gpt-4o â”‚ litellm\_params: â”‚ model: openai/gpt-4o â”‚ api\_key: os.environ/OPENAI\_API\_KEY â”‚  
â”‚ \- model\_name: gpt-4o-mini â”‚ litellm\_params: â”‚ model: openai/gpt-4o-mini â”‚ api\_key: os.environ/OPENAI\_API\_KEY â”‚  
â”‚ \- model\_name: gpt-4-turbo â”‚ litellm\_params: â”‚ model: openai/gpt-4-turbo â”‚ api\_key: os.environ/OPENAI\_API\_KEY â”‚  
â”‚ \- model\_name: gpt-3.5-turbo â”‚ litellm\_params: â”‚ model: openai/gpt-3.5-turbo â”‚ api\_key: os.environ/OPENAI\_API\_KEY â”‚  
â”‚ \- model\_name: text-embedding-3-small â”‚ litellm\_params: â”‚ model: openai/text-embedding-3-small â”‚ api\_key: os.environ/OPENAI\_API\_KEY â”‚ model\_info: â”‚ mode: embedding â”‚  
â”‚ \- model\_name: text-embedding-3-large â”‚ litellm\_params: â”‚ model: openai/text-embedding-3-large â”‚ api\_key: os.environ/OPENAI\_API\_KEY â”‚ model\_info: â”‚ mode: embedding â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Anthropic models â”€â”€ â”‚ if \[\[ \-n "${ANTHROPIC\_API\_KEY}" \]\]; then â”‚ cat \>\> config \<\< EOF â”‚ \# â”€â”€ Anthropic Models â”€â”€ â”‚ \- model\_name: claude-3.5-sonnet â”‚ litellm\_params: â”‚ model: anthropic/claude-3-5-sonnet-20241022 â”‚ api\_key: os.environ/ANTHROPIC\_API\_KEY â”‚  
â”‚ \- model\_name: claude-3-haiku â”‚ litellm\_params: â”‚ model: anthropic/claude-3-haiku-20240307 â”‚ api\_key: os.environ/ANTHROPIC\_API\_KEY â”‚  
â”‚ \- model\_name: claude-3-opus â”‚ litellm\_params: â”‚ model: anthropic/claude-3-opus-20240229 â”‚ api\_key: os.environ/ANTHROPIC\_API\_KEY â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Google models â”€â”€ â”‚ if \[\[ \-n "${GEMINI\_API\_KEY}" \]\]; then â”‚ cat \>\> config \<\< EOF â”‚ \# â”€â”€ Google Gemini Models â”€â”€ â”‚ \- model\_name: gemini-1.5-pro â”‚ litellm\_params: â”‚ model: gemini/gemini-1.5-pro-latest â”‚ api\_key: os.environ/GEMINI\_API\_KEY â”‚  
â”‚ \- model\_name: gemini-1.5-flash â”‚ litellm\_params: â”‚ model: gemini/gemini-1.5-flash-latest â”‚ api\_key: os.environ/GEMINI\_API\_KEY â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Conditional: Groq models â”€â”€ â”‚ if \[\[ \-n "${GROQ\_API\_KEY}" \]\]; then â”‚ cat \>\> config \<\< EOF â”‚ \# â”€â”€ Groq Models (fast inference) â”€â”€ â”‚ \- model\_name: groq-llama3-70b â”‚ litellm\_params: â”‚ model: groq/llama3-70b-8192 â”‚ api\_key: os.environ/GROQ\_API\_KEY â”‚  
â”‚ \- model\_name: groq-mixtral â”‚ litellm\_params: â”‚ model: groq/mixtral-8x7b-32768 â”‚ api\_key: os.environ/GROQ\_API\_KEY â”‚ EOF â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Router Settings â”€â”€ â”‚ cat \>\> config \<\< EOF â”‚  
â”‚ \# â”€â”€ Router Configuration â”€â”€ â”‚ router\_settings: â”‚ routing\_strategy: simple-shuffle â”‚ num\_retries: 3 â”‚ timeout: 120 â”‚ retry\_after: 5 â”‚ allowed\_fails: 3 â”‚ cooldown\_time: 60 â”‚  
â”‚ \# Fallback chains â”‚ fallbacks: â”‚ \- model\_name: mistral â”‚ fallback: gpt-4o-mini â”‚ (if\[\[âˆ’n" {OPENAI\_API\_KEY}" \]\]; then echo " â”‚ \- model\_name: gpt-4o â”‚ fallback: claude-3.5-sonnet"; fi) â”‚  
â”‚ \# â”€â”€ LiteLLM Settings â”€â”€ â”‚ litellm\_settings: â”‚ \# Callbacks â”‚ success\_callback: \["postgres"\] â”‚ failure\_callback: \["postgres"\] â”‚ service\_callback: \["postgres"\] â”‚  
â”‚ \# Caching â”‚ cache: true â”‚ cache\_params: â”‚ type: redis â”‚ host: redis â”‚ port: 6379 â”‚ password: os.environ/REDIS\_PASSWORD â”‚ ttl: 3600 â”‚ namespace: litellm\_cache â”‚  
â”‚ \# Budgets â”‚ max\_budget: ${MONTHLY\_BUDGET\_USD:-500} â”‚ budget\_duration: monthly â”‚  
â”‚ \# Logging â”‚ set\_verbose: false â”‚ json\_logs: true â”‚  
â”‚ \# Rate Limiting â”‚ global\_max\_parallel\_requests: 200 â”‚ max\_request\_size\_mb: 100 â”‚  
â”‚ \# Drop unsupported params silently â”‚ drop\_params: true â”‚  
â”‚ \# â”€â”€ General Settings â”€â”€ â”‚ general\_settings: â”‚ master\_key: os.environ/LITELLM\_MASTER\_KEY â”‚ database\_url: os.environ/DATABASE\_URL â”‚  
â”‚ \# Admin UI â”‚ store\_model\_in\_db: true â”‚  
â”‚ \# Alerting â”‚ alerting: â”‚ \- slack â”‚ alerting\_threshold: 300 \# alert if request takes \> 300s â”‚  
â”‚ EOF â”‚ â””â”€â”€ echo " âœ“ LiteLLM config generated" echo " Models configured: $(grep 'model\_name:' $CONFIG\_DIR/litellm/litellm\_config.yaml | wc \-l)"

\---

\#\# 24\. Monitoring Stack Configuration

\#\#\# Prometheus Configuration

generate\_prometheus\_config() â”‚ â”œâ”€â”€ File: $CONFIG\_DIR/prometheus/prometheus.yml â”‚ â”œâ”€â”€ global: â”‚ scrape\_interval: 15s â”‚ evaluation\_interval: 15s â”‚ scrape\_timeout: 10s â”‚  
â”‚ \# â”€â”€ Alert Rules â”€â”€ â”‚ rule\_files: â”‚ \- "alert\_rules.yml" â”‚  
â”‚ \# â”€â”€ Scrape Configs â”€â”€ â”‚ scrape\_configs: â”‚ \# â”€â”€ Prometheus self â”€â”€ â”‚ \- job\_name: 'prometheus' â”‚ static\_configs: â”‚ \- targets: \['localhost:9090'\] â”‚  
â”‚ \# â”€â”€ Node Exporter (host metrics) â”€â”€ â”‚ \- job\_name: 'node-exporter' â”‚ static\_configs: â”‚ \- targets: \['node-exporter:9100'\] â”‚  
â”‚ \# â”€â”€ cAdvisor (container metrics) â”€â”€ â”‚ \- job\_name: 'cadvisor' â”‚ static\_configs: â”‚ \- targets: \['cadvisor:8080'\] â”‚  
â”‚ \# â”€â”€ LiteLLM metrics â”€â”€ â”‚ \- job\_name: 'litellm' â”‚ metrics\_path: /metrics â”‚ static\_configs: â”‚ \- targets: \['litellm:4000'\] â”‚  
â”‚ \# â”€â”€ PostgreSQL Exporter â”€â”€ â”‚ \- job\_name: 'postgres' â”‚ static\_configs: â”‚ \- targets: \['postgres-exporter:9187'\] â”‚  
â”‚ \# â”€â”€ Redis Exporter â”€â”€ â”‚ \- job\_name: 'redis' â”‚ static\_configs: â”‚ \- targets: \['redis-exporter:9121'\] â”‚  
â”‚ \# â”€â”€ Ollama metrics (if available) â”€â”€ â”‚ \- job\_name: 'ollama' â”‚ static\_configs: â”‚ \- targets: \['host.docker.internal:11434'\] â”‚ metrics\_path: /api/metrics â”‚ scrape\_interval: 30s â”‚  
â”‚ \# â”€â”€ Custom platform metrics (textfile collector) â”€â”€ â”‚ \- job\_name: 'ai-platform-custom' â”‚ static\_configs: â”‚ \- targets: \['node-exporter:9100'\] â”‚ metrics\_path: /metrics â”‚ params: â”‚ collect\[\]: â”‚ \- textfile â”‚ â””â”€â”€ File: $CONFIG\_DIR/prometheus/alert\_rules.yml (Full alert rules as documented in Section 20\)

\#\#\# Grafana Provisioning

generate\_grafana\_provisioning() â”‚ â”œâ”€â”€ Directory structure: â”‚ $CONFIG\_DIR/grafana/ â”‚ â”œâ”€â”€ provisioning/ â”‚ â”‚ â”œâ”€â”€ datasources/ â”‚ â”‚ â”‚ â””â”€â”€ datasources.yml â”‚ â”‚ â””â”€â”€ dashboards/ â”‚ â”‚ â””â”€â”€ dashboards.yml â”‚ â””â”€â”€ dashboards/ â”‚ â”œâ”€â”€ ai-platform-overview.json â”‚ â”œâ”€â”€ litellm-metrics.json â”‚ â”œâ”€â”€ cost-management.json â”‚ â””â”€â”€ infrastructure.json â”‚ â”œâ”€â”€ File: provisioning/datasources/datasources.yml â”‚ apiVersion: 1 â”‚  
â”‚ datasources: â”‚ \- name: Prometheus â”‚ type: prometheus â”‚ access: proxy â”‚ url: [http://prometheus:9090](http://prometheus:9090) â”‚ isDefault: true â”‚ editable: false â”‚  
â”‚ \- name: PostgreSQL â”‚ type: postgres â”‚ access: proxy â”‚ url: postgres:5432 â”‚ database: platform â”‚ user: ${POSTGRES\_USER} â”‚ secureJsonData: â”‚ password: ${POSTGRES\_PASSWORD} â”‚ jsonData: â”‚ sslmode: disable â”‚ maxOpenConns: 5 â”‚ maxIdleConns: 2 â”‚ connMaxLifetime: 14400 â”‚ postgresVersion: 1600 â”‚ timescaledb: false â”‚ editable: false â”‚  
â”‚ \- name: LiteLLM-DB â”‚ type: postgres â”‚ access: proxy â”‚ url: postgres:5432 â”‚ database: litellm â”‚ user: ${POSTGRES\_USER} â”‚ secureJsonData: â”‚ password: ${POSTGRES\_PASSWORD} â”‚ jsonData: â”‚ sslmode: disable â”‚ postgresVersion: 1600 â”‚ editable: false â”‚ â”œâ”€â”€ File: provisioning/dashboards/dashboards.yml â”‚ apiVersion: 1 â”‚  
â”‚ providers: â”‚ \- name: 'AI Platform' â”‚ orgId: 1 â”‚ folder: 'AI Platform' â”‚ type: file â”‚ disableDeletion: false â”‚ editable: true â”‚ updateIntervalSeconds: 30 â”‚ options: â”‚ path: /var/lib/grafana/dashboards â”‚ foldersFromFilesStructure: false â”‚ â”œâ”€â”€ Dashboard: ai-platform-overview.json â”‚ (Auto-generated JSON with panels): â”‚  
â”‚ Row 1: Status Overview â”‚ \- Service health status (up/down per container) â”‚ \- Total requests today â”‚ \- Current spend (month) â”‚ \- Active models count â”‚  
â”‚ Row 2: Request Metrics â”‚ \- Requests per minute (time series) â”‚ \- Latency distribution (histogram) â”‚ \- Error rate (gauge) â”‚ \- Cache hit rate (gauge) â”‚  
â”‚ Row 3: Cost Analytics â”‚ \- Daily spend (bar chart) â”‚ \- Cost by model (pie chart) â”‚ \- Cost by user (table) â”‚ \- Budget gauge (threshold markers) â”‚  
â”‚ Row 4: Model Performance â”‚ \- Tokens per second by model â”‚ \- Time to first token â”‚ \- Model availability % â”‚ \- Queue depth â”‚  
â”‚ Row 5: Infrastructure â”‚ \- CPU usage (host \+ per container) â”‚ \- Memory usage (host \+ per container) â”‚ \- Disk I/O â”‚ \- Network traffic â”‚ \- GPU utilization (if available) â”‚  
â”‚ Row 6: RAG & Documents â”‚ \- Documents indexed count â”‚ \- Last sync timestamp â”‚ \- Embedding generation rate â”‚ \- Vector DB collection sizes â”‚ â””â”€â”€ echo " âœ“ Grafana provisioning generated" echo " Dashboards: 4 auto-provisioned" echo " Datasources: 3 configured"

\#\#\# Dashboard JSON Generation Helper

generate\_dashboard\_json() â”‚ â”œâ”€â”€ Rather than embedding 1000+ line JSON, Script 2 uses a â”‚ template approach with variable substitution: â”‚ â”œâ”€â”€ \# Panel template function â”‚ grafana\_panel() { â”‚ local id=$1 title=$2 type=$3 query=$4 x=$5 y=$6 w=$7 h=$8 â”‚ cat \<\< PANEL â”‚ { â”‚ "id": ${id}, â”‚ "title": "${title}", â”‚ "type": "${type}", â”‚ "gridPos": {"x": ${x}, "y": ${y}, "w": ${w}, "h": ${h}}, â”‚ "datasource": {"type": "prometheus", "uid": "prometheus"}, â”‚ "targets": \[ â”‚ { â”‚ "expr": "${query}", â”‚ "refId": "A" â”‚ } â”‚ \], â”‚ "fieldConfig": { â”‚ "defaults": { â”‚ "color": {"mode": "palette-classic"} â”‚ } â”‚ } â”‚ } â”‚ PANEL â”‚ } â”‚ â”œâ”€â”€ \# Build dashboard â”‚ PANELS="" â”‚ PANELS+= $ (grafana\_panel 1 "Total Requests" "stat"  
 â”‚ 'sum(increase(litellm\_requests\_total\[24h\]))' 0 0 6 4\) â”‚ PANELS+="," â”‚ PANELS+= $ (grafana\_panel 2 "Monthly Spend" "stat"  
 â”‚ 'ai\_platform\_monthly\_spend\_usd' 6 0 6 4\) â”‚ PANELS+="," â”‚ PANELS+= $ (grafana\_panel 3 "Error Rate" "gauge"  
 â”‚ 'rate(litellm\_request\_errors\_total\[5m\])/rate(litellm\_requests\_total\[5m\])\*100' 12 0 6 4\) â”‚ PANELS+="," â”‚ PANELS+= $ (grafana\_panel 4 "Cache Hit Rate" "gauge"  
 â”‚ 'rate(litellm\_cache\_hits\_total\[1h\])/rate(litellm\_requests\_total\[1h\])\*100' 18 0 6 4\) â”‚ \# ... additional panels ... â”‚ â””â”€â”€ \# Wrap in dashboard envelope cat \> $ CONFIG\_DIR/grafana/dashboards/ai-platform-overview.json \<\< EOF { "dashboard": { "id": null, "uid": "ai-platform-overview", "title": "AI Platform â€” Overview", "tags": \["ai-platform", "auto-generated"\], "timezone": "browser", "refresh": "30s", "time": {"from": "now-24h", "to": "now"}, "panels": \[ $ {PANELS}\] }, "overwrite": true } EOF

\---

\*\*End of Part 6 (Sections 21â€“24).\*\*  
\---

\#\# 25\. Backup & Restore System

\#\#\# Backup Script (Generated by Script 2, Phase 14\)

File: $ BASE\_DIR/scripts/backup.sh â”‚ â”œâ”€â”€ \#\!/bin/bash â”‚ set \-euo pipefail â”‚  
â”‚ \# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚ \# AI Platform â€” Backup Script â”‚ \# Usage: ./backup.sh \[full|db|configs|volumes\] \[--upload\] â”‚ \# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚ â”œâ”€â”€ â”€â”€ Configuration â”€â”€ â”‚ source /opt/ai-platform/env/master.env â”‚ BACKUP\_DIR=" $ {BACKUP\_BASE\_DIR:-/opt/ai-platform/backups}" â”‚ TIMESTAMP= $ (date \+%Y%m%d\_%H%M%S) â”‚ BACKUP\_NAME="ai-platform-backup- $ {TIMESTAMP}" â”‚ CURRENT\_BACKUP="${BACKUP\_DIR}/${BACKUP\_NAME}" â”‚ LOG="/var/log/ai-platform/backup.log" â”‚ RETENTION\_DAYS=${BACKUP\_RETENTION\_DAYS:-7} â”‚ BACKUP\_TYPE="${1:-full}" â”‚ UPLOAD\_FLAG="${2:-}" â”‚ â”œâ”€â”€ â”€â”€ Logging â”€â”€ â”‚ log() { â”‚ echo "$(date \-u \+%Y-%m-%dT%H:%M:%SZ) | BACKUP | 1"âˆ£teeâˆ’a" LOG" â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Pre-flight â”€â”€ â”‚ preflight\_check() { â”‚ \# Check disk space (need at least 2x current data size free) â”‚ local data\_size=$(du \-sm {DATA\_DIR} | awk '{print 1}') â”‚ local free\_space= $ (df \-m ${BACKUP\_DIR} | tail \-1 | awk '{print $4}') â”‚  
â”‚ if \[\[ $free\_space \-lt $(( data\_size \* 2 )) \]\]; then â”‚ log "ERROR: Insufficient disk space. Need $(( data\_size \* 2 ))MB, have ${free\_space}MB" â”‚ exit 1 â”‚ fi â”‚  
â”‚ mkdir \-p "${CURRENT\_BACKUP}"/{db,configs,volumes,meta} â”‚ log "Starting ${BACKUP\_TYPE} backup â†’ ${CURRENT\_BACKUP}" â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Database Backup â”€â”€ â”‚ backup\_databases() { â”‚ log "Backing up PostgreSQL databases..." â”‚  
â”‚ local DBS=("aiplatform" "litellm" "dify" "n8n" "supertokens" "grafana" "platform") â”‚  
â”‚ for db in "${DBS\[@\]}"; do â”‚ log " Dumping ${db}..." â”‚ docker exec ai-postgres pg\_dump  
 â”‚ \-U ${POSTGRES\_USER}  
 â”‚ \-d ${db}  
 â”‚ \--format=custom  
 â”‚ \--compress=6  
 â”‚ \--verbose  
 â”‚ 2\>\>" $ LOG"  
 â”‚ \> " $ {CURRENT\_BACKUP}/db/${db}.dump" || { â”‚ log " WARNING: Failed to dump ${db} (may not exist yet)" â”‚ continue â”‚ } â”‚  
â”‚ local size= (duâˆ’sh" {CURRENT\_BACKUP}/db/${db}.dump" | awk '{print $1}') â”‚ log " âœ“ ${db}: ${size}" â”‚ done â”‚  
â”‚ \# Also take a full cluster dump as safety net â”‚ log " Creating full cluster dump..." â”‚ docker exec ai-postgres pg\_dumpall  
 â”‚ \-U ${POSTGRES\_USER}  
 â”‚ \--clean  
 â”‚ 2\>\>" $ LOG"  
 â”‚ | gzip \> " $ {CURRENT\_BACKUP}/db/full\_cluster.sql.gz" â”‚  
â”‚ log "âœ“ Database backup complete" â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Redis Backup â”€â”€ â”‚ backup\_redis() { â”‚ log "Backing up Redis..." â”‚  
â”‚ \# Trigger synchronous save â”‚ docker exec ai-redis redis-cli  
 â”‚ \-a "${REDIS\_PASSWORD}"  
 â”‚ \--no-auth-warning  
 â”‚ BGSAVE â”‚  
â”‚ \# Wait for save to complete â”‚ local timeout=60 â”‚ while \[\[ $ timeout \-gt 0 \]\]; do â”‚ local status= $ (docker exec ai-redis redis-cli  
 â”‚ \-a "${REDIS\_PASSWORD}"  
 â”‚ \--no-auth-warning  
 â”‚ LASTSAVE) â”‚ sleep 2 â”‚ local new\_status= $ (docker exec ai-redis redis-cli  
 â”‚ \-a " $ {REDIS\_PASSWORD}"  
 â”‚ \--no-auth-warning  
 â”‚ LASTSAVE) â”‚ if \[\[ " status"\!=" new\_status" \]\] || \[\[ $ timeout \-lt 55 \]\]; then â”‚ break â”‚ fi â”‚ timeout= $ ((timeout \- 2)) â”‚ done â”‚  
â”‚ cp "${DATA\_DIR}/redis/dump.rdb" "${CURRENT\_BACKUP}/db/redis-dump.rdb" â”‚  
â”‚ \# Also backup AOF if exists â”‚ if \[\[ \-f "${DATA\_DIR}/redis/appendonly.aof" \]\]; then â”‚ cp "${DATA\_DIR}/redis/appendonly.aof" "${CURRENT\_BACKUP}/db/redis-appendonly.aof" â”‚ fi â”‚  
â”‚ log "âœ“ Redis backup complete" â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Configuration Backup â”€â”€ â”‚ backup\_configs() { â”‚ log "Backing up configurations..." â”‚  
â”‚ \# All config files â”‚ tar czf "${CURRENT\_BACKUP}/configs/config-files.tar.gz"  
 â”‚ \-C /  
 â”‚ opt/ai-platform/config  
 â”‚ opt/ai-platform/env  
 â”‚ opt/ai-platform/compose  
 â”‚ opt/ai-platform/scripts  
 â”‚ 2\>\>" $ LOG" â”‚  
â”‚ \# Docker compose files â”‚ cp " $ {COMPOSE\_DIR}/docker-compose.yml" "${CURRENT\_BACKUP}/configs/" â”‚  
â”‚ \# Crontabs â”‚ crontab \-l \> "${CURRENT\_BACKUP}/configs/crontab.bak" 2\>/dev/null || true â”‚  
â”‚ \# Systemd services â”‚ cp /etc/systemd/system/ollama.service "${CURRENT\_BACKUP}/configs/" 2\>/dev/null || true â”‚  
â”‚ \# UFW rules â”‚ if command \-v ufw &\>/dev/null; then â”‚ ufw status verbose \> "${CURRENT\_BACKUP}/configs/ufw-rules.txt" â”‚ fi â”‚  
â”‚ \# Installed packages list â”‚ dpkg \--get-selections \> "${CURRENT\_BACKUP}/configs/dpkg-selections.txt" â”‚  
â”‚ log "âœ“ Configuration backup complete" â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Volume Data Backup â”€â”€ â”‚ backup\_volumes() { â”‚ log "Backing up volume data..." â”‚  
â”‚ local VOLUMES=( â”‚ "dify/storage" â”‚ "n8n" â”‚ "open-webui" â”‚ "flowise" â”‚ "caddy" â”‚ ) â”‚  
â”‚ for vol in "${VOLUMES\[@\]}"; do â”‚ if \[\[ \-d "${DATA\_DIR}/${vol}" \]\]; then â”‚ log " Archiving ${vol}..." â”‚ tar czf "${CURRENT\_BACKUP}/volumes/${vol////*}.tar.gz"*  
 *â”‚ \-C "${DATA\_DIR}"*  
 *â”‚ "${vol}"*  
 *â”‚ 2\>\>" $ LOG" â”‚ local size= $ (du \-sh "${CURRENT\_BACKUP}/volumes/${vol////*}.tar.gz" | awk '{print $1}') â”‚ log " âœ“ ${vol}: ${size}" â”‚ fi â”‚ done â”‚  
â”‚ \# Vector DB data (can be large â€” optional) â”‚ if \[\[ \-d "${DATA\_DIR}/qdrant" \]\]; then â”‚ log " Archiving Qdrant data..." â”‚ \# Use Qdrant snapshot API instead of raw files â”‚ curl \-s \-X POST "[http://localhost:6333/snapshots](http://localhost:6333/snapshots)"  
 â”‚ \-H "api-key: ${QDRANT\_API\_KEY}"  
 â”‚ \> "${CURRENT\_BACKUP}/volumes/qdrant-snapshot-info.json" â”‚  
â”‚ \# Copy snapshot file â”‚ local snap\_name= (jqâˆ’râ€².result.nameâ€²" {CURRENT\_BACKUP}/volumes/qdrant-snapshot-info.json") â”‚ if \[\[ \-n " snap\_name" \]\] && \[\[ " snap\_name" \!= "null" \]\]; then â”‚ curl \-s "[http://localhost:6333/snapshots/${snap\_name}](http://localhost:6333/snapshots/${snap_name})"  
 â”‚ \-H "api-key: ${QDRANT\_API\_KEY}"  
 â”‚ \-o "${CURRENT\_BACKUP}/volumes/qdrant-snapshot.tar" â”‚ log " âœ“ Qdrant snapshot: ${snap\_name}" â”‚ fi â”‚ fi â”‚  
â”‚ log "âœ“ Volume backup complete" â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Metadata â”€â”€ â”‚ save\_metadata() { â”‚ log "Saving backup metadata..." â”‚  
 â”‚ cat \> "${CURRENT\_BACKUP}/meta/backup-info.json" \<\< EOF â”‚ { â”‚ "timestamp": " $ (date \-u \+%Y-%m-%dT%H:%M:%SZ)", â”‚ "hostname": " $ (hostname)", â”‚ "backup\_type": "${BACKUP\_TYPE}", â”‚ "backup\_name": "${BACKUP\_NAME}", â”‚ "platform\_version": "$(grep SCRIPT\_2\_VERSION ${ENV\_DIR}/master.env | cut \-d= \-f2)", â”‚ "docker\_compose\_hash": "$(md5sum ${COMPOSE\_DIR}/docker-compose.yml | awk '{print $1}')", â”‚ "services\_running": $(docker ps \--format '{{.Names}}' | jq \-R \-s 'split("\\n") | map(select(. \!= ""))'), â”‚ "database\_sizes": { â”‚ $(docker exec ai-postgres psql \-U ${POSTGRES\_USER} \-d aiplatform \-t \-c  
 â”‚ "SELECT json\_object\_agg(datname, pg\_size\_pretty(pg\_database\_size(datname))) â”‚ FROM pg\_database WHERE datistemplate \= false;" 2\>/dev/null || echo '"error": "unavailable"') â”‚ }, â”‚ "disk\_usage": { â”‚ "data\_dir": "$(du \-sh {DATA\_DIR} | awk '{print 1}')", â”‚ "backup\_size": " (du \-sh ${CURRENT\_BACKUP} | awk '{print 1}')" â”‚ } â”‚ } â”‚ EOF â”‚  
â”‚ \# Docker image versions â”‚ docker ps \--format '{{.Image}}' | sort \-u \> " $ {CURRENT\_BACKUP}/meta/docker-images.txt" â”‚  
â”‚ \# Environment snapshot (redacted) â”‚ grep \-v \-E '(PASSWORD|KEY|SECRET|TOKEN)' ${ENV\_DIR}/master.env  
 â”‚ \> "${CURRENT\_BACKUP}/meta/env-redacted.txt" â”‚  
â”‚ log "âœ“ Metadata saved" â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Create Archive â”€â”€ â”‚ create\_archive() { â”‚ log "Creating compressed archive..." â”‚ â”‚ cd "${BACKUP\_DIR}" â”‚ tar czf "${BACKUP\_NAME}.tar.gz" "${BACKUP\_NAME}/" â”‚  
â”‚ \# Generate checksum â”‚ sha256sum "${BACKUP\_NAME}.tar.gz" \> "${BACKUP\_NAME}.tar.gz.sha256" â”‚  
â”‚ local final\_size= (duâˆ’sh" {BACKUP\_NAME}.tar.gz" | awk '{print $1}') â”‚ log "âœ“ Archive created: ${BACKUP\_NAME}.tar.gz (${final\_size})" â”‚  
â”‚ \# Cleanup temp directory â”‚ rm \-rf "${CURRENT\_BACKUP}" â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Upload to Google Drive â”€â”€ â”‚ upload\_backup() { â”‚ if \[\[ "${UPLOAD\_FLAG}" \== "--upload" \]\] && command \-v rclone &\>/dev/null; then â”‚ log "Uploading to Google Drive..." â”‚  
â”‚ rclone copy  
 â”‚ "${BACKUP\_DIR}/${BACKUP\_NAME}.tar.gz"  
 â”‚ "gdrive:AI-Platform-Backups/"  
 â”‚ \--progress  
 â”‚ \--transfers 1  
 â”‚ 2\>\>" $ LOG" â”‚  
â”‚ rclone copy  
 â”‚ " $ {BACKUP\_DIR}/${BACKUP\_NAME}.tar.gz.sha256"  
 â”‚ "gdrive:AI-Platform-Backups/"  
 â”‚ 2\>\>"$LOG" â”‚  
â”‚ log "âœ“ Uploaded to Google Drive" â”‚ fi â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Rotation â”€â”€ â”‚ rotate\_backups() { â”‚ log "Rotating old backups (keeping ${RETENTION\_DAYS} days)..." â”‚  
â”‚ \# Local rotation â”‚ find "${BACKUP\_DIR}" \-name "ai-platform-backup-*.tar.gz"*  
 *â”‚ \-mtime \+${RETENTION\_DAYS} \-delete \-print | while read f; do â”‚ log " Deleted: (basename f)" â”‚ done â”‚*  
*â”‚ find " $ {BACKUP\_DIR}" \-name "*.sha256" \-mtime \+${RETENTION\_DAYS} \-delete â”‚  
â”‚ \# Remote rotation (keep 30 days on GDrive) â”‚ if command \-v rclone &\>/dev/null; then â”‚ rclone delete "gdrive:AI-Platform-Backups/"  
 â”‚ \--min-age 30d  
 â”‚ 2\>\>" $ LOG" || true â”‚ fi â”‚  
â”‚ local remaining= $ (ls \-1 ${BACKUP\_DIR}/ai-platform-backup-\*.tar.gz 2\>/dev/null | wc \-l) â”‚ log "âœ“ Rotation complete. ${remaining} backups retained locally" â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Main â”€â”€ â”‚ main() { â”‚ preflight\_check â”‚  
â”‚ case "${BACKUP\_TYPE}" in â”‚ full) â”‚ backup\_databases â”‚ backup\_redis â”‚ backup\_configs â”‚ backup\_volumes â”‚ ;; â”‚ db) â”‚ backup\_databases â”‚ backup\_redis â”‚ ;; â”‚ configs) â”‚ backup\_configs â”‚ ;; â”‚ volumes) â”‚ backup\_volumes â”‚ ;; â”‚ \*) â”‚ echo "Usage: $0 \[full|db|configs|volumes\] \[--upload\]" â”‚ exit 1 â”‚ ;; â”‚ esac â”‚  
â”‚ save\_metadata â”‚ create\_archive â”‚ upload\_backup â”‚ rotate\_backups â”‚  
â”‚ log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" â”‚ log "BACKUP COMPLETE: ${BACKUP\_NAME}.tar.gz" â”‚ log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" â”‚ } â”‚  
â”‚ main "$@" â”‚ â””â”€â”€ Crontab entry (installed by Script 2): 0 2 \* \* \* /opt/ai-platform/scripts/backup.sh full \--upload \>\> /var/log/ai-platform/backup-cron.log 2\>&1

\#\#\# Restore Script

File: $ BASE\_DIR/scripts/restore.sh â”‚ â”œâ”€â”€ \#\!/bin/bash â”‚ set \-euo pipefail â”‚  
â”‚ \# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚ \# AI Platform â€” Restore Script â”‚ \# Usage: ./restore.sh \<backup-archive.tar.gz\> \[--confirm\] â”‚ \# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚ â”œâ”€â”€ ARCHIVE=" {1:?Usage: 0 \<backup.tar.gz\> \[--confirm\]}" â”‚ CONFIRM=" $ {2:-}" â”‚ RESTORE\_DIR="/tmp/ai-platform-restore- $ (date \+%s)" â”‚ LOG="/var/log/ai-platform/restore.log" â”‚ â”œâ”€â”€ â”€â”€ Validation â”€â”€ â”‚ validate() { â”‚ \[\[ \-f " ARCHIVE" \]\] || { echo "File not found: ARCHIVE"; exit 1; } â”‚  
â”‚ \# Verify checksum if available â”‚ if \[\[ \-f " $ {ARCHIVE}.sha256" \]\]; then â”‚ echo "Verifying checksum..." â”‚ sha256sum \-c "${ARCHIVE}.sha256" || { echo "CHECKSUM FAILED"; exit 1; } â”‚ echo " âœ“ Checksum verified" â”‚ fi â”‚  
â”‚ \# Extract and check metadata â”‚ mkdir \-p " $ RESTORE\_DIR" â”‚ tar xzf " ARCHIVE"âˆ’C" RESTORE\_DIR" â”‚  
â”‚ local backup\_dir= (ls" RESTORE\_DIR") â”‚ local meta\_file=" $ {RESTORE\_DIR}/${backup\_dir}/meta/backup-info.json" â”‚  
â”‚ if \[\[ \-f "$meta\_file" \]\]; then â”‚ echo "â•â•â• Backup Info â•â•â•" â”‚ echo "Date: $(jq \-r '.timestamp' $meta\_file)" â”‚ echo "Type: $(jq \-r '.backup\_type' $meta\_file)" â”‚ echo "Host: $(jq \-r '.hostname' $meta\_file)" â”‚ echo "Version: $(jq \-r '.platform\_version' $meta\_file)" â”‚ echo "Size: (jqâˆ’râ€².diskuâ€‹sage.backupsâ€‹izeâ€² meta\_file)" â”‚ echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" â”‚ fi â”‚  
â”‚ if \[\[ " $ CONFIRM" \!= "--confirm" \]\]; then â”‚ echo "" â”‚ echo "âš  WARNING: This will overwrite current data\!" â”‚ echo " Run with \--confirm to proceed" â”‚ echo " Recommended: take a backup first with ./backup.sh full" â”‚ rm \-rf "$RESTORE\_DIR" â”‚ exit 0 â”‚ fi â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Stop Services â”€â”€ â”‚ stop\_services() { â”‚ echo "Stopping services..." â”‚ cd ${COMPOSE\_DIR} â”‚ docker compose down \--timeout 30 â”‚ echo " âœ“ Services stopped" â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Restore Databases â”€â”€ â”‚ restore\_databases() { â”‚ local backup\_path="$1" â”‚ echo "Restoring databases..." â”‚  
â”‚ \# Start only postgres â”‚ docker compose up \-d postgres â”‚ sleep 10 \# Wait for PG to be ready â”‚  
â”‚ \# Wait for health â”‚ local retries=30 â”‚ until docker exec ai-postgres pg\_isready \-U POSTGRESUâ€‹SERâˆ£âˆ£\[\[ retries \-eq 0 \]\]; do â”‚ sleep 2 â”‚ retries= $ ((retries \- 1)) â”‚ done â”‚  
â”‚ \# Restore each database â”‚ for dump\_file in ${backup\_path}/db/*.dump; do â”‚ local db\_name= (basename" dump\_file" .dump) â”‚ echo " Restoring ${db\_name}..." â”‚*  
*â”‚ \# Drop and recreate â”‚ docker exec ai-postgres psql \-U ${POSTGRES\_USER} \-c*  
 *â”‚ "SELECT pg\_terminate\_backend(pid) FROM pg\_stat\_activity WHERE datname='${db\_name}';"*  
 *â”‚ 2\>/dev/null || true â”‚ docker exec ai-postgres psql \-U ${POSTGRES\_USER} \-c*  
 *â”‚ "DROP DATABASE IF EXISTS ${db\_name};" 2\>/dev/null || true â”‚ docker exec ai-postgres psql \-U ${POSTGRES\_USER} \-c*  
 *â”‚ "CREATE DATABASE ${db\_name} OWNER ${POSTGRES\_USER};" â”‚*  
*â”‚ \# Restore from dump â”‚ cat "$dump\_file" | docker exec \-i ai-postgres pg\_restore*  
 *â”‚ \-U ${POSTGRES\_USER}*  
 *â”‚ \-d ${db\_name}*  
 *â”‚ \--no-owner*  
 *â”‚ \--no-privileges*  
 *â”‚ \--clean*  
 *â”‚ \--if-exists*  
 *â”‚ 2\>\>"$LOG" || { â”‚ echo " âš  Warning: Some errors during ${db\_name} restore (may be normal)" â”‚ } â”‚*  
*â”‚ echo " âœ“ ${db\_name} restored" â”‚ done â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Restore Redis â”€â”€ â”‚ restore\_redis() { â”‚ local backup\_path=" $ 1" â”‚ echo "Restoring Redis..." â”‚*  
*â”‚ if \[\[ \-f " $ {backup\_path}/db/redis-dump.rdb" \]\]; then â”‚ cp "${backup\_path}/db/redis-dump.rdb" "${DATA\_DIR}/redis/dump.rdb" â”‚ echo " âœ“ Redis data restored" â”‚ fi â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Restore Configs â”€â”€ â”‚ restore\_configs() { â”‚ local backup\_path=" $ 1" â”‚ echo "Restoring configurations..." â”‚*  
*â”‚ if \[\[ \-f " $ {backup\_path}/configs/config-files.tar.gz" \]\]; then â”‚ \# Backup current configs first â”‚ cp \-r /opt/ai-platform/config /opt/ai-platform/config.pre-restore.bak â”‚*  
*â”‚ tar xzf "${backup\_path}/configs/config-files.tar.gz" \-C / â”‚ echo " âœ“ Configuration files restored" â”‚ fi â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Restore Volumes â”€â”€ â”‚ restore\_volumes() { â”‚ local backup\_path="$1" â”‚ echo "Restoring volume data..." â”‚*  
*â”‚ for archive in ${backup\_path}/volumes/*.tar.gz; do â”‚ local name= (basename" archive" .tar.gz) â”‚ echo " Extracting ${name}..." â”‚ tar xzf " archive"âˆ’C" {DATA\_DIR}/" â”‚ echo " âœ“ ${name}" â”‚ done â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Main â”€â”€ â”‚ main() { â”‚ validate â”‚  
â”‚ local backup\_path="${RESTORE\_DIR}/$(ls ${RESTORE\_DIR})" â”‚  
â”‚ echo "" â”‚ echo "Starting restore..." â”‚  
â”‚ stop\_services â”‚  
â”‚ \[\[ \-d "${backup\_path}/db" \]\] && restore\_databases " $ backup\_path" â”‚ \[\[ \-f " {backup\_path}/db/redis-dump.rdb" \]\] && restore\_redis " backup\_path" â”‚ \[\[ \-d " {backup\_path}/configs" \]\] && restore\_configs " backup\_path" â”‚ \[\[ \-d " $ {backup\_path}/volumes" \]\] && restore\_volumes "$backup\_path" â”‚  
â”‚ \# Start all services â”‚ echo "Starting all services..." â”‚ cd ${COMPOSE\_DIR} â”‚ docker compose up \-d â”‚  
â”‚ echo "" â”‚ echo "Waiting for services to start..." â”‚ sleep 30 â”‚  
â”‚ \# Run health checks â”‚ /opt/ai-platform/scripts/ai-status.sh â”‚  
â”‚ \# Cleanup â”‚ rm \-rf " $ RESTORE\_DIR" â”‚  
â”‚ echo "" â”‚ echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" â”‚ echo "RESTORE COMPLETE" â”‚ echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" â”‚ } â”‚  
â”‚ main " $ @"

\---

\#\# 26\. Convenience Scripts

\#\#\# ai-status

File: $BASE\_DIR/scripts/ai-status.sh Symlink: /usr/local/bin/ai-status â”‚ â”œâ”€â”€ \#\!/bin/bash â”‚ source /opt/ai-platform/env/master.env 2\>/dev/null || true â”‚ â”œâ”€â”€ echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" â”‚ echo " AI PLATFORM STATUS â€” $ (date)" â”‚ echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" â”‚ â”œâ”€â”€ â”€â”€ Docker Services â”€â”€ â”‚ echo "" â”‚ echo "â”€â”€ Docker Services â”€â”€" â”‚ printf "%-25s %-12s %-10s %s\\n" "CONTAINER" "STATUS" "HEALTH" "UPTIME" â”‚ printf "%-25s %-12s %-10s %s\\n" "â”€â”€â”€â”€â”€â”€â”€â”€â”€" "â”€â”€â”€â”€â”€â”€" "â”€â”€â”€â”€â”€â”€" "â”€â”€â”€â”€â”€â”€" â”‚  
â”‚ docker ps \-a \--filter "name=ai-"  
 â”‚ \--format '{{.Names}}\\t{{.Status}}' |  
 â”‚ while IFS= $ '\\t' read name status; do â”‚ \# Parse health â”‚ health="â€”" â”‚ if echo " $ status" | grep \-q "healthy"; then health="âœ“ healthy" â”‚ elif echo " $ status" | grep \-q "unhealthy"; then health="âœ— unhealthy" â”‚ elif echo " $ status" | grep \-q "starting"; then health="â—Œ starting" â”‚ fi â”‚  
â”‚ \# Parse uptime â”‚ uptime= (echo" status" | grep \-oP 'Up \\K.*' | sed 's/ (.*)//') â”‚  
â”‚ \# Color code â”‚ if echo " $ status" | grep \-q "Up"; then â”‚ state="running" â”‚ else â”‚ state="stopped" â”‚ fi â”‚  
â”‚ printf "%-25s %-12s %-10s %s\\n" " name"" state" " health"" uptime" â”‚ done â”‚ â”œâ”€â”€ â”€â”€ Resource Usage â”€â”€ â”‚ echo "" â”‚ echo "â”€â”€ Resource Usage â”€â”€" â”‚ printf "%-25s %-10s %-10s %-10s %s\\n" "CONTAINER" "CPU %" "MEM" "MEM %" "NET I/O" â”‚ printf "%-25s %-10s %-10s %-10s %s\\n" "â”€â”€â”€â”€â”€â”€â”€â”€â”€" "â”€â”€â”€â”€â”€" "â”€â”€â”€" "â”€â”€â”€â”€â”€" "â”€â”€â”€â”€â”€â”€â”€" â”‚  
â”‚ docker stats \--no-stream \--format  
 â”‚ '{{.Name}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.MemPerc}}\\t{{.NetIO}}'  
 â”‚ $ (docker ps \--filter "name=ai-" \-q) 2\>/dev/null |  
 â”‚ while IFS= $ '\\t' read name cpu mem memperc net; do â”‚ printf "%-25s %-10s %-10s %-10s %s\\n" " name"" cpu" " mem"" memperc" "$net" â”‚ done â”‚ â”œâ”€â”€ â”€â”€ Ollama â”€â”€ â”‚ echo "" â”‚ echo "â”€â”€ Ollama Models â”€â”€" â”‚ if command \-v ollama &\>/dev/null && systemctl is-active ollama &\>/dev/null; then â”‚ ollama list 2\>/dev/null || echo " (unavailable)" â”‚ else â”‚ echo " Ollama not running" â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ GPU Status â”€â”€ â”‚ if command \-v nvidia-smi &\>/dev/null; then â”‚ echo "" â”‚ echo "â”€â”€ GPU Status â”€â”€" â”‚ nvidia-smi \--query-gpu=name,temperature.gpu,utilization.gpu,utilization.memory,memory.used,memory.total  
 â”‚ \--format=csv,noheader,nounits 2\>/dev/null |  
 â”‚ while IFS=, read name temp gpu\_util mem\_util mem\_used mem\_total; do â”‚ echo " ${name}: ${temp}Â°C | GPU: ${gpu\_util}% | VRAM: ${mem\_used}/${mem\_total} MiB (${mem\_util}%)" â”‚ done â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Endpoints â”€â”€ â”‚ echo "" â”‚ echo "â”€â”€ Service Endpoints â”€â”€" â”‚  
â”‚ check\_endpoint() { â”‚ local name=$1 url= $ 2 â”‚ local code= (curlâˆ’sâˆ’o/dev/nullâˆ’w" url" 2\>/dev/null) â”‚ if \[\[ " code"= (200âˆ£301âˆ£302âˆ£307âˆ£308) \]\]; then â”‚ printf " âœ“ %-15s %s (HTTP %s)\\n" " name"" url" " $ code" â”‚ else â”‚ printf " âœ— %-15s %s (HTTP %s)\\n" " name"" url" " $ code" â”‚ fi â”‚ } â”‚  
â”‚ check\_endpoint "LiteLLM" "[http://localhost:4000/health](http://localhost:4000/health)" â”‚ \[\[ \-n "${DIFY\_DOMAIN:-}" \]\] && check\_endpoint "Dify" "[http://localhost:5001/health](http://localhost:5001/health)" â”‚ \[\[ \-n "${N8N\_DOMAIN:-}" \]\] && check\_endpoint "n8n" "[http://localhost:5678/healthz](http://localhost:5678/healthz)" â”‚ \[\[ \-n "${OPEN\_WEBUI\_DOMAIN:-}" \]\] && check\_endpoint "Open WebUI" "[http://localhost:3000/](http://localhost:3000/)" â”‚ check\_endpoint "Caddy" "[http://localhost:80](http://localhost:80)" â”‚ â”œâ”€â”€ â”€â”€ Disk Usage â”€â”€ â”‚ echo "" â”‚ echo "â”€â”€ Disk Usage â”€â”€" â”‚ echo " Data directory: $(du \-sh ${DATA\_DIR:-/opt/ai-platform/data} 2\>/dev/null | awk '{print $1}')" â”‚ echo " Docker images: $(docker system df \--format '{{.Size}}' 2\>/dev/null | head \-1)" â”‚ echo " Docker volumes: $(docker system df \--format '{{.Size}}' 2\>/dev/null | tail \-1)" â”‚ echo " System free: (df \-h / | tail \-1 | awk '{print 4}')" â”‚ â”œâ”€â”€ â”€â”€ Cost Summary â”€â”€ â”‚ echo "" â”‚ echo "â”€â”€ Cost Summary (this month) â”€â”€" â”‚ \# Query LiteLLM for spend â”‚ local spend= $ (curl \-s "[http://localhost:4000/global/spend/report](http://localhost:4000/global/spend/report)"  
 â”‚ \-H "Authorization: Bearer ${LITELLM\_MASTER\_KEY:-}" 2\>/dev/null |  
 â”‚ jq \-r '.total\_spend // "unavailable"' 2\>/dev/null || echo "unavailable") â”‚ echo " API spend: ${spend}" â”‚ echo " Budget: ${MONTHLY\_BUDGET\_USD:-not set}" â”‚ â””â”€â”€ echo "" echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

\#\#\# ai-logs

File: $ BASE\_DIR/scripts/ai-logs.sh Symlink: /usr/local/bin/ai-logs â”‚ â”œâ”€â”€ \#\!/bin/bash â”‚ \# Usage: ai-logs \[service\] \[--follow|-f\] \[--lines|-n NUM\] â”‚  
â”‚ SERVICE=" $ {1:-all}" â”‚ FOLLOW="" â”‚ LINES="100" â”‚  
â”‚ shift || true â”‚ while \[\[ $ \# \-gt 0 \]\]; do â”‚ case " $ 1" in â”‚ \-f|--follow) FOLLOW="--follow" ;; â”‚ \-n|--lines) LINES=" $ 2"; shift ;; â”‚ esac â”‚ shift â”‚ done â”‚ â”œâ”€â”€ case " $ SERVICE" in â”‚ all) â”‚ docker compose \-f /opt/ai-platform/compose/docker-compose.yml  
 â”‚ logs \--tail="$LINES" $ FOLLOW â”‚ ;; â”‚ litellm|dify|n8n|postgres|redis|caddy|grafana|prometheus) â”‚ docker logs ai- {SERVICE} \--tail="$LINES" FOLLOW 2\>&1 â”‚ ;; â”‚ dify-api|dify-worker|dify-web) â”‚ docker logs ai- {SERVICE} \--tail="$LINES" FOLLOW 2\>&1 â”‚ ;; â”‚ ollama) â”‚ journalctl \-u ollama \--no-pager \-n " LINES" ( \[\[ \-n " $ FOLLOW" \]\] && echo "-f" ) â”‚ ;; â”‚ backup) â”‚ tail (\[\[âˆ’n" FOLLOW" \]\] && echo "-f" ) \-n "$LINES" /var/log/ai-platform/backup.log â”‚ ;; â”‚ platform) â”‚ tail (\[\[âˆ’n" FOLLOW" \]\] && echo "-f" ) \-n "$LINES" /var/log/ai-platform/\*.log â”‚ ;; â”‚ \*) â”‚ echo "Usage: ai-logs \[all|litellm|dify|n8n|postgres|redis|caddy|grafana|prometheus|ollama|backup|platform\] \[-f\] \[-n NUM\]" â”‚ exit 1 â”‚ ;; â”‚ esac

\#\#\# ai-backup

File: $ BASE\_DIR/scripts/ai-backup.sh Symlink: /usr/local/bin/ai-backup â”‚ â”œâ”€â”€ \#\!/bin/bash â”‚ \# Wrapper for backup script â”‚ exec /opt/ai-platform/scripts/backup.sh " $ @"

\#\#\# ai-update

File: $BASE\_DIR/scripts/ai-update.sh Symlink: /usr/local/bin/ai-update â”‚ â”œâ”€â”€ \#\!/bin/bash â”‚ set \-euo pipefail â”‚  
â”‚ echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" â”‚ echo " AI Platform â€” Update" â”‚ echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" â”‚ â”œâ”€â”€ â”€â”€ Pre-update backup â”€â”€ â”‚ echo "Step 1: Creating pre-update backup..." â”‚ /opt/ai-platform/scripts/backup.sh full â”‚ â”œâ”€â”€ â”€â”€ Pull new images â”€â”€ â”‚ echo "Step 2: Pulling latest images..." â”‚ cd /opt/ai-platform/compose â”‚ docker compose pull â”‚ â”œâ”€â”€ â”€â”€ Rolling restart â”€â”€ â”‚ echo "Step 3: Restarting services..." â”‚  
â”‚ \# Core infra first (quick restart) â”‚ docker compose up \-d postgres redis â”‚ sleep 10 â”‚  
â”‚ \# Main services â”‚ docker compose up \-d litellm â”‚ sleep 10 â”‚  
â”‚ \# Everything else â”‚ docker compose up \-d â”‚  
â”‚ echo "Step 4: Waiting for health checks..." â”‚ sleep 30 â”‚ â”œâ”€â”€ â”€â”€ Update Ollama models â”€â”€ â”‚ echo "Step 5: Updating Ollama models..." â”‚ source /opt/ai-platform/env/master.env â”‚ for model in $(ollama list 2\>/dev/null | tail \-n \+2 | awk '{print $1}'); do â”‚ echo " Updating ${model}..." â”‚ ollama pull "${model}" 2\>/dev/null || true â”‚ done â”‚ â”œâ”€â”€ â”€â”€ Verify â”€â”€ â”‚ echo "Step 6: Verifying..." â”‚ /opt/ai-platform/scripts/ai-status.sh â”‚ â””â”€â”€ echo "" echo "â•â•â• Update complete â•â•â•" echo "Previous images can be cleaned with: docker image prune \-f"

\#\#\# ai-models

File: $ BASE\_DIR/scripts/ai-models.sh Symlink: /usr/local/bin/ai-models â”‚ â”œâ”€â”€ \#\!/bin/bash â”‚ \# Manage Ollama models and LiteLLM model list â”‚  
â”‚ ACTION=" $ {1:-list}" â”‚ MODEL="${2:-}" â”‚ source /opt/ai-platform/env/master.env 2\>/dev/null || true â”‚ â”œâ”€â”€ case "$ACTION" in â”‚ list) â”‚ echo "â”€â”€ Local Ollama Models â”€â”€" â”‚ ollama list 2\>/dev/null || echo "Ollama not running" â”‚  
â”‚ echo "" â”‚ echo "â”€â”€ LiteLLM Registered Models â”€â”€" â”‚ curl \-s "[http://localhost:4000/model/info](http://localhost:4000/model/info)"  
 â”‚ \-H "Authorization: Bearer ${LITELLM\_MASTER\_KEY}" 2\>/dev/null |  
 â”‚ jq \-r '.data\[\] | "(.model\_name)\\t(.litellm\_params.model)\\t(.model\_info.mode // "chat")"' 2\>/dev/null |  
 â”‚ column \-t \-s $ '\\t' || echo "LiteLLM not available" â”‚ ;; â”‚  
â”‚ pull) â”‚ \[\[ \-z " $ MODEL" \]\] && { echo "Usage: ai-models pull \<model-name\>"; exit 1; } â”‚ echo "Pulling ${MODEL} via Ollama..." â”‚ ollama pull " $ MODEL" â”‚ echo "âœ“ Model available. Add to LiteLLM via admin UI or config." â”‚ ;; â”‚  
â”‚ remove) â”‚ \[\[ \-z " $ MODEL" \]\] && { echo "Usage: ai-models remove \<model-name\>"; exit 1; } â”‚ echo "Removing ${MODEL}..." â”‚ ollama rm " $ MODEL" â”‚ ;; â”‚  
â”‚ test) â”‚ MODEL=" $ {MODEL:-mistral}" â”‚ echo "Testing ${MODEL} via LiteLLM..." â”‚ curl \-s "[http://localhost:4000/chat/completions](http://localhost:4000/chat/completions)"  
 â”‚ \-H "Authorization: Bearer ${LITELLM\_MASTER\_KEY}"  
 â”‚ \-H "Content-Type: application/json"  
 â”‚ \-d "{ â”‚ "model": "${MODEL}", â”‚ "messages": \[{"role": "user", "content": "Say hello in exactly 5 words."}\], â”‚ "max\_tokens": 50 â”‚ }" | jq '.' 2\>/dev/null || echo "Request failed" â”‚ ;; â”‚  
â”‚ \*) â”‚ echo "Usage: ai-models \[list|pull|remove|test\] \[model-name\]" â”‚ ;; â”‚ esac

\---

\#\# 27\. n8n Workflow Templates

\#\#\# Auto-import System

Script 2, Phase 15: Import n8n workflows â”‚ â”œâ”€â”€ n8n provides a CLI and API for importing workflows â”‚ Workflows are stored as JSON and imported after n8n starts â”‚ â”œâ”€â”€ â”€â”€ Wait for n8n readiness â”€â”€ â”‚ import\_n8n\_workflows() { â”‚ echo "Importing n8n workflow templates..." â”‚  
â”‚ local retries=30 â”‚ until curl \-sf [http://localhost:5678/healthz](http://localhost:5678/healthz) \>/dev/null 2\>&1; do â”‚ sleep 5 â”‚ retries=$((retries \- 1)) â”‚ \[\[ $retries \-eq 0 \]\] && { echo " âš  n8n not ready, skipping import"; return; } â”‚ done â”‚ â”œâ”€â”€ â”€â”€ Workflow 1: Budget Monitor â”€â”€ â”‚ File: $CONFIG\_DIR/n8n/workflows/budget-monitor.json â”‚  
â”‚ { â”‚ "name": "AI Platform â€” Budget Monitor", â”‚ "nodes": \[ â”‚ { â”‚ "name": "Schedule Trigger", â”‚ "type": "n8n-nodes-base.scheduleTrigger", â”‚ "parameters": { â”‚ "rule": { "interval": \[{"field": "minutes", "minutesInterval": 15}\] } â”‚ }, â”‚ "position": \[250, 300\] â”‚ }, â”‚ { â”‚ "name": "Query Spend", â”‚ "type": "n8n-nodes-base.httpRequest", â”‚ "parameters": { â”‚ "url": "[http://litellm:4000/global/spend/report](http://litellm:4000/global/spend/report)", â”‚ "method": "GET", â”‚ "headerParameters": { â”‚ "parameters": \[{ â”‚ "name": "Authorization", â”‚ "value": "Bearer {{ $env.LITELLM\_MASTER\_KEY }}" â”‚ }\] â”‚ } â”‚ }, â”‚ "position": \[470, 300\] â”‚ }, â”‚ { â”‚ "name": "Check Threshold", â”‚ "type": "n8n-nodes-base.if", â”‚ "parameters": { â”‚ "conditions": { â”‚ "number": \[{ â”‚ "value1": "={{ $json.total\_spend }}", â”‚ "operation": "largerEqual", â”‚ "value2": "={{ $ env.MONTHLY\_BUDGET\_USD \* 0.8 }}" â”‚ }\] â”‚ } â”‚ }, â”‚ "position": \[690, 300\] â”‚ }, â”‚ { â”‚ "name": "Log Alert", â”‚ "type": "n8n-nodes-base.postgres", â”‚ "parameters": { â”‚ "operation": "executeQuery", â”‚ "query": "INSERT INTO platform.audit\_log (event\_type, details) VALUES ('budget\_alert', '{{ JSON.stringify( $ json) }}')", â”‚ "additionalFields": {} â”‚ }, â”‚ "position": \[910, 200\] â”‚ }, â”‚ { â”‚ "name": "Budget OK", â”‚ "type": "n8n-nodes-base.noOp", â”‚ "position": \[910, 400\] â”‚ } â”‚ \], â”‚ "connections": { â”‚ "Schedule Trigger": { "main": \[\[{"node": "Query Spend", "type": "main", "index": 0}\]\] }, â”‚ "Query Spend": { "main": \[\[{"node": "Check Threshold", "type": "main", "index": 0}\]\] }, â”‚ "Check Threshold": { â”‚ "main": \[ â”‚ \[{"node": "Log Alert", "type": "main", "index": 0}\], â”‚ \[{"node": "Budget OK", "type": "main", "index": 0}\] â”‚ \] â”‚ } â”‚ }, â”‚ "settings": { "executionOrder": "v1" }, â”‚ "tags": \[{"name": "ai-platform"}, {"name": "auto-imported"}\] â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Workflow 2: Health Monitor â”€â”€ â”‚ File: $CONFIG\_DIR/n8n/workflows/health-monitor.json â”‚  
â”‚ { â”‚ "name": "AI Platform â€” Health Monitor", â”‚ "nodes": \[ â”‚ { â”‚ "name": "Schedule", â”‚ "type": "n8n-nodes-base.scheduleTrigger", â”‚ "parameters": { â”‚ "rule": { "interval": \[{"field": "minutes", "minutesInterval": 5}\] } â”‚ } â”‚ }, â”‚ { â”‚ "name": "Check Services", â”‚ "type": "n8n-nodes-base.httpRequest", â”‚ "parameters": { â”‚ "url": "[http://litellm:4000/health](http://litellm:4000/health)", â”‚ "method": "GET", â”‚ "options": { "timeout": 10000 } â”‚ } â”‚ }, â”‚ { â”‚ "name": "Check Ollama", â”‚ "type": "n8n-nodes-base.httpRequest", â”‚ "parameters": { â”‚ "url": "http://host.docker.internal:11434/api/tags", â”‚ "method": "GET", â”‚ "options": { "timeout": 10000 } â”‚ } â”‚ }, â”‚ { â”‚ "name": "Evaluate Health", â”‚ "type": "n8n-nodes-base.code", â”‚ "parameters": { â”‚ "jsCode": "const results \= $input.all();\\nconst unhealthy \= results.filter(r \=\> r.json.statusCode \>= 400);\\nif (unhealthy.length \> 0\) {\\n return \[{json: {status: 'unhealthy', failed: unhealthy.length, details: unhealthy}}\];\\n}\\nreturn \[{json: {status: 'healthy'}}\];" â”‚ } â”‚ }, â”‚ { â”‚ "name": "Alert If Unhealthy", â”‚ "type": "n8n-nodes-base.if", â”‚ "parameters": { â”‚ "conditions": { â”‚ "string": \[{ â”‚ "value1": "={{ $json.status }}", â”‚ "value2": "unhealthy" â”‚ }\] â”‚ } â”‚ } â”‚ } â”‚ \] â”‚ } â”‚ â”œâ”€â”€ â”€â”€ Workflow 3: Google Drive Sync â”€â”€ â”‚ File: $CONFIG\_DIR/n8n/workflows/gdrive-sync.json â”‚  
â”‚ Triggers rclone sync on schedule and logs results â”‚ (Uses n8n Execute Command node to call gdrive-sync.sh) â”‚ â”œâ”€â”€ â”€â”€ Workflow 4: Daily Report â”€â”€ â”‚ File: $CONFIG\_DIR/n8n/workflows/daily-report.json â”‚  
â”‚ \- Runs at 8 AM daily â”‚ \- Queries PostgreSQL for yesterday's usage stats â”‚ \- Queries LiteLLM for model performance â”‚ \- Formats summary â”‚ \- (Optional) sends email/webhook â”‚ â”œâ”€â”€ â”€â”€ Import via n8n API â”€â”€ â”‚ for workflow\_file in ${CONFIG\_DIR}/n8n/workflows/\*.json; do â”‚ local wf\_name= (jqâˆ’râ€².nameâ€²" workflow\_file") â”‚ echo " Importing: ${wf\_name}" â”‚  
â”‚ curl \-s \-X POST "[http://localhost:5678/api/v1/workflows](http://localhost:5678/api/v1/workflows)"  
 â”‚ \-H "Content-Type: application/json"  
 â”‚ \-u "${N8N\_BASIC\_AUTH\_USER}:${N8N\_BASIC\_AUTH\_PASSWORD}"  
 â”‚ \-d @"$workflow\_file" \>/dev/null 2\>&1 &&  
 â”‚ echo " âœ“ Imported" ||  
 â”‚ echo " âš  Import failed (can import manually via n8n UI)" â”‚ done â”‚ â””â”€â”€ echo " âœ“ n8n workflows imported"

\---

\#\# 28\. Troubleshooting & Recovery

\#\#\# Common Issues Decision Tree

ai-troubleshoot (generated convenience script) â”‚ â”œâ”€â”€ \#\!/bin/bash â”‚ echo "â•â•â• AI Platform Troubleshooter â•â•â•" â”‚ echo "" â”‚ â”œâ”€â”€ â”€â”€ Check 1: Docker â”€â”€ â”‚ echo "1. Docker daemon..." â”‚ if \! docker info &\>/dev/null; then â”‚ echo " âœ— Docker not running\!" â”‚ echo " FIX: sudo systemctl start docker" â”‚ exit 1 â”‚ fi â”‚ echo " âœ“ Docker OK" â”‚ â”œâ”€â”€ â”€â”€ Check 2: Containers â”€â”€ â”‚ echo "2. Container status..." â”‚ STOPPED= $ (docker ps \-a \--filter "name=ai-" \--filter "status=exited" \--format '{{.Names}}') â”‚ if \[\[ \-n " $ STOPPED" \]\]; then â”‚ echo " âœ— Stopped containers:" â”‚ echo " $ STOPPED" | sed 's/^/ \- /' â”‚ echo " FIX: cd /opt/ai-platform/compose && docker compose up \-d" â”‚ echo " LOGS: docker logs \<container-name\>" â”‚ fi â”‚  
â”‚ UNHEALTHY= $ (docker ps \--filter "name=ai-" \--filter "health=unhealthy" \--format '{{.Names}}') â”‚ if \[\[ \-n " $ UNHEALTHY" \]\]; then â”‚ echo " âš  Unhealthy containers:" â”‚ echo " $ UNHEALTHY" | sed 's/^/ \- /' â”‚ for c in $UNHEALTHY; do â”‚ echo " Last 5 log lines for $ c:" â”‚ docker logs \--tail 5 " $ c" 2\>&1 | sed 's/^/ /' â”‚ done â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Check 3: Disk Space â”€â”€ â”‚ echo "3. Disk space..." â”‚ local usage=$(df / | tail \-1 | awk '{print $5}' | tr \-d '%') â”‚ if \[\[ $usage \-gt 90 \]\]; then â”‚ echo " âœ— Disk ${usage}% full\!" â”‚ echo " FIX options:" â”‚ echo " docker system prune \-f \# Remove unused images/containers" â”‚ echo " docker volume prune \-f \# Remove unused volumes" â”‚ echo " journalctl \--vacuum-size=500M \# Trim system logs" â”‚ echo " ollama rm \<unused-model\> \# Remove unused models" â”‚ else â”‚ echo " âœ“ Disk ${usage}% used" â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Check 4: Memory â”€â”€ â”‚ echo "4. Memory..." â”‚ local mem\_avail=$(free \-m | awk '/Mem:/ {print $7}') â”‚ if \[\[ $mem\_avail \-lt 1024 \]\]; then â”‚ echo " âš  Low memory: ${mem\_avail}MB available" â”‚ echo " Top memory consumers:" â”‚ docker stats \--no-stream \--format '{{.Name}}\\t{{.MemUsage}}' $(docker ps \-q) |  
 â”‚ sort \-k2 \-h \-r | head \-5 | sed 's/^/ /' â”‚ echo " FIX: Consider disabling unused services in master.env" â”‚ else â”‚ echo " âœ“ Memory OK: ${mem\_avail}MB available" â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Check 5: PostgreSQL â”€â”€ â”‚ echo "5. PostgreSQL..." â”‚ if docker exec ai-postgres pg\_isready \-U aiplatform &\>/dev/null; then â”‚ echo " âœ“ PostgreSQL accepting connections" â”‚  
â”‚ \# Check connection count â”‚ local conns=$(docker exec ai-postgres psql \-U aiplatform \-t \-c  
 â”‚ "SELECT count(\*) FROM pg\_stat\_activity;" 2\>/dev/null | tr \-d ' ') â”‚ echo " Active connections: ${conns}/200" â”‚  
â”‚ if \[\[ ${conns:-0} \-gt 180 \]\]; then â”‚ echo " âš  Near connection limit\!" â”‚ echo " FIX: docker exec ai-postgres psql \-U aiplatform \-c \\" â”‚ echo " "SELECT pg\_terminate\_backend(pid) FROM pg\_stat\_activity WHERE state \= 'idle' AND state\_change \< NOW() \- INTERVAL '10 minutes';"" â”‚ fi â”‚ else â”‚ echo " âœ— PostgreSQL not responding\!" â”‚ echo " LOGS: docker logs ai-postgres \--tail 20" â”‚ echo " FIX: docker restart ai-postgres" â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Check 6: Ollama â”€â”€ â”‚ echo "6. Ollama..." â”‚ if curl \-sf [http://localhost:11434/api/tags](http://localhost:11434/api/tags) &\>/dev/null; then â”‚ local model\_count= $ (curl \-s [http://localhost:11434/api/tags](http://localhost:11434/api/tags) | jq '.models | length') â”‚ echo " âœ“ Ollama running ( $ {model\_count} models)" â”‚  
â”‚ \# Test inference â”‚ local start= $ (date \+%s%N) â”‚ local test= $ (curl \-s \--max-time 30 [http://localhost:11434/api/generate](http://localhost:11434/api/generate)  
 â”‚ \-d '{"model":"tinyllama","prompt":"hi","stream":false}' 2\>/dev/null | jq \-r '.response' 2\>/dev/null) â”‚ local elapsed= ((( (date \+%s%N) \- start) / 1000000 )) â”‚  
â”‚ if \[\[ \-n " $ test" \]\]; then â”‚ echo " âœ“ Inference test passed ( $ {elapsed}ms)" â”‚ else â”‚ echo " âš  Inference test failed" â”‚ echo " LOGS: journalctl \-u ollama \--no-pager \-n 20" â”‚ fi â”‚ else â”‚ echo " âœ— Ollama not responding\!" â”‚ echo " FIX: sudo systemctl restart ollama" â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Check 7: LiteLLM â”€â”€ â”‚ echo "7. LiteLLM proxy..." â”‚ local health= $ (curl \-sf [http://localhost:4000/health](http://localhost:4000/health) 2\>/dev/null) â”‚ if \[\[ \-n " $ health" \]\]; then â”‚ echo " âœ“ LiteLLM healthy" â”‚  
â”‚ \# Check model availability â”‚ local models=$(curl \-s [http://localhost:4000/model/info](http://localhost:4000/model/info)  
 â”‚ \-H "Authorization: Bearer ${LITELLM\_MASTER\_KEY}" 2\>/dev/null |  
 â”‚ jq '.data | length' 2\>/dev/null) â”‚ echo " Models registered: ${models:-unknown}" â”‚ else â”‚ echo " âœ— LiteLLM not responding\!" â”‚ echo " LOGS: docker logs ai-litellm \--tail 30" â”‚ echo " Common fixes:" â”‚ echo " \- Check API keys in /opt/ai-platform/env/litellm.env" â”‚ echo " \- Verify config: docker exec ai-litellm cat /app/config.yaml" â”‚ echo " \- Restart: docker restart ai-litellm" â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Check 8: SSL/Caddy â”€â”€ â”‚ echo "8. SSL/Reverse proxy..." â”‚ if docker exec ai-caddy caddy validate \--config /etc/caddy/Caddyfile &\>/dev/null; then â”‚ echo " âœ“ Caddyfile valid" â”‚ else â”‚ echo " âœ— Invalid Caddyfile\!" â”‚ echo " FIX: Regenerate with script-2-deploy.sh \--regenerate" â”‚ fi â”‚  
â”‚ if \[\[ \-n "${DOMAIN\_BASE:-}" \]\]; then â”‚ local ssl\_test= (curlâˆ’sI"https:// {LITELLM\_DOMAIN:-localhost}" 2\>/dev/null | head \-1) â”‚ if echo "$ssl\_test" | grep \-q "200|301|302"; then â”‚ echo " âœ“ SSL working" â”‚ else â”‚ echo " âš  SSL may not be configured" â”‚ echo " Check DNS points to this server: dig \+short ${DOMAIN\_BASE}" â”‚ fi â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Check 9: Network â”€â”€ â”‚ echo "9. Network..." â”‚ \# Check inter-container connectivity â”‚ if docker exec ai-litellm curl \-sf [http://postgres:5432](http://postgres:5432) &\>/dev/null 2\>&1 ||  
 â”‚ docker exec ai-litellm nc \-z postgres 5432 &\>/dev/null 2\>&1; then â”‚ echo " âœ“ Container networking OK" â”‚ else â”‚ echo " âš  Container network issue" â”‚ echo " FIX: docker network inspect ai-platform\_ai-platform" â”‚ echo " FIX: docker compose down && docker compose up \-d" â”‚ fi â”‚ â”œâ”€â”€ â”€â”€ Summary â”€â”€ â”‚ echo "" â”‚ echo "â•â•â• Quick Fixes â•â•â•" â”‚ echo " Restart everything: cd /opt/ai-platform/compose && docker compose restart" â”‚ echo " Full rebuild: cd /opt/ai-platform/compose && docker compose down && docker compose up \-d" â”‚ echo " Regenerate configs: /opt/ai-platform/scripts/script-2-deploy.sh \--regenerate" â”‚ echo " View all logs: ai-logs all \-f" â”‚ echo " Full status: ai-status" â”‚ â””â”€â”€ echo " Restore from backup: /opt/ai-platform/scripts/restore.sh \<backup.tar.gz\> \--confirm"

\#\#\# Nuclear Recovery Option

File: $BASE\_DIR/scripts/factory-reset.sh â”‚ â”œâ”€â”€ \#\!/bin/bash â”‚ echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—" â”‚ echo "â•‘ âš  FACTORY RESET â€” AI PLATFORM â•‘" â”‚ echo "â•‘ This will DESTROY all data\! â•‘" â”‚ echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" â”‚ echo "" â”‚ echo "This will:" â”‚ echo " 1\. Stop and remove all containers" â”‚ echo " 2\. Remove all Docker volumes" â”‚ echo " 3\. Delete all data in ${DATA\_DIR}" â”‚ echo " 4\. Keep configurations and scripts" â”‚ echo " 5\. Keep Ollama models (system-level)" â”‚ echo "" â”‚ read \-p "Type 'RESET' to confirm: " CONFIRM â”‚ \[\[ " $ CONFIRM" \!= "RESET" \]\] && { echo "Aborted."; exit 0; } â”‚  
â”‚ read \-p "Create backup first? (y/N): " BACKUP â”‚ \[\[ " $ BACKUP" \== "y" \]\] && /opt/ai-platform/scripts/backup.sh full \--upload â”‚  
â”‚ echo "Stopping services..." â”‚ cd /opt/ai-platform/compose â”‚ docker compose down \-v \--remove-orphans â”‚  
â”‚ echo "Removing data..." â”‚ rm \-rf ${DATA\_DIR}/\* â”‚  
â”‚ echo "Recreating directory structure..." â”‚ \# Re-run directory creation from Script 1 â”‚ mkdir \-p ${DATA\_DIR}/{postgres,redis,dify/storage,n8n,open-webui,flowise,qdrant/{storage,snapshots},prometheus,grafana} â”‚  
â”‚ echo "Redeploying..." â”‚ docker compose up \-d â”‚  
â”‚ echo "" â”‚ echo "Factory reset complete. Services restarting with fresh data." â”‚ echo "Run 'ai-status' in 60 seconds to verify."

\---

\*\*End of Part 7 (Sections 25â€“28).\*\*

\---

\#\# 29\. Complete File Manifest & Directory Tree

\#\#\# Full Directory Structure After Script 2 Completes

/opt/ai-platform/ â† BASE\_DIR â”‚ â”œâ”€â”€ env/ â† ENV\_DIR â€” All environment files â”‚ â”œâ”€â”€ master.env â† Master configuration (source of truth) â”‚ â”œâ”€â”€ postgres.env â† PostgreSQL credentials â”‚ â”œâ”€â”€ redis.env â† Redis credentials â”‚ â”œâ”€â”€ litellm.env â† LiteLLM config \+ API keys â”‚ â”œâ”€â”€ dify.env â† Dify configuration â”‚ â”œâ”€â”€ n8n.env â† n8n configuration â”‚ â”œâ”€â”€ open-webui.env â† Open WebUI configuration â”‚ â”œâ”€â”€ flowise.env â† Flowise configuration â”‚ â”œâ”€â”€ supertokens.env â† SuperTokens configuration â”‚ â”œâ”€â”€ caddy.env â† Caddy/SSL configuration â”‚ â”œâ”€â”€ monitoring.env â† Prometheus/Grafana config â”‚ â”œâ”€â”€ qdrant.env â† Qdrant vector DB config â”‚ â””â”€â”€ .env.backup.{timestamp} â† Auto-backup before regeneration â”‚ â”œâ”€â”€ config/ â† CONFIG\_DIR â€” Service configs â”‚ â”œâ”€â”€ postgres/ â”‚ â”‚ â””â”€â”€ init.sql â† Database initialization SQL â”‚ â”œâ”€â”€ redis/ â”‚ â”‚ â””â”€â”€ redis.conf â† Redis configuration â”‚ â”œâ”€â”€ litellm/ â”‚ â”‚ â”œâ”€â”€ config.yaml â† LiteLLM model routing config â”‚ â”‚ â””â”€â”€ custom\_callbacks/ â† Custom callback plugins â”‚ â”‚ â””â”€â”€ cost\_tracker.py â”‚ â”œâ”€â”€ caddy/ â”‚ â”‚ â””â”€â”€ Caddyfile â† Reverse proxy configuration â”‚ â”œâ”€â”€ prometheus/ â”‚ â”‚ â”œâ”€â”€ prometheus.yml â† Prometheus scrape config â”‚ â”‚ â””â”€â”€ alert-rules.yml â† Alert rules â”‚ â”œâ”€â”€ grafana/ â”‚ â”‚ â”œâ”€â”€ provisioning/ â”‚ â”‚ â”‚ â”œâ”€â”€ dashboards/ â”‚ â”‚ â”‚ â”‚ â””â”€â”€ dashboards.yml â† Dashboard provisioning config â”‚ â”‚ â”‚ â””â”€â”€ datasources/ â”‚ â”‚ â”‚ â””â”€â”€ datasources.yml â† Datasource provisioning config â”‚ â”‚ â””â”€â”€ dashboards/ â”‚ â”‚ â”œâ”€â”€ ai-platform-overview.json â† Main dashboard â”‚ â”‚ â”œâ”€â”€ llm-performance.json â† LLM metrics dashboard â”‚ â”‚ â”œâ”€â”€ cost-tracking.json â† Cost/budget dashboard â”‚ â”‚ â””â”€â”€ infrastructure.json â† System resources dashboard â”‚ â”œâ”€â”€ dify/ â”‚ â”‚ â””â”€â”€ .env â† Dify-specific internal env â”‚ â”œâ”€â”€ n8n/ â”‚ â”‚ â””â”€â”€ workflows/ â”‚ â”‚ â”œâ”€â”€ budget-monitor.json â† Auto-imported workflow â”‚ â”‚ â”œâ”€â”€ health-monitor.json â† Auto-imported workflow â”‚ â”‚ â”œâ”€â”€ gdrive-sync.json â† Auto-imported workflow â”‚ â”‚ â””â”€â”€ daily-report.json â† Auto-imported workflow â”‚ â”œâ”€â”€ supertokens/ â”‚ â”‚ â””â”€â”€ config.yaml â† SuperTokens configuration â”‚ â””â”€â”€ qdrant/ â”‚ â””â”€â”€ config.yaml â† Qdrant configuration â”‚ â”œâ”€â”€ compose/ â† COMPOSE\_DIR â”‚ â”œâ”€â”€ docker-compose.yml â† Main compose file (generated) â”‚ â””â”€â”€ docker-compose.override.yml â† User overrides (preserved on regenerate) â”‚ â”œâ”€â”€ data/ â† DATA\_DIR â€” Persistent volumes â”‚ â”œâ”€â”€ postgres/ â† PostgreSQL data â”‚ â”œâ”€â”€ redis/ â† Redis data \+ AOF â”‚ â”œâ”€â”€ dify/ â”‚ â”‚ â””â”€â”€ storage/ â† Dify file uploads & assets â”‚ â”œâ”€â”€ n8n/ â† n8n workflow data â”‚ â”œâ”€â”€ open-webui/ â† Open WebUI data â”‚ â”œâ”€â”€ flowise/ â† Flowise data â”‚ â”œâ”€â”€ qdrant/ â”‚ â”‚ â”œâ”€â”€ storage/ â† Qdrant vector storage â”‚ â”‚ â””â”€â”€ snapshots/ â† Qdrant snapshots â”‚ â”œâ”€â”€ caddy/ â”‚ â”‚ â”œâ”€â”€ data/ â† SSL certificates â”‚ â”‚ â””â”€â”€ config/ â† Caddy auto-config â”‚ â”œâ”€â”€ prometheus/ â† Prometheus TSDB â”‚ â”œâ”€â”€ grafana/ â† Grafana data (dashboards, prefs) â”‚ â”œâ”€â”€ supertokens/ â† SuperTokens data â”‚ â””â”€â”€ ollama-shared/ â† Shared Ollama socket/config â”‚ â”œâ”€â”€ backups/ â† BACKUP\_DIR â”‚ â”œâ”€â”€ ai-platform-backup-{date}.tar.gz â† Compressed backups â”‚ â””â”€â”€ ai-platform-backup-{date}.tar.gz.sha256 â† Checksums â”‚ â”œâ”€â”€ scripts/ â† Operational scripts â”‚ â”œâ”€â”€ script-2-deploy.sh â† This script (preserved for re-run) â”‚ â”œâ”€â”€ ai-status.sh â† Platform status checker â”‚ â”œâ”€â”€ ai-logs.sh â† Log viewer â”‚ â”œâ”€â”€ ai-backup.sh â† Backup wrapper â”‚ â”œâ”€â”€ ai-update.sh â† Update/upgrade tool â”‚ â”œâ”€â”€ ai-models.sh â† Model management â”‚ â”œâ”€â”€ ai-troubleshoot.sh â† Diagnostic tool â”‚ â”œâ”€â”€ backup.sh â† Full backup script â”‚ â”œâ”€â”€ restore.sh â† Full restore script â”‚ â”œâ”€â”€ factory-reset.sh â† Nuclear reset option â”‚ â”œâ”€â”€ gdrive-sync.sh â† Google Drive sync helper â”‚ â””â”€â”€ validate-env.sh â† Environment validator â”‚ â”œâ”€â”€ logs/ â† Platform-level logs â”‚ â””â”€â”€ (symlink â†’ /var/log/ai-platform/) â”‚ â””â”€â”€ docs/ â† Auto-generated documentation â”œâ”€â”€ README.md â† Quick-start guide â”œâ”€â”€ endpoints.md â† All URLs and access info â”œâ”€â”€ passwords.md â† Credential reference (encrypted) â””â”€â”€ architecture.md â† Architecture overview

/var/log/ai-platform/ â† System log directory â”œâ”€â”€ deploy.log â† Script 2 deployment log â”œâ”€â”€ backup.log â† Backup operation logs â”œâ”€â”€ backup-cron.log â† Cron backup logs â”œâ”€â”€ health.log â† Health check logs â””â”€â”€ update.log â† Update operation logs

/usr/local/bin/ â† System-wide convenience symlinks â”œâ”€â”€ ai-status â†’ /opt/ai-platform/scripts/ai-status.sh â”œâ”€â”€ ai-logs â†’ /opt/ai-platform/scripts/ai-logs.sh â”œâ”€â”€ ai-backup â†’ /opt/ai-platform/scripts/ai-backup.sh â”œâ”€â”€ ai-update â†’ /opt/ai-platform/scripts/ai-update.sh â”œâ”€â”€ ai-models â†’ /opt/ai-platform/scripts/ai-models.sh â””â”€â”€ ai-troubleshoot â†’ /opt/ai-platform/scripts/ai-troubleshoot.sh

/etc/systemd/system/ â””â”€â”€ ollama.service â† Ollama systemd service (from Script 1\)

Crontab entries: â”œâ”€â”€ 0 2 \* \* \* /opt/ai-platform/scripts/backup.sh full \--upload â”œâ”€â”€ \*/5 \* \* \* \* /opt/ai-platform/scripts/health-check.sh â””â”€â”€ 0 4 \* \* 0 docker system prune \-f \>\> /var/log/ai-platform/cleanup.log 2\>&1

\---

\#\# 30\. Environment Variable Complete Reference

\#\#\# master.env â€” All Variables

Variable Name â”‚ Source â”‚ Example Value â”‚ Description â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# **â”€â”€ Identity â”€â”€ â”‚ â”‚ â”‚**

PLATFORM\_NAME â”‚ Script 2 â”‚ "AI Platform" â”‚ Display name DEPLOYMENT\_ID â”‚ Script 2 â”‚ "ai-plat-a1b2c3" â”‚ Unique deployment identifier SCRIPT\_2\_VERSION â”‚ Script 2 â”‚ "2.0.0" â”‚ Script version DEPLOYED\_AT â”‚ Script 2 â”‚ "2025-01-15T10:30:00Z" â”‚ Deployment timestamp â”‚ â”‚ â”‚

# **â”€â”€ System Detection â”€â”€ â”‚ â”‚ â”‚**

TOTAL\_RAM\_GB â”‚ Script 1 â”‚ 64 â”‚ Detected total RAM CPU\_CORES â”‚ Script 1 â”‚ 16 â”‚ Detected CPU cores GPU\_AVAILABLE â”‚ Script 1 â”‚ true â”‚ GPU detected flag GPU\_TYPE â”‚ Script 1 â”‚ "nvidia" â”‚ GPU vendor GPU\_VRAM\_MB â”‚ Script 1 â”‚ 24576 â”‚ Total GPU VRAM CUDA\_VERSION â”‚ Script 1 â”‚ "12.2" â”‚ Detected CUDA version DISK\_TOTAL\_GB â”‚ Script 1 â”‚ 500 â”‚ Total disk space DISK\_FREE\_GB â”‚ Script 1 â”‚ 420 â”‚ Free disk at deploy time â”‚ â”‚ â”‚

# **â”€â”€ Network & DNS â”€â”€ â”‚ â”‚ â”‚**

DOMAIN\_BASE â”‚ User â”‚ "myai.example.com" â”‚ Base domain USE\_SUBDOMAINS â”‚ Script 2 â”‚ true â”‚ Subdomain mode flag LITELLM\_DOMAIN â”‚ Script 2 â”‚ "llm.myai.example.com" â”‚ LiteLLM URL DIFY\_DOMAIN â”‚ Script 2 â”‚ "dify.myai.example.com" â”‚ Dify URL N8N\_DOMAIN â”‚ Script 2 â”‚ "n8n.myai.example.com" â”‚ n8n URL OPEN\_WEBUI\_DOMAIN â”‚ Script 2 â”‚ "chat.myai.example.com" â”‚ Open WebUI URL FLOWISE\_DOMAIN â”‚ Script 2 â”‚ "flow.myai.example.com" â”‚ Flowise URL GRAFANA\_DOMAIN â”‚ Script 2 â”‚ "grafana.myai.example.com" â”‚ Grafana URL SUPERTOKENS\_DOMAIN â”‚ Script 2 â”‚ "auth.myai.example.com" â”‚ SuperTokens URL SSL\_MODE â”‚ Script 2 â”‚ "auto" â”‚ auto|manual|selfsigned SSL\_EMAIL â”‚ User â”‚ "admin@example.com" â”‚ Let's Encrypt email SERVER\_IP â”‚ Script 1 â”‚ "203.0.113.50" â”‚ Public IP address â”‚ â”‚ â”‚

# **â”€â”€ Service Toggles â”€â”€ â”‚ â”‚ â”‚**

ENABLED\_SERVICES â”‚ Script 2 â”‚ "postgres,redis,litellm..." â”‚ Comma-separated list ENABLE\_DIFY â”‚ Script 2 â”‚ true â”‚ Dify enabled ENABLE\_N8N â”‚ Script 2 â”‚ true â”‚ n8n enabled ENABLE\_OPEN\_WEBUI â”‚ Script 2 â”‚ true â”‚ Open WebUI enabled ENABLE\_FLOWISE â”‚ Script 2 â”‚ true â”‚ Flowise enabled ENABLE\_MONITORING â”‚ Script 2 â”‚ true â”‚ Prometheus+Grafana ENABLE\_SUPERTOKENS â”‚ Script 2 â”‚ true â”‚ Auth service VECTOR\_DB\_CHOICE â”‚ Script 2 â”‚ "qdrant" â”‚ qdrant|weaviate|milvus â”‚ â”‚ â”‚

# **â”€â”€ PostgreSQL â”€â”€ â”‚ â”‚ â”‚**

POSTGRES\_USER â”‚ Script 2 â”‚ "aiplatform" â”‚ DB superuser POSTGRES\_PASSWORD â”‚ Generate â”‚ "xK9m2...(32 chars)" â”‚ DB password POSTGRES\_HOST â”‚ Script 2 â”‚ "postgres" â”‚ Docker hostname POSTGRES\_PORT â”‚ Script 2 â”‚ 5432 â”‚ Internal port POSTGRES\_EXTERNAL\_PORT â”‚ Script 2 â”‚ 5432 â”‚ External port (or "none") â”‚ â”‚ â”‚

# **â”€â”€ Redis â”€â”€ â”‚ â”‚ â”‚**

REDIS\_PASSWORD â”‚ Generate â”‚ "rP8k4...(32 chars)" â”‚ Redis password REDIS\_HOST â”‚ Script 2 â”‚ "redis" â”‚ Docker hostname REDIS\_PORT â”‚ Script 2 â”‚ 6379 â”‚ Internal port REDIS\_MAXMEMORY â”‚ Script 2 â”‚ "2gb" â”‚ Memory limit â”‚ â”‚ â”‚

# **â”€â”€ LiteLLM â”€â”€ â”‚ â”‚ â”‚**

LITELLM\_MASTER\_KEY â”‚ Generate â”‚ "sk-litellm-...(48 chars)" â”‚ Admin API key LITELLM\_SALT\_KEY â”‚ Generate â”‚ "salt-...(32 chars)" â”‚ Encryption salt LITELLM\_PORT â”‚ Script 2 â”‚ 4000 â”‚ Proxy port LITELLM\_UI\_USERNAME â”‚ Script 2 â”‚ "admin" â”‚ Web UI username LITELLM\_UI\_PASSWORD â”‚ Generate â”‚ "uP3x...(24 chars)" â”‚ Web UI password LITELLM\_LOG\_LEVEL â”‚ Script 2 â”‚ "INFO" â”‚ Log verbosity â”‚ â”‚ â”‚

# **â”€â”€ API Keys (Cloud LLMs) â”€â”€ â”‚ â”‚ â”‚**

OPENAI\_API\_KEY â”‚ User â”‚ "sk-..." â”‚ OpenAI key ANTHROPIC\_API\_KEY â”‚ User â”‚ "sk-ant-..." â”‚ Anthropic key GOOGLE\_API\_KEY â”‚ User â”‚ "AIza..." â”‚ Google AI key GROQ\_API\_KEY â”‚ User â”‚ "gsk\_..." â”‚ Groq key MISTRAL\_API\_KEY â”‚ User â”‚ "..." â”‚ Mistral key OPENROUTER\_API\_KEY â”‚ User â”‚ "sk-or-..." â”‚ OpenRouter key COHERE\_API\_KEY â”‚ User â”‚ "..." â”‚ Cohere key â”‚ â”‚ â”‚

# **â”€â”€ Dify â”€â”€ â”‚ â”‚ â”‚**

DIFY\_SECRET\_KEY â”‚ Generate â”‚ "dify-...(48 chars)" â”‚ App secret DIFY\_INIT\_PASSWORD â”‚ Generate â”‚ "dP5j...(16 chars)" â”‚ Initial admin password DIFY\_API\_PORT â”‚ Script 2 â”‚ 5001 â”‚ API port DIFY\_WEB\_PORT â”‚ Script 2 â”‚ 3001 â”‚ Web UI port DIFY\_SANDBOX\_PORT â”‚ Script 2 â”‚ 8194 â”‚ Sandbox port DIFY\_WEAVIATE\_PORT â”‚ Script 2 â”‚ 8080 â”‚ Vector DB port (if used) â”‚ â”‚ â”‚

# **â”€â”€ n8n â”€â”€ â”‚ â”‚ â”‚**

N8N\_ENCRYPTION\_KEY â”‚ Generate â”‚ "n8n-...(48 chars)" â”‚ Workflow encryption N8N\_BASIC\_AUTH\_USER â”‚ Script 2 â”‚ "admin" â”‚ Basic auth user N8N\_BASIC\_AUTH\_PASSWORD â”‚ Generate â”‚ "nP4z...(24 chars)" â”‚ Basic auth password N8N\_PORT â”‚ Script 2 â”‚ 5678 â”‚ Web UI port N8N\_WEBHOOK\_URL â”‚ Script 2 â”‚ "[https://n8n.example.com](https://n8n.example.com)" â”‚ Webhook base URL â”‚ â”‚ â”‚

# **â”€â”€ Open WebUI â”€â”€ â”‚ â”‚ â”‚**

OPEN\_WEBUI\_PORT â”‚ Script 2 â”‚ 3000 â”‚ Web UI port OPEN\_WEBUI\_SECRET\_KEY â”‚ Generate â”‚ "ow-...(32 chars)" â”‚ Session secret WEBUI\_AUTH â”‚ Script 2 â”‚ true â”‚ Enable auth â”‚ â”‚ â”‚

# **â”€â”€ Flowise â”€â”€ â”‚ â”‚ â”‚**

FLOWISE\_PORT â”‚ Script 2 â”‚ 3002 â”‚ Web UI port FLOWISE\_USERNAME â”‚ Script 2 â”‚ "admin" â”‚ Login username FLOWISE\_PASSWORD â”‚ Generate â”‚ "fP7w...(24 chars)" â”‚ Login password FLOWISE\_SECRETKEY\_OVERWRITE â”‚ Generate â”‚ "fl-...(32 chars)" â”‚ API secret â”‚ â”‚ â”‚

# **â”€â”€ Qdrant â”€â”€ â”‚ â”‚ â”‚**

QDRANT\_PORT â”‚ Script 2 â”‚ 6333 â”‚ HTTP API port QDRANT\_GRPC\_PORT â”‚ Script 2 â”‚ 6334 â”‚ gRPC port QDRANT\_API\_KEY â”‚ Generate â”‚ "qd-...(32 chars)" â”‚ API key â”‚ â”‚ â”‚

# **â”€â”€ SuperTokens â”€â”€ â”‚ â”‚ â”‚**

SUPERTOKENS\_API\_KEY â”‚ Generate â”‚ "st-...(48 chars)" â”‚ Core API key SUPERTOKENS\_PORT â”‚ Script 2 â”‚ 3567 â”‚ Core port â”‚ â”‚ â”‚

# **â”€â”€ Monitoring â”€â”€ â”‚ â”‚ â”‚**

GRAFANA\_ADMIN\_USER â”‚ Script 2 â”‚ "admin" â”‚ Grafana admin GRAFANA\_ADMIN\_PASSWORD â”‚ Generate â”‚ "gP2m...(24 chars)" â”‚ Grafana password GRAFANA\_PORT â”‚ Script 2 â”‚ 3003 â”‚ Grafana web port PROMETHEUS\_PORT â”‚ Script 2 â”‚ 9090 â”‚ Prometheus port PROMETHEUS\_RETENTION â”‚ Script 2 â”‚ "30d" â”‚ Data retention â”‚ â”‚ â”‚

# **â”€â”€ Ollama â”€â”€ â”‚ â”‚ â”‚**

OLLAMA\_HOST â”‚ Script 2 â”‚ "[http://host.docker.internal:11434](http://host.docker.internal:11434)" â”‚ Ollama URL for containers OLLAMA\_BASE\_URL â”‚ Script 2 â”‚ "[http://host.docker.internal:11434](http://host.docker.internal:11434)" â”‚ Alternative form OLLAMA\_MODELS\_DIR â”‚ Script 1 â”‚ "/usr/share/ollama/.ollama/models" â”‚ Model storage path OLLAMA\_DEFAULT\_MODEL â”‚ Script 2 â”‚ "mistral" â”‚ Default chat model OLLAMA\_EMBEDDING\_MODEL â”‚ Script 2 â”‚ "nomic-embed-text" â”‚ Default embedding model â”‚ â”‚ â”‚

# **â”€â”€ Budget â”€â”€ â”‚ â”‚ â”‚**

MONTHLY\_BUDGET\_USD â”‚ User â”‚ 50 â”‚ Monthly budget cap BUDGET\_ALERT\_THRESHOLD â”‚ Script 2 â”‚ 0.8 â”‚ Alert at 80% BUDGET\_HARD\_LIMIT â”‚ Script 2 â”‚ true â”‚ Enforce limit? â”‚ â”‚ â”‚

# **â”€â”€ Backup â”€â”€ â”‚ â”‚ â”‚**

BACKUP\_BASE\_DIR â”‚ Script 2 â”‚ "/opt/ai-platform/backups" â”‚ Backup storage BACKUP\_RETENTION\_DAYS â”‚ Script 2 â”‚ 7 â”‚ Local retention BACKUP\_SCHEDULE â”‚ Script 2 â”‚ "0 2 \* \* \*" â”‚ Cron schedule ENABLE\_GDRIVE\_BACKUP â”‚ User â”‚ false â”‚ GDrive sync enabled GDRIVE\_FOLDER\_ID â”‚ User â”‚ "" â”‚ GDrive target folder â”‚ â”‚ â”‚

# **â”€â”€ Resource Limits â”€â”€ â”‚ â”‚ â”‚**

POSTGRES\_MEMORY\_LIMIT â”‚ Script 2 â”‚ "4g" â”‚ PG container limit REDIS\_MEMORY\_LIMIT â”‚ Script 2 â”‚ "2g" â”‚ Redis container limit LITELLM\_MEMORY\_LIMIT â”‚ Script 2 â”‚ "2g" â”‚ LiteLLM container limit DIFY\_API\_MEMORY\_LIMIT â”‚ Script 2 â”‚ "4g" â”‚ Dify API limit N8N\_MEMORY\_LIMIT â”‚ Script 2 â”‚ "2g" â”‚ n8n container limit â”‚ â”‚ â”‚

# **â”€â”€ Paths â”€â”€ â”‚ â”‚ â”‚**

BASE\_DIR â”‚ Script 2 â”‚ "/opt/ai-platform" â”‚ Root directory CONFIG\_DIR â”‚ Script 2 â”‚ "/opt/ai-platform/config" â”‚ Configuration dir DATA\_DIR â”‚ Script 2 â”‚ "/opt/ai-platform/data" â”‚ Persistent data COMPOSE\_DIR â”‚ Script 2 â”‚ "/opt/ai-platform/compose" â”‚ Compose files ENV\_DIR â”‚ Script 2 â”‚ "/opt/ai-platform/env" â”‚ Environment files SCRIPTS\_DIR â”‚ Script 2 â”‚ "/opt/ai-platform/scripts" â”‚ Script directory LOG\_DIR â”‚ Script 2 â”‚ "/var/log/ai-platform" â”‚ Log directory TIMEZONE â”‚ Script 2 â”‚ "UTC" â”‚ System timezone

\---

\#\# 31\. IP Address & Port Map

\#\#\# Internal Network (Docker: 172.28.0.0/16)

Container Name â”‚ Docker IP â”‚ Internal Port â”‚ Host Port â”‚ Protocol â”‚ URL Path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ai-postgres â”‚ 172.28.0.10 â”‚ 5432 â”‚ 5432\* â”‚ TCP â”‚ â€” ai-redis â”‚ 172.28.0.11 â”‚ 6379 â”‚ â€”\*\* â”‚ TCP â”‚ â€” ai-litellm â”‚ 172.28.0.20 â”‚ 4000 â”‚ 4000 â”‚ HTTP â”‚ / ai-dify-api â”‚ 172.28.0.30 â”‚ 5001 â”‚ 5001 â”‚ HTTP â”‚ / ai-dify-worker â”‚ 172.28.0.31 â”‚ â€” â”‚ â€” â”‚ â€” â”‚ â€” ai-dify-web â”‚ 172.28.0.32 â”‚ 3001 â”‚ 3001 â”‚ HTTP â”‚ / ai-dify-sandbox â”‚ 172.28.0.33 â”‚ 8194 â”‚ â€” â”‚ HTTP â”‚ â€” ai-n8n â”‚ 172.28.0.40 â”‚ 5678 â”‚ 5678 â”‚ HTTP â”‚ / ai-open-webui â”‚ 172.28.0.50 â”‚ 3000 (â†’8080) â”‚ 3000 â”‚ HTTP â”‚ / ai-flowise â”‚ 172.28.0.60 â”‚ 3002 (â†’3000) â”‚ 3002 â”‚ HTTP â”‚ / ai-qdrant â”‚ 172.28.0.70 â”‚ 6333/6334 â”‚ 6333 â”‚ HTTP/gRPCâ”‚ / ai-supertokens â”‚ 172.28.0.80 â”‚ 3567 â”‚ 3567 â”‚ HTTP â”‚ / ai-caddy â”‚ 172.28.0.2 â”‚ 80/443 â”‚ 80/443 â”‚ HTTP/S â”‚ / ai-prometheus â”‚ 172.28.1.10 â”‚ 9090 â”‚ 9090\* â”‚ HTTP â”‚ / ai-grafana â”‚ 172.28.1.20 â”‚ 3000 â”‚ 3003 â”‚ HTTP â”‚ / â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

* \= Only exposed to localhost (127.0.0.1:port:port) unless explicitly opened \*\* \= Redis never exposed to host network

Host-level service: Ollama â”‚ 0.0.0.0 â”‚ 11434 â”‚ 11434 â”‚ HTTP â”‚ /api/

\#\#\# Caddy Reverse Proxy Routes

External URL â”‚ Internal Target â”‚ Auth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€ [https://llm.{domain}/](https://llm.{domain}/) â”‚ [http://ai-litellm:4000](http://ai-litellm:4000) â”‚ API Key [https://dify.{domain}/](https://dify.{domain}/) â”‚ [http://ai-dify-web:3001](http://ai-dify-web:3001) â”‚ Built-in [https://dify.{domain}/v1/](https://dify.{domain}/v1/)\* â”‚ [http://ai-dify-api:5001](http://ai-dify-api:5001) â”‚ API Key [https://n8n.{domain}/](https://n8n.{domain}/) â”‚ [http://ai-n8n:5678](http://ai-n8n:5678) â”‚ Basic Auth [https://chat.{domain}/](https://chat.{domain}/) â”‚ [http://ai-open-webui:8080](http://ai-open-webui:8080) â”‚ Built-in [https://flow.{domain}/](https://flow.{domain}/) â”‚ [http://ai-flowise:3000](http://ai-flowise:3000) â”‚ Basic Auth [https://grafana.{domain}/](https://grafana.{domain}/) â”‚ [http://ai-grafana:3000](http://ai-grafana:3000) â”‚ Built-in [https://auth.{domain}/](https://auth.{domain}/) â”‚ [http://ai-supertokens:3567â”‚](http://ai-supertokens:3567â”‚) API Key

Without domain (IP-only mode): http://{IP}:80/litellm/\* â”‚ [http://ai-litellm:4000](http://ai-litellm:4000) â”‚ API Key http://{IP}:80/dify/\* â”‚ [http://ai-dify-web:3001](http://ai-dify-web:3001) â”‚ Built-in http://{IP}:80/n8n/\* â”‚ [http://ai-n8n:5678](http://ai-n8n:5678) â”‚ Basic Auth http://{IP}:80/chat/\* â”‚ [http://ai-open-webui:8080](http://ai-open-webui:8080) â”‚ Built-in http://{IP}:80/flowise/\* â”‚ [http://ai-flowise:3000](http://ai-flowise:3000) â”‚ Basic Auth http://{IP}:80/grafana/\* â”‚ [http://ai-grafana:3000](http://ai-grafana:3000) â”‚ Built-in

\#\#\# Firewall Rules (UFW)

Rule â”‚ Purpose â”‚ Set By â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€ 22/tcp ALLOW â”‚ SSH access â”‚ Script 1 80/tcp ALLOW â”‚ HTTP (Caddy redirect) â”‚ Script 2 443/tcp ALLOW â”‚ HTTPS (Caddy SSL) â”‚ Script 2 11434/tcp ALLOW from 172.28.0.0/16 â”‚ Ollama (Docker only) â”‚ Script 2 5432/tcp DENY from any â”‚ PostgreSQL (no external) â”‚ Script 2 6379/tcp DENY from any â”‚ Redis (no external) â”‚ Script 2

\---

\#\# 32\. Post-Deployment Checklist

\#\#\# Automated Verification (Script 2, Phase 16\)

run\_post\_deployment\_checks() â”‚ â”œâ”€â”€ CHECK 1: All containers running â”‚ Expected: All enabled services show "running" â”‚ Command: docker ps \--filter "name=ai-" \--format '{{.Names}} {{.Status}}' â”‚ Pass: All containers show "Up" â”‚  
â”œâ”€â”€ CHECK 2: Health endpoints responding â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ Service â”‚ Health URL â”‚ Expected â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ PostgreSQL â”‚ pg\_isready \-U aiplatform â”‚ exit 0 â”‚ â”‚ â”‚ Redis â”‚ redis-cli \-a $PASS ping â”‚ PONG â”‚ â”‚ â”‚ LiteLLM â”‚ [http://localhost:4000/health](http://localhost:4000/health) â”‚ HTTP 200 â”‚ â”‚ â”‚ Dify API â”‚ [http://localhost:5001/health](http://localhost:5001/health) â”‚ HTTP 200 â”‚ â”‚ â”‚ Dify Web â”‚ [http://localhost:3001/](http://localhost:3001/) â”‚ HTTP 200 â”‚ â”‚ â”‚ n8n â”‚ [http://localhost:5678/healthz](http://localhost:5678/healthz) â”‚ HTTP 200 â”‚ â”‚ â”‚ Open WebUI â”‚ [http://localhost:3000/](http://localhost:3000/) â”‚ HTTP 200 â”‚ â”‚ â”‚ Flowise â”‚ [http://localhost:3002/](http://localhost:3002/) â”‚ HTTP 200 â”‚ â”‚ â”‚ Qdrant â”‚ [http://localhost:6333/healthz](http://localhost:6333/healthz) â”‚ HTTP 200 â”‚ â”‚ â”‚ Caddy â”‚ [http://localhost:80/](http://localhost:80/) â”‚ HTTP 2xx â”‚ â”‚ â”‚ Prometheus â”‚ [http://localhost:9090/-/healthy](http://localhost:9090/-/healthy) â”‚ HTTP 200 â”‚ â”‚ â”‚ Grafana â”‚ [http://localhost:3003/api/health](http://localhost:3003/api/health) â”‚ HTTP 200 â”‚ â”‚ â”‚ SuperTokens â”‚ [http://localhost:3567/hello](http://localhost:3567/hello) â”‚ HTTP 200 â”‚ â”‚ â”‚ Ollama â”‚ [http://localhost:11434/api/tags](http://localhost:11434/api/tags) â”‚ HTTP 200 â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”œâ”€â”€ CHECK 3: Database connectivity â”‚ \- Each service database exists â”‚ \- Extensions loaded (uuid-ossp, vector, etc.) â”‚ \- Platform schema created â”‚  
â”œâ”€â”€ CHECK 4: LiteLLM model access â”‚ \- At least 1 model responding â”‚ \- Test: simple chat completion â”‚ \- Response received within timeout â”‚  
â”œâ”€â”€ CHECK 5: Ollama models loaded â”‚ \- At least default model present â”‚ \- Test inference works â”‚  
â”œâ”€â”€ CHECK 6: Inter-service communication â”‚ \- Dify â†’ LiteLLM connectivity â”‚ \- n8n â†’ PostgreSQL connectivity  
â”‚ \- Open WebUI â†’ Ollama connectivity â”‚ \- All services â†’ Redis connectivity â”‚  
â”œâ”€â”€ CHECK 7: SSL/TLS (if configured) â”‚ \- Certificates obtained or self-signed present â”‚ \- HTTPS responding on port 443 â”‚ \- HTTP â†’ HTTPS redirect working â”‚  
â”œâ”€â”€ CHECK 8: Disk space adequate â”‚ \- At least 20% free space remaining â”‚ \- All data directories writable â”‚  
â”œâ”€â”€ CHECK 9: Backup system ready â”‚ \- Backup script executable â”‚ \- Crontab entry present â”‚ \- Backup directory writable â”‚ \- Test backup of configs succeeds â”‚  
â”œâ”€â”€ CHECK 10: Monitoring operational â”‚ \- Prometheus scraping targets â”‚ \- Grafana datasource connected â”‚ \- Dashboards loaded â”‚ â””â”€â”€ â”€â”€ Output Summary â”€â”€

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
    AI PLATFORM â€” DEPLOYMENT COMPLETE  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Status:     âœ“ ALL CHECKS PASSED (10/10)  
Deployed:   2025-01-15 10:45:32 UTC  
Duration:   12 minutes 34 seconds

â”€â”€ Service URLs â”€â”€

LiteLLM Proxy:    https://llm.myai.example.com  
                  API Key: sk-litellm-xxxx...xxxx

Dify Platform:    https://dify.myai.example.com  
                  Initial password: dP5j...

n8n Workflows:    https://n8n.myai.example.com  
                  Login: admin / nP4z...

Open WebUI:       https://chat.myai.example.com  
                  (Create account on first visit)

Flowise:          https://flow.myai.example.com  
                  Login: admin / fP7w...

Grafana:          https://grafana.myai.example.com  
                  Login: admin / gP2m...

â”€â”€ Local Models (Ollama) â”€â”€

mistral:latest         4.1 GB    âœ“ Ready  
nomic-embed-text:latest 274 MB   âœ“ Ready

â”€â”€ Quick Commands â”€â”€

ai-status          Check platform status  
ai-logs \[service\]  View logs (-f to follow)  
ai-models list     List available models  
ai-models pull X   Download new model  
ai-backup full     Create full backup  
ai-update          Update all services  
ai-troubleshoot    Run diagnostics

â”€â”€ Files â”€â”€

Master config:   /opt/ai-platform/env/master.env  
Docker compose:  /opt/ai-platform/compose/docker-compose.yml  
Service configs: /opt/ai-platform/config/  
Backups:         /opt/ai-platform/backups/  
Logs:            /var/log/ai-platform/

â”€â”€ Credentials â”€â”€

All passwords saved to: /opt/ai-platform/env/master.env  
(File permissions: 600 â€” owner-read only)

âš  IMPORTANT: Save your credentials securely\!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Full credential list:  
grep \-E '(PASSWORD|KEY|SECRET)' /opt/ai-platform/env/master.env

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

\---

\#\# 33\. Architecture Diagram

\#\#\# System Architecture (ASCII)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ INTERNET â”‚ â”‚ â”‚ â”‚ Users / API Clients / Webhooks â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ CADDY REVERSE PROXY â”‚ â”‚ (Auto-SSL / Let's Encrypt) â”‚ â”‚ Port 80 â”€â”€â†’ 443 redirect â”‚ â”‚ Port 443 â”€â”€â†’ Route by subdomain: â”‚ â”‚ â”‚ â”‚ llm.domain â†’ LiteLLM:4000 chat.domain â†’ Open-WebUI:8080 â”‚ â”‚ dify.domain â†’ Dify-Web:3001 flow.domain â†’ Flowise:3000 â”‚ â”‚ n8n.domain â†’ n8n:5678 grafana.domain â†’ Grafana:3000 â”‚ â”‚ auth.domain â†’ SuperTokens:3567 â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Docker Network: ai-platform (172.28.0.0/16)â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ LiteLLM â”‚ â”‚ Dify â”‚ â”‚ â”‚ â”‚ Proxy â”‚ â”‚ Platform â”‚ â”‚ â”‚ â”‚ :4000 â”‚ â”‚ API :5001 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Web :3001 â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”‚ Worker â”‚ â”‚ â”‚ â”‚ â”‚Routerâ”‚ â”‚ â”‚ Sandbox:8194 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ Cost â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚Track â”‚ â”‚ â”‚ (uses LiteLLM â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ as LLM backend) â”‚ â”‚ â”‚ â”‚Cache â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”¬â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â””â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â–¼ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Redis â”‚ â”‚ n8n â”‚ â”‚ Open â”‚ â”‚ â”‚ â”‚ :6379 â”‚ â”‚ :5678 â”‚ â”‚ WebUI â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ :8080 â”‚ â”‚ â”‚ â”‚ Cache â”‚ â”‚Workflowsâ”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Queue â”‚ â”‚Automatonâ”‚ â”‚ Chat UI â”‚ â”‚ â”‚ â”‚ Sessionâ”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â–¼ â–¼ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚PostgreSQLâ”‚ â”‚ Flowise â”‚ â”‚ â”‚ â”‚ â”‚ :5432 â”‚ â”‚ :3000 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ pgvector â”‚ â”‚ Flow â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Builder â”‚ â”‚ â”‚ â”‚ â”‚ DBs: â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ litellm â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ dify â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ n8n â”‚ â–¼ â–¼ â”‚ â”‚ â”‚ platform â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ grafana â”‚ â”‚ Qdrant â”‚ â”‚ â”‚ â”‚ super- â”‚ â”‚ Vector DB â”‚ â”‚ â”‚ â”‚ tokens â”‚ â”‚ :6333 HTTP â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ :6334 gRPC â”‚ â”‚ â”‚ â”‚ Embeddings Store â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Monitoring Network â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ Prometheus â”‚ â”‚ Grafana â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ :9090 â”‚ â”‚ :3000 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Scrapes: â”‚ â”‚ Dashboards: â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ \- LiteLLM â”‚â”€â”€â”‚ \- Overview â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ \- Caddy â”‚ â”‚ \- LLM Perf â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ \- Postgres â”‚ â”‚ \- Cost Track â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ \- Redis â”‚ â”‚ \- Infra â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ \- Node â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ SuperTokens â”‚ â”‚ â”‚ â”‚ Auth :3567 â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ HOST SYSTEM â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ Ollama â”‚ â”‚ â”‚ â”‚ :11434 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ Models: â”‚ â”‚ â”‚ â”‚ \- mistral â”‚ â”‚ â”‚ â”‚ \- nomic-embed â”‚ â”‚ â”‚ â”‚ \- llama3.1 â”‚ â”‚ â”‚ â”‚ \- codellama â”‚ â”‚ â”‚ â”‚ \- (user added) â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ GPU/CUDA â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ or CPU â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
     â”‚         CLOUD LLM PROVIDERS          â”‚  
     â”‚  (via LiteLLM Router)                â”‚  
     â”‚                                      â”‚  
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  
     â”‚  â”‚ OpenAI   â”‚  â”‚ Anthropic        â”‚ â”‚  
     â”‚  â”‚ GPT-4o   â”‚  â”‚ Claude 3.5       â”‚ â”‚  
     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  
     â”‚  â”‚ Google   â”‚  â”‚ Groq             â”‚ â”‚  
     â”‚  â”‚ Gemini   â”‚  â”‚ (fast inference) â”‚ â”‚  
     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  
     â”‚  â”‚ Mistral  â”‚  â”‚ OpenRouter       â”‚ â”‚  
     â”‚  â”‚          â”‚  â”‚ (100+ models)    â”‚ â”‚  
     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\#\#\# Data Flow: Chat Request

User sends message â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Caddy â”‚ â”€â”€ SSL termination â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ LiteLLM â”‚ â”€â”€ Authentication (API key check) â”‚ Proxy â”‚ â”€â”€ Check Redis cache â”‚ â”‚ â”€â”€ Budget check â”‚ â”‚ â”€â”€ Route decision: â”‚ â”‚ â”‚ Cache HIT? â”œâ”€â”€YESâ”€â”€â†’ Return cached response (0 cost) â”‚ â”‚ â”‚ Cache MISS â”‚ â”‚ Route: â”‚ â”‚ â”‚ â”‚ Simple? â”€â”€â”œâ”€â”€YESâ”€â”€â†’ Ollama (local, free) â”‚ â”‚ â”‚ â”‚ Complex? â”€â”€â”œâ”€â”€YESâ”€â”€â†’ Cloud Provider (paid) â”‚ â”‚ â”‚ â”‚ Budget â”‚ â”‚ â”‚ exceeded?â”€â”€â”œâ”€â”€YESâ”€â”€â†’ Ollama fallback (free) â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Log to â”‚ â”‚ Response â”‚ â”‚ PostgreSQL â”‚ â”‚ returned â”‚ â”‚ \+ Redis â”‚ â”‚ to user â”‚ â”‚ cache â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\#\#\# Data Flow: RAG (Retrieval-Augmented Generation)

User uploads document â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Dify / â”‚ â”€â”€ Document received â”‚ Flowise â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Text â”‚ â”€â”€ Extract text from PDF/DOC/etc. â”‚ Extraction â”‚ â”€â”€ Chunk into segments (\~512 tokens) â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ LiteLLM â”‚ â”€â”€ Request â”€â”€â†’ â”‚ Ollama â”‚ â”‚ (embedding) â”‚ embedding â”‚ nomic- â”‚ â”‚ â”‚ â—€â”€â”€ Vector â”€â”€ â”‚ embed â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ \[768 dims\] â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Qdrant â”‚ â”€â”€ Store vector \+ metadata â”‚ Vector DB â”‚ â”€â”€ Collection: "user\_docs" â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     â‹® (later, user asks question)  
      

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ User Query â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Embed query â”‚ â”€â”€ Same embedding model â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Qdrant â”‚ â”€â”€ Similarity search â”‚ Search â”‚ â”€â”€ Return top-K chunks â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Construct prompt: â”‚ â”‚ â”‚ â”‚ System: You are helpful... â”‚ â”‚ Context: {retrieved chunks} â”‚ â”‚ User: {original question} â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ LiteLLM â”‚ â”€â”€ Route to best model â”‚ â†’ LLM â”‚ â”€â”€ Generate answer using context â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Response â”‚ â”€â”€ Answer grounded in documents â”‚ to user â”‚ â”€â”€ With source citations â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

\---

\#\# END OF DOCUMENT

\#\#\# Document Statistics

Total Sections: 33 Total Parts: 8 Estimated Lines: \~4,500+ (pseudocode/config) Services Covered: 14 (PostgreSQL, Redis, LiteLLM, Dify, n8n, Open WebUI, Flowise, Qdrant, Caddy, Prometheus, Grafana, SuperTokens, Ollama, \+ supporting scripts) Config Files: 25+ generated files Scripts Generated: 12 operational scripts Env Variables: 80+ tracked Docker Containers: 14+ orchestrated

\#\#\# How to Use This Document

1. Feed Part 1-2 to an LLM to generate Script 1 (system prep)  
2. Feed Part 1 \+ Parts 3-8 to generate Script 2 (deployment)  
3. Each section is self-contained enough for targeted regeneration  
4. The pseudocode is implementation-language-agnostic but optimized for bash script generation  
5. Variable references (${VAR}) are consistent throughout â€” they all trace back to master.env

\#\#\# Version

Document Version: 1.0.0 Created: 2025 Target: Ubuntu 22.04/24.04 LTS Architecture: Single-server, Docker-based

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
              END OF SPECIFICATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

---

**This concludes all 8 parts of the complete specification document.**

The entire document provides a comprehensive blueprint covering:

Copy table

| Part | Sections | Content |
| ----- | ----- | ----- |
| **1** | 1â€“4 | Architecture, philosophy, prerequisites, Script 1 structure |
| **2** | 5â€“8 | Script 1 execution phases, hardware detection, Docker/Ollama install |
| **3** | 9â€“12 | Script 2 overview, phase execution, interactive questionnaire |
| **4** | 13â€“16 | Credential generation, master.env builder, service env files |
| **5** | 17â€“20 | PostgreSQL init, Redis config, LiteLLM routing, budget system |
| **6** | 21â€“24 | Docker Compose builder, Caddyfile, monitoring configs |
| **7** | 25â€“28 | Backup/restore, convenience scripts, n8n workflows, troubleshooting |
| **8** | 29â€“33 | File manifest, env reference, port map, checklist, architecture diagrams |


