# **ðŸ¦ž AI PLATFORM DEPLOYMENT GUIDE v75.2.0**

**Complete Self-Hosted AI Infrastructure**  
*Production-Ready | GPU/CPU Adaptive | Privacy-First | Enterprise-Grade*

---

## **ðŸ“‹ TABLE OF CONTENTS**

1. System Overview  
2. Architecture Diagram  
3. Service Inventory  
4. Storage Architecture  
5. Network & Access Patterns  
6. Hardware Requirements  
7. Script 0: Cleanup & Dependencies  
8. Script 1: System Setup & Configuration  
9. Script 2: Service Deployment  
10. Script 3: Service Configuration  
11. Script 4: Add Optional Services  
12. Troubleshooting  
13. Maintenance & Operations  
14. Security Considerations  
15. Changelog

---

## **ðŸŽ¯ SYSTEM OVERVIEW**

### **What This Deploys**

A **complete AI infrastructure platform** combining:

* **Local LLM inference** (Ollama with GPU/CPU support)  
* **API routing layer** (LiteLLM: simple queries â†’ local, complex â†’ cloud APIs)  
* **AI applications** (Dify, Anything LLM, Open WebUI)  
* **Autonomous coding agent** (OpenClaw via Tailscale)  
* **Workflow automation** (n8n, Flowise)  
* **Vector databases** (Qdrant/Weaviate/Chroma \- user selects one)  
* **Monitoring stack** (Prometheus \+ Grafana)  
* **Secure networking** (Tailscale VPN \+ reverse proxy)  
* **Object storage** (MinIO S3-compatible)

### **Key Design Principles**

1. **Privacy-First**: All data stays on your infrastructure  
2. **GPU/CPU Adaptive**: Automatically detects and optimizes for available hardware  
3. **Hybrid Intelligence**: Local models for speed, cloud APIs for complexity  
4. **Production-Ready**: Health checks, monitoring, automated backups  
5. **Modular**: Enable/disable services based on needs  
6. **Secure by Default**: Tailscale VPN, encrypted storage, credential isolation

---

## **ðŸ—ï¸ ARCHITECTURE DIAGRAM**

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚                         EXTERNAL ACCESS LAYER                            â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚                                                                           â”‚  
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  
â”‚  â”‚  Tailscale VPN    â”‚                    â”‚   Public Domain/IP       â”‚  â”‚  
â”‚  â”‚  100.x.x.x        â”‚                    â”‚   ai.jglaine.com         â”‚  â”‚  
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  
â”‚           â”‚                                            â”‚                 â”‚  
â”‚           â”‚ Port 18789                                 â”‚ Port 80/443     â”‚  
â”‚           â”‚ (OpenClaw ONLY)                            â”‚ (All Others)    â”‚  
â”‚           â”‚                                            â”‚                 â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
            â”‚                                            â”‚  
            â”‚                                            â–¼  
            â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
            â”‚                              â”‚   REVERSE PROXY         â”‚  
            â”‚                              â”‚   (Caddy OR nginx)      â”‚  
            â”‚                              â”‚   Ports: 80, 443        â”‚  
            â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
            â”‚                                           â”‚  
            â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
            â”‚                              â”‚  Load Balancing &       â”‚  
            â”‚                              â”‚  SSL/TLS Termination    â”‚  
            â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
            â”‚                                           â”‚  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚           â”‚              APPLICATION LAYER            â”‚                 â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚           â”‚                                           â”‚                 â”‚  
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  
â”‚  â”‚   OpenClaw       â”‚                    â”‚   Open WebUI         â”‚      â”‚  
â”‚  â”‚   Port: 18789    â”‚â—„â”€â”€â”€Tailscale       â”‚   Port: 3003         â”‚      â”‚  
â”‚  â”‚   (Direct Only)  â”‚    Only            â”‚   (Proxied)          â”‚      â”‚  
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  
â”‚                                                                          â”‚  
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  
â”‚  â”‚   Dify           â”‚  â”‚  Anything LLM    â”‚  â”‚   Flowise        â”‚     â”‚  
â”‚  â”‚   Port: 3001     â”‚  â”‚  Port: 3002      â”‚  â”‚   Port: 3004     â”‚     â”‚  
â”‚  â”‚   (Proxied)      â”‚  â”‚  (Proxied)       â”‚  â”‚   (Proxied)      â”‚     â”‚  
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  
â”‚           â”‚                     â”‚                      â”‚                â”‚  
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  
â”‚                                 â”‚                                       â”‚  
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚  
â”‚                     â”‚      LiteLLM         â”‚                            â”‚  
â”‚                     â”‚   (Routing Layer)    â”‚                            â”‚  
â”‚                     â”‚   Port: 4000         â”‚                            â”‚  
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚  
â”‚                                 â”‚                                       â”‚  
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚  
â”‚                    â”‚                         â”‚                          â”‚  
â”‚            Simple Queries              Complex Queries                  â”‚  
â”‚            (Fast/Local)                (Cloud APIs)                     â”‚  
â”‚                    â”‚                         â”‚                          â”‚  
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚  
â”‚           â”‚     Ollama       â”‚      â”‚  External APIs   â”‚               â”‚  
â”‚           â”‚  Port: 11434     â”‚      â”‚  \- OpenAI GPT-4  â”‚               â”‚  
â”‚           â”‚  GPU/CPU Adaptiveâ”‚      â”‚  \- Claude        â”‚               â”‚  
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  \- Gemini        â”‚               â”‚  
â”‚                                     â”‚  \- Groq          â”‚               â”‚  
â”‚                                     â”‚  \- DeepSeek      â”‚               â”‚  
â”‚                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
            â”‚                                           â”‚  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚           â”‚              DATA & STORAGE LAYER         â”‚                 â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚           â”‚                                           â”‚                 â”‚  
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  
â”‚  â”‚   PostgreSQL     â”‚  â”‚   Redis          â”‚  â”‚   MinIO          â”‚     â”‚  
â”‚  â”‚   Port: 5432     â”‚  â”‚   Port: 6379     â”‚  â”‚   Port: 9000/1   â”‚     â”‚  
â”‚  â”‚   (Primary DB)   â”‚  â”‚   (Cache)        â”‚  â”‚   (Object Store) â”‚     â”‚  
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  
â”‚                                                                          â”‚  
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  
â”‚  â”‚           Vector Database (User Selects ONE)                     â”‚  â”‚  
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚  â”‚  
â”‚  â”‚  â”‚  Qdrant    â”‚  â”‚  Weaviate  â”‚  â”‚  Chroma    â”‚                 â”‚  â”‚  
â”‚  â”‚  â”‚  Port:6333 â”‚  â”‚  Port:8080 â”‚  â”‚  Port:8000 â”‚                 â”‚  â”‚  
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚  â”‚  
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
            â”‚                                           â”‚  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚           â”‚          MONITORING & AUTOMATION          â”‚                 â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚           â”‚                                           â”‚                 â”‚  
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  
â”‚  â”‚   Prometheus     â”‚  â”‚   Grafana        â”‚  â”‚   n8n            â”‚     â”‚  
â”‚  â”‚   Port: 9090     â”‚  â”‚   Port: 3000     â”‚  â”‚   Port: 5678     â”‚     â”‚  
â”‚  â”‚   (Metrics)      â”‚  â”‚   (Dashboards)   â”‚  â”‚   (Workflows)    â”‚     â”‚  
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  
â”‚                                                                          â”‚  
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚  
â”‚  â”‚   Signal-CLI     â”‚   (Notifications to phone)                        â”‚  
â”‚  â”‚   (No port)      â”‚                                                   â”‚  
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
            â”‚  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚                         PERSISTENT STORAGE                               â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚                                                                           â”‚  
â”‚  Configuration (System Disk)                Growing Data (/mnt/data)    â”‚  
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚  
â”‚  $ROOT\_PATH (../scripts)/                 /mnt/data/ai-platform/      â”‚  
â”‚  â”œâ”€â”€ scripts/                               â”œâ”€â”€ volumes/                â”‚  
â”‚  â”‚   â”œâ”€â”€ 0-cleanup-environment.sh           â”‚   â”œâ”€â”€ postgres/           â”‚  
â”‚  â”‚   â”œâ”€â”€ 1-setup-system.sh                  â”‚   â”œâ”€â”€ redis/              â”‚  
â”‚  â”‚   â”œâ”€â”€ 2-deploy-services.sh               â”‚   â”œâ”€â”€ ollama/             â”‚  
â”‚  â”‚   â”œâ”€â”€ 3-configure-services.sh            â”‚   â”œâ”€â”€ minio/              â”‚  
â”‚  â”‚   â””â”€â”€ 4-add-services.sh                  â”‚   â”œâ”€â”€ qdrant/             â”‚  
â”‚  â””â”€â”€ deployment/                            â”‚   â”œâ”€â”€ n8n/                â”‚  
â”‚      â”œâ”€â”€ stack/                             â”‚   â””â”€â”€ prometheus/         â”‚  
â”‚      â”‚   â”œâ”€â”€ docker-compose.yml             â”œâ”€â”€ openclaw/               â”‚  
â”‚      â”‚   â””â”€â”€ docker-compose.override.yml    â”‚   â”œâ”€â”€ projects/           â”‚  
â”‚      â”œâ”€â”€ configs/                           â”‚   â”œâ”€â”€ conversations/      â”‚  
â”‚      â”‚   â”œâ”€â”€ Caddyfile                      â”‚   â””â”€â”€ artifacts/          â”‚  
â”‚      â”‚   â”œâ”€â”€ litellm\_config.yaml            â”œâ”€â”€ logs/                   â”‚  
â”‚      â”‚   â”œâ”€â”€ prometheus.yml                 â”‚   â”œâ”€â”€ caddy/              â”‚  
â”‚      â”‚   â””â”€â”€ openclaw/                      â”‚   â”œâ”€â”€ ollama/             â”‚  
â”‚      â”‚       â””â”€â”€ config.json                â”‚   â””â”€â”€ litellm/            â”‚  
â”‚      â””â”€â”€ .secrets/                          â”œâ”€â”€ backups/                â”‚  
â”‚          â”œâ”€â”€ .env (encrypted)               â”‚   â”œâ”€â”€ postgres/           â”‚  
â”‚          â”œâ”€â”€ api\_keys.enc                   â”‚   â””â”€â”€ vector\_db/          â”‚  
â”‚          â”œâ”€â”€ gdrive\_token.json              â””â”€â”€ gdrive/                 â”‚  
â”‚          â””â”€â”€ signal\_credentials.json            â””â”€â”€ (synced from cloud) â”‚  
â”‚                                                                           â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LEGEND:  
  â”€â”€â–º Direct connection (no proxy)  
  â•â•â•â–º Proxied through Caddy/nginx  
  â”„â”„â–º Optional integration  
  \[GPU\] GPU-accelerated when available  
  \[CPU\] CPU fallback mode

---

## **ðŸ“¦ SERVICE INVENTORY**

### **Total Services: 19**

Copy table

| \# | Service | Port | Access Method | Purpose | Required |
| ----- | ----- | ----- | ----- | ----- | ----- |
| 1 | **Tailscale** | 8443 | VPN mesh network | Secure access to OpenClaw | Yes |
| 2 | **Caddy** OR **nginx** | 80/443 | N/A (is the proxy) | Reverse proxy for all services except OpenClaw | Yes (pick one) |
| 3 | **PostgreSQL** | 5432 | Internal only | Primary relational database | Yes |
| 4 | **Redis** | 6379 | Internal only | Caching layer | Yes |
| 5 | **MinIO** | 9000/9001 | Proxied via domain | S3-compatible object storage | Yes |
| 6 | **Prometheus** | 9090 | Proxied via domain | Metrics collection | Yes |
| 7 | **Grafana** | 3000 | Proxied via domain | Monitoring dashboards | Yes |
| 8 | **Ollama** | 11434 | Internal \+ proxied | Local LLM inference (GPU/CPU) | Yes |
| 9 | **LiteLLM** | 4000 | Internal only | Routing: simpleâ†’local, complexâ†’APIs | Yes |
| 10 | **OpenClaw** | 18789 | **Tailscale IP ONLY** | AI coding agent | Yes |
| 11 | **Dify** | 3001 | Proxied via domain | LLM application builder | Yes |
| 12 | **Anything LLM** | 3002 | Proxied via domain | Document chat with RAG | Yes |
| 13 | **Open WebUI** | 3003 | Proxied via domain | litelllm web interface | Yes |
| 14 | **Qdrant** | 6333 | Internal \+ proxied | Vector database (option 1\) | Pick ONE |
| 15 | **Weaviate** | 8080 | Internal \+ proxied | Vector database (option 2\) | Pick ONE |
| 16 | **Chroma** | 8000 | Internal \+ proxied | Vector database (option 3\) | Pick ONE |
| 17 | **n8n** | 5678 | Proxied via domain | Workflow automation | Yes |
| 18 | **Flowise** | 3004 | Proxied via domain | Visual LLM flow builder | Optional |
| 19 | **Signal-CLI** | None | Internal only | Phone notifications | Optional |

### **Access Pattern Summary**

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚  SERVICE ACCESS METHODS                                     â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚                                                             â”‚  
â”‚  OpenClaw (Port 18789):                                     â”‚  
â”‚    âœ“ Tailscale IP: http://100.64.0.5:18789                 â”‚  
â”‚    âœ— NOT via domain                                         â”‚  
â”‚    âœ— NOT proxied                                            â”‚  
â”‚                                                             â”‚  
â”‚  All Other Services:                                        â”‚  
â”‚    âœ“ Domain: https://ai.jglaine.com/service-name           â”‚  
â”‚    âœ“ Proxied via Caddy or nginx                            â”‚  
â”‚    âœ“ SSL/TLS (if domain configured)                        â”‚  
â”‚    âœ“ HTTP fallback (if no domain)                          â”‚  
â”‚                                                             â”‚  
â”‚  Examples:                                                  â”‚  
â”‚    https://ai.jglaine.com/webui    â†’ Open WebUI           â”‚  
â”‚    https://ai.jglaine.com/dify     â†’ Dify                  â”‚  
â”‚    https://ai.jglaine.com/n8n      â†’ n8n                   â”‚  
â”‚    https://ai.jglaine.com/grafana  â†’ Grafana               â”‚  
â”‚                                                             â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## **ðŸ’¾ STORAGE ARCHITECTURE**

### **Design Philosophy**

**Separation of Concerns**: Static configuration vs. growing data

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚  STORAGE TIER 1: Configuration (System Disk)               â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚  Location: $ROOT\_PATH (../scripts)/                      â”‚  
â”‚  Purpose: Scripts, configs, credentials                     â”‚  
â”‚  Size: \~20 MB (static)                                      â”‚  
â”‚  Backup: Critical (includes .secrets/)                      â”‚  
â”‚                                                             â”‚  
â”‚  Structure:                                                 â”‚  
â”‚  â”œâ”€â”€ scripts/                                               â”‚  
â”‚  â”‚   â”œâ”€â”€ 0-cleanup-environment.sh                          â”‚  
â”‚  â”‚   â”œâ”€â”€ 1-setup-system.sh                                 â”‚  
â”‚  â”‚   â”œâ”€â”€ 2-deploy-services.sh                              â”‚  
â”‚  â”‚   â”œâ”€â”€ 3-configure-services.sh                           â”‚  
â”‚  â”‚   â””â”€â”€ 4-add-services.sh                                 â”‚  
â”‚  â””â”€â”€ deployment/                                            â”‚  
â”‚      â”œâ”€â”€ stack/                                             â”‚  
â”‚      â”‚   â”œâ”€â”€ docker-compose.yml                            â”‚  
â”‚      â”‚   â””â”€â”€ docker-compose.override.yml (if GPU)          â”‚  
â”‚      â”œâ”€â”€ configs/                                           â”‚  
â”‚      â”‚   â”œâ”€â”€ Caddyfile                                     â”‚  
â”‚      â”‚   â”œâ”€â”€ litellm\_config.yaml                           â”‚  
â”‚      â”‚   â”œâ”€â”€ prometheus.yml                                â”‚  
â”‚      â”‚   â””â”€â”€ openclaw/                                      â”‚  
â”‚      â”‚       â””â”€â”€ config.json                               â”‚  
â”‚      â””â”€â”€ .secrets/ (encrypted)                             â”‚  
â”‚          â”œâ”€â”€ .env                                           â”‚  
â”‚          â”œâ”€â”€ api\_keys.enc                                   â”‚  
â”‚          â”œâ”€â”€ gdrive\_token.json (if Google Drive enabled)   â”‚  
â”‚          â””â”€â”€ signal\_credentials.json (if Signal enabled)   â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚  STORAGE TIER 2: Growing Data (/mnt/data preferred)        â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚  Location: /mnt/data/ai-platform/ (or user-specified)      â”‚  
â”‚  Purpose: Databases, logs, artifacts, models                â”‚  
â”‚  Size: 50 GB \- 2 TB+ (grows over time)                     â”‚  
â”‚  Backup: Important (especially volumes/)                    â”‚  
â”‚                                                             â”‚  
â”‚  Structure:                                                 â”‚  
â”‚  â”œâ”€â”€ volumes/ (Docker persistent storage)                  â”‚  
â”‚  â”‚   â”œâ”€â”€ postgres/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 5-50 GB (queries, embeddings)â”‚  
â”‚  â”‚   â”œâ”€â”€ redis/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 100 MB \- 5 GB (cache)      â”‚  
â”‚  â”‚   â”œâ”€â”€ ollama/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 10-100 GB (LLM models)     â”‚  
â”‚  â”‚   â”œâ”€â”€ minio/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 1-500 GB (objects)         â”‚  
â”‚  â”‚   â”œâ”€â”€ qdrant/ (or weaviate/chroma) â–º 1-50 GB (vectors) â”‚  
â”‚  â”‚   â”œâ”€â”€ n8n/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º 100 MB \- 2 GB (workflows)  â”‚  
â”‚  â”‚   â””â”€â”€ prometheus/ â”€â”€â”€â”€â”€â”€â”€â”€â–º 1-10 GB (metrics)          â”‚  
â”‚  â”œâ”€â”€ openclaw/ (AI coding agent workspace)                 â”‚  
â”‚  â”‚   â”œâ”€â”€ projects/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º User projects, generated codeâ”‚  
â”‚  â”‚   â”œâ”€â”€ conversations/ â”€â”€â”€â”€â–º Chat history (.md files)    â”‚  
â”‚  â”‚   â””â”€â”€ artifacts/ â”€â”€â”€â”€â”€â”€â”€â”€â–º Build outputs, temp files   â”‚  
â”‚  â”œâ”€â”€ logs/ (application logs)                              â”‚  
â”‚  â”‚   â”œâ”€â”€ caddy/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Access logs, errors        â”‚  
â”‚  â”‚   â”œâ”€â”€ ollama/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Inference logs             â”‚  
â”‚  â”‚   â””â”€â”€ litellm/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Routing decisions          â”‚  
â”‚  â”œâ”€â”€ backups/ (automated backups)                          â”‚  
â”‚  â”‚   â”œâ”€â”€ postgres/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Daily pg\_dump files        â”‚  
â”‚  â”‚   â””â”€â”€ vector\_db/ â”€â”€â”€â”€â”€â”€â”€â”€â–º Vector index snapshots      â”‚  
â”‚  â””â”€â”€ gdrive/ (if Google Drive sync enabled)                â”‚  
â”‚      â””â”€â”€ (synced documents for Anything LLM ingestion)     â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

### **Storage Detection Logic (Script 1\)**

\# Script 1 automatically detects best storage location

Scenario 1: /mnt/data exists, writable, \>50GB free  
  â†’ Use /mnt/data/ai-platform/ for DATA\_ROOT  
  â†’ Use ../script/ for CONFIG\_ROOT(where the script is executed)

Scenario 2: /mnt/data doesn't exist  
  â†’ Prompt user for alternative  
  â†’ Default:../script/ for CONFIG\_ROOT  
  â†’ Warn if system disk \< 100 GB free

Scenario 3: /mnt/data exists but not writable  
  â†’ Attempt sudo chown jglaine:jglaine /mnt/data  
  â†’ If successful: Use /mnt/data  
  â†’ If fails: Fall back to Scenario 2

Scenario 4: User wants custom location  
  â†’ Accept custom path  
  â†’ Validate: exists, writable, sufficient space  
  â†’ Update DATA\_ROOT in .env

### **Environment Variables**

\# In deployment/.secrets/.env

\# Storage paths  
CONFIG\_ROOT="$ROOT\_PATH (../scripts)"  
DATA\_ROOT="/mnt/data/ai-platform"

\# Docker uses these for bind mounts  
POSTGRES\_DATA="${DATA\_ROOT}/volumes/postgres"  
REDIS\_DATA="${DATA\_ROOT}/volumes/redis"  
OLLAMA\_DATA="${DATA\_ROOT}/volumes/ollama"  
OPENCLAW\_WORKSPACE="${DATA\_ROOT}/openclaw"  
LOGS\_DIR="${DATA\_ROOT}/logs"  
BACKUPS\_DIR="${DATA\_ROOT}/backups"

---

## **ðŸŒ NETWORK & ACCESS PATTERNS**

### **External Access Architecture**

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚  USER DEVICE                                                â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚                                                             â”‚  
â”‚  For OpenClaw:                                              â”‚  
â”‚    1\. Connect to Tailscale VPN                             â”‚  
â”‚    2\. Access: http://100.64.0.5:18789                      â”‚  
â”‚    3\. No proxy, no domain, direct connection               â”‚  
â”‚                                                             â”‚  
â”‚  For All Other Services:                                    â”‚  
â”‚    1\. Open browser: https://ai.jglaine.com                 â”‚  
â”‚    2\. Caddy/nginx proxies to appropriate service           â”‚  
â”‚    3\. SSL/TLS handled automatically (if domain configured) â”‚  
â”‚                                                             â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
         â”‚                                 â”‚  
         â”‚ Tailscale                       â”‚ HTTPS/HTTP  
         â”‚ (encrypted)                     â”‚ (port 443/80)  
         â–¼                                 â–¼  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚  Tailscale       â”‚          â”‚  Reverse Proxy           â”‚  
â”‚  Mesh Network    â”‚          â”‚  (Caddy OR nginx)        â”‚  
â”‚  IP: 100.x.x.x   â”‚          â”‚  Ports: 80, 443          â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  
         â”‚                             â”‚  
         â”‚ Port 18789                  â”‚ Internal routing  
         â”‚ (OpenClaw only)             â”‚ (all other services)  
         â–¼                             â–¼  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚  OpenClaw        â”‚          â”‚  Service Mesh            â”‚  
â”‚  Container       â”‚          â”‚  \- Open WebUI :3003      â”‚  
â”‚  (Direct)        â”‚          â”‚  \- Dify       :3001      â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  \- Anything   :3002      â”‚  
                              â”‚  \- n8n        :5678      â”‚  
                              â”‚  \- Grafana    :3000      â”‚  
                              â”‚  \- Flowise    :3004      â”‚  
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

### **Reverse Proxy Configuration**

#### **Option 1: Caddy (Recommended)**

**Pros**:

* Automatic HTTPS with Let's Encrypt  
* Simpler configuration syntax  
* Auto-renewal of certificates  
* Built-in security headers

**Caddyfile Example** (generated by Script 1):

\# AI Platform \- Caddy Configuration  
\# Generated by 1-setup-system.sh v75.2.0

{  
    email admin@jglaine.com  
    \# Global options  
}

\# Main domain entry  
ai.jglaine.com {  
    \# Serve a simple landing page at root  
    respond "AI Platform \- Service Directory" 200  
}

\# Open WebUI  
ai.jglaine.com/webui/\* {  
    reverse\_proxy localhost:3003  
}

\# Dify  
ai.jglaine.com/dify/\* {  
    reverse\_proxy localhost:3001  
}

\# Anything LLM  
ai.jglaine.com/anything/\* {  
    reverse\_proxy localhost:3002  
}

\# n8n  
ai.jglaine.com/n8n/\* {  
    reverse\_proxy localhost:5678  
}

\# Grafana  
ai.jglaine.com/grafana/\* {  
    reverse\_proxy localhost:3000  
}

\# Flowise  
ai.jglaine.com/flowise/\* {  
    reverse\_proxy localhost:3004  
}

\# Ollama API (for external access)  
ai.jglaine.com/ollama/\* {  
    reverse\_proxy localhost:11434  
}

\# MinIO Console  
ai.jglaine.com/minio/\* {  
    reverse\_proxy localhost:9001  
}

\# Vector DB (example: Qdrant)  
ai.jglaine.com/qdrant/\* {  
    reverse\_proxy localhost:6333  
}

\# Prometheus  
ai.jglaine.com/prometheus/\* {  
    reverse\_proxy localhost:9090  
}

\# Security headers for all routes  
header {  
    X-Frame-Options "SAMEORIGIN"  
    X-Content-Type-Options "nosniff"  
    X-XSS-Protection "1; mode=block"  
    Referrer-Policy "strict-origin-when-cross-origin"  
}

#### **Option 2: nginx**

**Pros**:

* More fine-grained control  
* Better performance at extreme scale  
* Mature ecosystem

**nginx.conf Example** (generated by Script 1):

\# AI Platform \- nginx Configuration  
\# Generated by 1-setup-system.sh v75.2.0

http {  
    \# Rate limiting  
    limit\_req\_zone $binary\_remote\_addr zone=api\_limit:10m rate=10r/s;

    \# SSL configuration (requires manual cert management)  
    ssl\_protocols TLSv1.2 TLSv1.3;  
    ssl\_ciphers HIGH:\!aNULL:\!MD5;  
    ssl\_prefer\_server\_ciphers on;

    \# Main server block  
    server {  
        listen 80;  
        listen 443 ssl http2;  
        server\_name ai.jglaine.com;

        ssl\_certificate /etc/nginx/ssl/fullchain.pem;  
        ssl\_certificate\_key /etc/nginx/ssl/privkey.pem;

        \# Redirect HTTP to HTTPS  
        if ($scheme \= http) {  
            return 301 https://$server\_name$request\_uri;  
        }

        \# Open WebUI  
        location /webui/ {  
            proxy\_pass http://localhost:3003/;  
            proxy\_http\_version 1.1;  
            proxy\_set\_header Upgrade $http\_upgrade;  
            proxy\_set\_header Connection 'upgrade';  
            proxy\_set\_header Host $host;  
            proxy\_cache\_bypass $http\_upgrade;  
        }

        \# Dify  
        location /dify/ {  
            proxy\_pass http://localhost:3001/;  
            proxy\_set\_header Host $host;  
            proxy\_set\_header X-Real-IP $remote\_addr;  
        }

        \# Anything LLM  
        location /anything/ {  
            proxy\_pass http://localhost:3002/;  
        }

        \# n8n  
        location /n8n/ {  
            proxy\_pass http://localhost:5678/;  
        }

        \# Grafana  
        location /grafana/ {  
            proxy\_pass http://localhost:3000/;  
        }

        \# Flowise  
        location /flowise/ {  
            proxy\_pass http://localhost:3004/;  
        }

        \# Ollama API  
        location /ollama/ {  
            proxy\_pass http://localhost:11434/;  
            limit\_req zone=api\_limit burst=20;  
        }

        \# Security headers  
        add\_header X-Frame-Options "SAMEORIGIN" always;  
        add\_header X-Content-Type-Options "nosniff" always;  
        add\_header X-XSS-Protection "1; mode=block" always;  
    }  
}

### **Service Selection in Script 1**

\[?\] Select reverse proxy \[1\]:  
    1\. Caddy (automatic HTTPS, easier configuration)  
    2\. nginx (more control, manual SSL management)

â†’ Selected: Caddy

âœ“ Generating Caddyfile...  
âœ“ Caddy will automatically obtain Let's Encrypt certificates  
âœ“ Certificate renewal: automatic (every 30 days)

---

## **ðŸ–¥ï¸ HARDWARE REQUIREMENTS**

### **Minimum Requirements (CPU-Only)**

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚  MINIMUM SPECS (Development/Testing)                        â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚  CPU:      4 cores (x86\_64)                                 â”‚  
â”‚  RAM:      16 GB                                            â”‚  
â”‚  Storage:  100 GB SSD (system \+ data combined)             â”‚  
â”‚  Network:  100 Mbps                                         â”‚  
â”‚  OS:       Ubuntu 22.04+ / Debian 12+ / Rocky Linux 9+     â”‚  
â”‚                                                             â”‚  
â”‚  Performance Expectations:                                  â”‚  
â”‚    \- Ollama (7B models): 2-5 tokens/sec                    â”‚  
â”‚    \- Concurrent users: 1-2                                  â”‚  
â”‚    \- Context limit: 4K tokens                               â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

### **Recommended (GPU-Accelerated)**

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚  RECOMMENDED SPECS (Production)                             â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚  CPU:      8+ cores (x86\_64)                                â”‚  
â”‚  RAM:      32 GB                                            â”‚  
â”‚  GPU:      NVIDIA RTX 3060 (12GB VRAM) or better           â”‚  
â”‚  Storage:  System: 100 GB SSD                               â”‚  
â”‚            Data (/mnt/data): 500 GB \- 2 TB SSD/NVMe        â”‚  
â”‚  Network:  1 Gbps                                           â”‚  
â”‚  OS:       Ubuntu 22.04 LTS (best NVIDIA driver support)   â”‚  
â”‚                                                             â”‚  
â”‚  Performance Expectations:                                  â”‚  
â”‚    \- Ollama (13B models): 20-40 tokens/sec                 â”‚  
â”‚    \- Concurrent users: 5-10                                 â”‚  
â”‚    \- Context limit: 32K+ tokens                             â”‚  
â”‚    \- GPU utilization: 80-95%                                â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

### **Optimal (Enterprise/Heavy Usage)**

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  
â”‚  OPTIMAL SPECS (Enterprise/Team)                            â”‚  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚  CPU:      16+ cores (AMD EPYC or Intel Xeon)              â”‚  
â”‚  RAM:      64-128 GB ECC                                    â”‚  
â”‚  GPU:      NVIDIA A100 (40GB) or RTX 4090 (24GB)           â”‚  
â”‚  Storage:  System: 250 GB NVMe                              â”‚  
â”‚            Data: 2-4 TB NVMe RAID                           â”‚  
â”‚  Network:  10 Gbps                                          â”‚  
â”‚  OS:       Ubuntu 22.04 LTS Server                          â”‚  
â”‚                                                             â”‚  
â”‚  Performance Expectations:                                  â”‚  
â”‚    \- Ollama (70B models): 30-60 tokens/sec                 â”‚  
â”‚    \- Concurrent users: 20-50                                â”‚  
â”‚    \- Context limit: 128K tokens                             â”‚  
â”‚    \- Multi-model parallel inference                         â”‚  
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

### **GPU vs CPU Decision Matrix**

Copy table

| Use Case | CPU Sufficient | GPU Recommended | GPU Required |
| ----- | ----- | ----- | ----- |
| Personal experimentation | âœ… |  |  |
| Code generation (small projects) | âœ… | âœ… |  |
| Document Q\&A (RAG, \<100 docs) | âœ… | âœ… |  |
| Multi-user team (5+ users) |  | âœ… | âœ… |
| Large context (\>8K tokens) |  | âœ… | âœ… |
| 13B+ parameter models |  | âœ… | âœ… |
| Real-time response (\<1s latency) |  | âœ… | âœ… |
| Production workloads |  | âœ… | âœ… |

### **Storage Sizing Guide**

Service               Minimum    Typical    Heavy Use  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
PostgreSQL            2 GB       10 GB      50 GB  
Redis                 500 MB     2 GB       10 GB  
Ollama models         10 GB      30 GB      100 GB  
MinIO objects         5 GB       50 GB      500 GB  
Vector DB (Qdrant)    1 GB       10 GB      50 GB  
OpenClaw workspace    1 GB       10 GB      50 GB  
Logs                  1 GB       5 GB       20 GB  
Backups               5 GB       20 GB      100 GB  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
TOTAL DATA TIER       25 GB      137 GB     880 GB

Configuration         \~20 MB (static)  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
GRAND TOTAL           \~25 GB     \~140 GB    \~900 GB

---

## **ðŸ§¹ SCRIPT 0: CLEANUP & DEPENDENCIES**

**Purpose**: Remove any existing AI Platform deployment, install system dependencies, detect and configure GPU support.

### **Expected Output**

$ bash 0-cleanup-environment.sh

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ¦ž AI PLATFORM \- ENVIRONMENT CLEANUP & PREPARATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
Version: v75.2.0  
Host: homelab.local (192.168.1.100)  
User: jglaine  
Date: 2025-02-07 14:32:15 UTC

This script will:  
  1\. Stop and remove existing AI Platform containers  
  2\. Delete deployment directories and volumes  
  3\. Install system dependencies (Docker, Docker Compose, etc.)  
  4\. Detect and configure GPU support (if available)  
  5\. Configure Docker rootless mode  
  6\. Prepare system for fresh deployment

âš ï¸  WARNING: This will DELETE all existing AI Platform data\!  
    \- Docker containers and images  
    \- Volumes in /mnt/data/ai-platform/ (if exists)  
    \- Volumes in $ROOT\_PATH (../scripts)/ (if exists)  
    \- Configuration files  
      
ðŸ”’ Credentials and backups will be preserved if found in:  
    \- deployment/.secrets/ (backed up to \~/ai-platform-backup-TIMESTAMP/)

\[?\] Proceed with cleanup? \[y/N\]: y

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 1/7: DETECTING EXISTING DEPLOYMENT  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Scanning for AI Platform containers...  
  âœ“ Found 12 running containers with label 'ai-platform'  
    
â†’ Scanning for deployment directories...  
  âœ“ Found: $ROOT\_PATH (../scripts)/deployment/  
  âœ“ Found: /mnt/data/ai-platform/

â†’ Checking for credentials...  
  âœ“ Found: $ROOT\_PATH (../scripts)/deployment/.secrets/.env  
  âœ“ Found: $ROOT\_PATH (../scripts)/deployment/.secrets/api\_keys.enc

â†’ Creating backup...  
  âœ“ Backup created: $ROOT\_PATH (../scripts)-backup-20250207-143220/  
  âœ“ Preserved: .env, api\_keys.enc, gdrive\_token.json

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 2/7: STOPPING SERVICES  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Stopping Docker containers...  
  âœ“ openclaw-ai-platform      (stopped)  
  âœ“ open-webui-ai-platform    (stopped)  
  âœ“ dify-ai-platform          (stopped)  
  âœ“ anything-llm-ai-platform  (stopped)  
  âœ“ ollama-ai-platform        (stopped)  
  âœ“ litellm-ai-platform       (stopped)  
  âœ“ postgres-ai-platform      (stopped)  
  âœ“ redis-ai-platform         (stopped)  
  âœ“ qdrant-ai-platform        (stopped)  
  âœ“ n8n-ai-platform           (stopped)  
  âœ“ caddy-ai-platform         (stopped)  
  âœ“ prometheus-ai-platform    (stopped)

â†’ Removing containers...  
  âœ“ All AI Platform containers removed (12 total)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 3/7: CLEANING VOLUMES & DATA  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Removing Docker volumes...  
  âœ“ ai-platform\_postgres\_data   (deleted, 8.2 GB freed)  
  âœ“ ai-platform\_redis\_data      (deleted, 1.1 GB freed)  
  âœ“ ai-platform\_ollama\_data     (deleted, 42.3 GB freed)  
  âœ“ ai-platform\_qdrant\_data     (deleted, 3.7 GB freed)  
  âœ“ ai-platform\_n8n\_data        (deleted, 512 MB freed)

â†’ Removing deployment directories...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/ (deleted)  
  âœ“ /mnt/data/ai-platform/ (deleted, 55.8 GB freed)

â†’ Preserving scripts directory...  
  âœ“ $ROOT\_PATH (../scripts)/scripts/ (kept)

Total space freed: 111.6 GB

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 4/7: DEPENDENCY CHECK & INSTALLATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Checking system dependencies...  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Core System Tools:  
  âœ“ curl           v7.81.0  
  âœ“ wget           v1.21.2  
  âœ“ git            v2.34.1  
  âœ“ jq             v1.6  
  âœ“ openssl        v3.0.2  
  âœ“ gpg            v2.2.27  
  âœ“ unzip          v6.0  
  âœ“ tar            v1.34  
  âœ“ awk            v1.3.4  
  âœ“ sed            v4.8

Docker Components:  
  âœ— docker         (not installed)  
  âœ— docker-compose (not installed)

â†’ Installing Docker Engine...  
  âœ“ Added Docker GPG key  
  âœ“ Added Docker repository  
  âœ“ Installing docker-ce docker-ce-cli containerd.io  
  âœ“ Docker installed: v27.3.1

â†’ Installing Docker Compose v2...  
  âœ“ Downloaded docker-compose v2.29.1  
  âœ“ Installed to /usr/local/bin/docker-compose  
  âœ“ Docker Compose installed: v2.29.1

â†’ Configuring Docker for user 'jglaine'...  
  âœ“ Added user to 'docker' group  
  âœ“ Docker rootless mode configured  
  âœ“ Docker daemon started  
  âš ï¸  Logout/login required for group changes to take effect

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 5/7: GPU DETECTION & CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Detecting NVIDIA GPU...  
  âœ“ NVIDIA GPU detected: NVIDIA GeForce RTX 3090  
  âœ“ Driver version: 535.129.03  
  âœ“ CUDA version: 12.2

â†’ Installing nvidia-docker2...  
  âœ“ Added NVIDIA Docker repository  
  âœ“ Installing nvidia-docker2 nvidia-container-toolkit  
  âœ“ nvidia-docker2 installed

â†’ Configuring Docker for GPU access...  
  âœ“ Updated /etc/docker/daemon.json with nvidia runtime  
  âœ“ Restarted Docker daemon  
  âœ“ Added user 'jglaine' to 'video' group

â†’ Testing GPU access in Docker...  
  âœ“ Test container launched  
  âœ“ GPU accessible: NVIDIA GeForce RTX 3090 (24GB VRAM)  
  âœ“ GPU configuration successful

â†’ Setting environment variables...  
  âœ“ GPU\_AVAILABLE=true  
  âœ“ GPU\_VENDOR=nvidia  
  âœ“ GPU\_MODEL=GeForce RTX 3090  
  âœ“ GPU\_VRAM=24GB  
  âœ“ CUDA\_VERSION=12.2

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 6/7: SYSTEM OPTIMIZATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Configuring system limits...  
  âœ“ vm.max\_map\_count \= 262144 (for vector DBs)  
  âœ“ fs.file-max \= 2097152  
  âœ“ User limits: nofile \= 65536

â†’ Configuring swap...  
  âœ“ Current swap: 8 GB  
  âš ï¸  Recommendation: 16 GB swap for 32 GB RAM  
  \[?\] Adjust swap size? \[y/N\]: n

â†’ Disabling unnecessary services...  
  âœ“ snapd disabled (saves \~200 MB RAM)  
  âœ“ systemd-resolved configured for Docker DNS

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 7/7: FINAL VALIDATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Verifying installation...  
  âœ“ Docker: v27.3.1 (running)  
  âœ“ Docker Compose: v2.29.1  
  âœ“ User 'jglaine' in 'docker' group  
  âœ“ User 'jglaine' in 'video' group  
  âœ“ GPU: NVIDIA GeForce RTX 3090 (accessible)  
  âœ“ nvidia-docker2: installed and configured

â†’ Disk space check...  
  âœ“ /home/jglaine: 387 GB free (sufficient)  
  âœ“ /mnt/data: 1.2 TB free (excellent)

â†’ Network connectivity...  
  âœ“ Internet: connected  
  âœ“ DNS: functional  
  âœ“ Docker registry: docker.io (reachable)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
âœ… CLEANUP & PREPARATION COMPLETE  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Summary:  
  âœ“ Existing deployment removed  
  âœ“ Dependencies installed  
  âœ“ GPU detected and configured (NVIDIA RTX 3090, 24GB VRAM)  
  âœ“ Docker ready (rootless mode)  
  âœ“ System optimized

Backup Location:  
  $ROOT\_PATH (../scripts)-backup-20250207-143220/

Next Steps:  
  1\. Review backup if needed  
  2\. \*REBOOT RECOMMENDED\* for group changes to take effect  
  3\. Run: bash 1-setup-system.sh

âš ï¸  IMPORTANT: A system reboot is recommended to ensure all  
    group memberships and GPU configurations are active.

\[?\] Reboot now? \[y/N\]: y

â†’ System will reboot in 10 seconds... (Ctrl+C to cancel)  
â†’ Rebooting...

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### **Alternative Output: CPU-Only System**

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 5/7: GPU DETECTION & CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Detecting NVIDIA GPU...  
  âŠ˜ No NVIDIA GPU detected (nvidia-smi not found)

â†’ CPU-only mode will be used  
  âœ“ GPU\_AVAILABLE=false  
  âœ“ Ollama will use CPU inference  
  âš ï¸  Performance: Expect 2-5 tokens/sec for 7B models

â†’ GPU-related tools will be skipped:  
  âŠ˜ nvidia-docker2 (not needed)  
  âŠ˜ CUDA toolkit (not needed)  
  âŠ˜ nvidia-container-toolkit (not needed)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### **Script 0 Success Criteria**

**GPU System Requirements**:

* NVIDIA GPU detected via nvidia-smi  
* CUDA version identified (12.x preferred)  
* nvidia-docker2 installed  
* Docker daemon configured with GPU runtime  
* User added to 'video' group  
* GPU accessible in test container  
* GPU\_AVAILABLE=true in environment

**CPU System Requirements**:

* No GPU detected (expected behavior)  
* GPU installation skipped  
* GPU\_AVAILABLE=false in environment  
* CPU-only mode confirmed

**Universal Requirements (Both GPU & CPU)**:

* Exit code 0 (success)  
* Docker Engine installed (v27.x+)  
* Docker Compose v2 installed (v2.29.x+)  
* Docker rootless mode configured  
* User in 'docker' group  
* All core dependencies installed (curl, wget, git, jq, etc.)  
* Deployment directory deleted (if existed)  
* Data directory deleted (if existed)  
* Scripts directory preserved  
* No AI Platform containers remain  
* No AI Platform volumes remain  
* Credentials backed up (if existed)  
* System optimized (vm.max\_map\_count, file limits)  
* System rebooted (if user confirmed)

---

## **âš™ï¸ SCRIPT 1: SYSTEM SETUP & CONFIGURATION**

**Purpose**: Generate all configuration files, create directory structure, collect user preferences (domain, API keys, model selection, etc.).

### **Expected Output**

$ bash 1-setup-system.sh

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ¦ž AI PLATFORM \- SYSTEM SETUP & CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
Version: v75.2.0  
Host: homelab.local (192.168.1.100)  
User: jglaine  
Date: 2025-02-07 15:10:42 UTC

This script will:  
  1\. Detect system environment (GPU, storage, network)  
  2\. Collect configuration preferences (domain, models, API keys)  
  3\. Generate .env file with all variables  
  4\. Create directory structure (CONFIG\_ROOT \+ DATA\_ROOT)  
  5\. Generate docker-compose.yml and service configs  
  6\. Set up reverse proxy (Caddy or nginx)  
  7\. Configure integrations (Google Drive, Signal, etc.)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 0/14: ENVIRONMENT DETECTION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Detecting system capabilities...  
  âœ“ OS: Ubuntu 22.04.3 LTS (Jammy Jellyfish)  
  âœ“ Kernel: 6.5.0-35-generic  
  âœ“ Architecture: x86\_64  
  âœ“ CPU: AMD Ryzen 9 5950X (32 threads)  
  âœ“ RAM: 64 GB (60 GB available)  
  âœ“ GPU: NVIDIA GeForce RTX 3090 (24GB VRAM, CUDA 12.2)  
  âœ“ Docker: v27.3.1 (rootless mode enabled)  
  âœ“ Docker Compose: v2.29.1

â†’ Detecting storage locations...  
  âœ“ /mnt/data exists (1.2 TB free, writable)  
  âœ“ /home/jglaine (387 GB free)

\[?\] Store growing data on /mnt/data? \[Y/n\]: Y

â†’ Storage configuration:  
  âœ“ CONFIG\_ROOT: $ROOT\_PATH (../scripts)  
  âœ“ DATA\_ROOT: /mnt/data/ai-platform

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 1/14: NETWORK CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

\[?\] Do you have a domain name? \[y/N\]: y

\[?\] Enter your domain (e.g., ai.jglaine.com): ai.jglaine.com

â†’ Validating domain...  
  âœ“ Domain resolves to: 192.168.1.100  
  âœ“ DNS configuration correct

\[?\] Enable HTTPS with Let's Encrypt? \[Y/n\]: Y

\[?\] Email for Let's Encrypt notifications: admin@jglaine.com

â†’ Domain configuration saved:  
  âœ“ DOMAIN=ai.jglaine.com  
  âœ“ HTTPS\_ENABLED=true  
  âœ“ LETSENCRYPT\_EMAIL=admin@jglaine.com

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 2/14: REVERSE PROXY SELECTION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

\[?\] Select reverse proxy \[1\]:  
    1\. Caddy (automatic HTTPS, simpler config)  
    2\. nginx (more control, manual cert management)

â†’ Selection: 1

â†’ Reverse proxy configuration:  
  âœ“ PROXY\_TYPE=caddy  
  âœ“ Automatic HTTPS: enabled  
  âœ“ Certificate renewal: automatic (every 30 days)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 3/14: TAILSCALE CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Tailscale is required for OpenClaw access

\[?\] Tailscale auth key: tskey-auth-xxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxx

â†’ Validating Tailscale key...  
  âœ“ Key valid  
  âœ“ Network: homelab-network

â†’ Tailscale configuration:  
  âœ“ TAILSCALE\_AUTHKEY=tskey-auth-xxxxxxxxxxxx-\*\*\*  
  âœ“ OpenClaw will be accessible at: http://\<tailscale-ip\>:18789  
  âš ï¸  Tailscale IP will be assigned during deployment (Phase 2-deploy)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 4/14: DATABASE CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Generating PostgreSQL credentials...  
  âœ“ POSTGRES\_USER=ai\_platform  
  âœ“ POSTGRES\_PASSWORD=\<generated 32-char secure password\>  
  âœ“ POSTGRES\_DB=ai\_platform\_db

â†’ Generating Redis configuration...  
  âœ“ REDIS\_PASSWORD=\<generated 32-char secure password\>

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 5/14: VECTOR DATABASE SELECTION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

\[?\] Select vector database \[1\]:  
    1\. Qdrant (recommended, easiest setup)  
    2\. Weaviate (advanced features, semantic search)  
    3\. Chroma (lightweight, good for small datasets)

â†’ Selection: 1

â†’ Vector database configuration:  
  âœ“ VECTOR\_DB=qdrant  
  âœ“ QDRANT\_PORT=6333  
  âœ“ QDRANT\_GRPC\_PORT=6334  
  âœ“ QDRANT\_API\_KEY=\<generated\>

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 6/14: OLLAMA MODEL SELECTION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

\[?\] Select Ollama models to download (space to select, enter to confirm):  
    \[x\] llama3.2:latest (8B, general purpose) \- 4.7 GB  
    \[ \] llama3.2:3b (3B, fast inference) \- 2.0 GB  
    \[x\] mistral:latest (7B, coding & reasoning) \- 4.1 GB  
    \[ \] codellama:13b (13B, specialized coding) \- 7.4 GB  
    \[ \] gemma2:9b (9B, Google) \- 5.4 GB  
    \[x\] qwen2.5:7b (7B, multilingual) \- 4.7 GB

â†’ Selected models (will be downloaded during deployment):  
  âœ“ llama3.2:latest (4.7 GB)  
  âœ“ mistral:latest (4.1 GB)  
  âœ“ qwen2.5:7b (4.7 GB)  
  Total download size: \~13.5 GB

â†’ Ollama configuration:  
  âœ“ OLLAMA\_MODELS=llama3.2:latest,mistral:latest,qwen2.5:7b  
  âœ“ OLLAMA\_NUM\_PARALLEL=3 (based on 24GB VRAM)  
  âœ“ OLLAMA\_GPU\_LAYERS=999 (full GPU offload)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 7/14: EXTERNAL API CONFIGURATION (Optional)  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LiteLLM can route complex queries to external APIs.  
Leave blank to skip (local Ollama only).

\[?\] OpenAI API Key (for GPT-4, GPT-4o): sk-proj-xxxxxxxxxxxxxxxxxxxx

â†’ Validating OpenAI key...  
  âœ“ Key valid  
  âœ“ Available models: gpt-4-turbo, gpt-4o, gpt-3.5-turbo

\[?\] Anthropic API Key (for Claude): 

â†’ Skipped

\[?\] Google Gemini API Key: 

â†’ Skipped

\[?\] Groq API Key (fast inference): gsk\_xxxxxxxxxxxxxxxxxxxx

â†’ Validating Groq key...  
  âœ“ Key valid  
  âœ“ Available models: llama-3.1-70b, mixtral-8x7b

\[?\] DeepSeek API Key: 

â†’ Skipped

â†’ External API configuration:  
  âœ“ OPENAI\_API\_KEY=sk-proj-\*\*\*  
  âœ“ GROQ\_API\_KEY=gsk\_\*\*\*  
  âŠ˜ ANTHROPIC\_API\_KEY (not configured)  
  âŠ˜ GOOGLE\_GEMINI\_API\_KEY (not configured)  
  âŠ˜ DEEPSEEK\_API\_KEY (not configured)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 8/14: LITELLM ROUTING CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Configuring LiteLLM routing logic...

  Simple Queries (Ollama local):  
    âœ“ General chat  
    âœ“ Code completion  
    âœ“ Fast responses  
    â†’ Route to: ollama:11434

  Complex Queries (External APIs):  
    âœ“ Long context (\>8K tokens)  
    âœ“ Advanced reasoning  
    âœ“ Production-critical  
    â†’ Primary: OpenAI GPT-4o  
    â†’ Fallback: Groq llama-3.1-70b  
    â†’ Final fallback: Ollama mistral:latest

â†’ Generating litellm\_config.yaml...  
  âœ“ Configuration file created  
  âœ“ 3 model routes configured  
  âœ“ Automatic fallback enabled

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 9/14: GOOGLE DRIVE INTEGRATION (Optional)  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

\[?\] Enable Google Drive sync for Anything LLM? \[y/N\]: y

â†’ Google Drive setup instructions:  
  1\. Go to: https://console.cloud.google.com/apis/credentials  
  2\. Create OAuth 2.0 Client ID (Desktop app)  
  3\. Download credentials.json

\[?\] Path to credentials.json: \~/Downloads/credentials.json

â†’ Validating credentials...  
  âœ“ credentials.json valid  
  âœ“ Client ID: 123456789-xxxxxxxxx.apps.googleusercontent.com

â†’ Initiating OAuth flow...  
  Please visit this URL to authorize:  
    
  https://accounts.google.com/o/oauth2/auth?client\_id=...  
    
  \[?\] Paste authorization code: 4/0AeaYSHBxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

â†’ Exchanging code for token...  
  âœ“ Token received  
  âœ“ Refresh token stored in .secrets/gdrive\_token.json  
  âœ“ Access expires: 2025-02-07 16:10:42 UTC (auto-refresh enabled)

â†’ Google Drive configuration:  
  âœ“ GDRIVE\_ENABLED=true  
  âœ“ GDRIVE\_SYNC\_INTERVAL=3600 (1 hour)  
  âœ“ GDRIVE\_FOLDER\_ID=root (entire Drive)  
  âœ“ Sync destination: /mnt/data/ai-platform/gdrive/

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 10/14: SIGNAL NOTIFICATIONS (Optional)  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

\[?\] Enable Signal notifications? \[y/N\]: n

â†’ Skipped

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 11/14: OPENCLAW CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Configuring OpenClaw (AI coding agent)...

  âœ“ OPENCLAW\_PORT=18789  
  âœ“ OPENCLAW\_WORKSPACE=/mnt/data/ai-platform/openclaw  
  âœ“ Access method: Tailscale VPN only (not proxied)

\[?\] OpenClaw model preference \[1\]:  
    1\. Use local Ollama models (mistral:latest)  
    2\. Use external API (OpenAI GPT-4o)

â†’ Selection: 1

â†’ OpenClaw configuration:  
  âœ“ OPENCLAW\_MODEL=ollama/mistral:latest  
  âœ“ OPENCLAW\_API\_URL=http://ollama:11434  
 âœ“ OPENCLAW\_MAX\_CONTEXT=32768  
  âœ“ OPENCLAW\_TEMPERATURE=0.2 (deterministic for code)  
  âœ“ OPENCLAW\_AUTO\_SAVE=true  
  âœ“ OPENCLAW\_CONVERSATION\_HISTORY=true

â†’ OpenClaw will be accessible at:  
  http://\<tailscale-ip\>:18789 (once Tailscale assigns IP in Script 2\)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 12/14: OPTIONAL SERVICES SELECTION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

\[?\] Enable optional services? (space to select, enter to confirm):  
    \[x\] Flowise (visual LLM flow builder) \- Port 3004  
    \[ \] Additional Ollama models (select later)  
    \[ \] Custom model fine-tuning (requires advanced setup)

â†’ Selected optional services:  
  âœ“ Flowise will be deployed  
  âœ“ Accessible at: https://ai.jglaine.com/flowise

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 13/14: DIRECTORY STRUCTURE CREATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Creating CONFIG\_ROOT structure...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/stack/  
  âœ“ $ROOT\_PATH (../scripts)/deployment/configs/  
  âœ“ $ROOT\_PATH (../scripts)/deployment/configs/openclaw/  
  âœ“ $ROOT\_PATH (../scripts)/deployment/.secrets/

â†’ Creating DATA\_ROOT structure...  
  âœ“ /mnt/data/ai-platform/volumes/  
  âœ“ /mnt/data/ai-platform/volumes/postgres/  
  âœ“ /mnt/data/ai-platform/volumes/redis/  
  âœ“ /mnt/data/ai-platform/volumes/ollama/  
  âœ“ /mnt/data/ai-platform/volumes/minio/  
  âœ“ /mnt/data/ai-platform/volumes/qdrant/  
  âœ“ /mnt/data/ai-platform/volumes/n8n/  
  âœ“ /mnt/data/ai-platform/volumes/prometheus/  
  âœ“ /mnt/data/ai-platform/openclaw/  
  âœ“ /mnt/data/ai-platform/openclaw/projects/  
  âœ“ /mnt/data/ai-platform/openclaw/conversations/  
  âœ“ /mnt/data/ai-platform/openclaw/artifacts/  
  âœ“ /mnt/data/ai-platform/logs/  
  âœ“ /mnt/data/ai-platform/logs/caddy/  
  âœ“ /mnt/data/ai-platform/logs/ollama/  
  âœ“ /mnt/data/ai-platform/logs/litellm/  
  âœ“ /mnt/data/ai-platform/backups/  
  âœ“ /mnt/data/ai-platform/backups/postgres/  
  âœ“ /mnt/data/ai-platform/backups/vector\_db/  
  âœ“ /mnt/data/ai-platform/gdrive/

â†’ Setting permissions...  
  âœ“ chown \-R jglaine:jglaine $ROOT\_PATH (../scripts)/  
  âœ“ chown \-R jglaine:jglaine /mnt/data/ai-platform/  
  âœ“ chmod 700 $ROOT\_PATH (../scripts)/deployment/.secrets/

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 14/14: CONFIGURATION FILE GENERATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Generating .env file...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/.secrets/.env  
  âœ“ 87 environment variables configured  
  âœ“ File encrypted with GPG

â†’ Generating docker-compose.yml...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/stack/docker-compose.yml  
  âœ“ 17 services configured  
  âœ“ All volume binds use DATA\_ROOT paths

â†’ Generating docker-compose.override.yml (GPU)...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/stack/docker-compose.override.yml  
  âœ“ GPU configuration for Ollama service  
  âœ“ NVIDIA runtime enabled

â†’ Generating Caddyfile...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/configs/Caddyfile  
  âœ“ 14 service routes configured  
  âœ“ HTTPS enabled with Let's Encrypt  
  âœ“ OpenClaw excluded (Tailscale only)

â†’ Generating litellm\_config.yaml...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/configs/litellm\_config.yaml  
  âœ“ Local models: llama3.2, mistral, qwen2.5  
  âœ“ External APIs: OpenAI (GPT-4o), Groq (llama-3.1-70b)  
  âœ“ Routing logic: simple â†’ local, complex â†’ external

â†’ Generating prometheus.yml...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/configs/prometheus.yml  
  âœ“ Scrape targets: Ollama, LiteLLM, Caddy, PostgreSQL  
  âœ“ Retention: 30 days

â†’ Generating grafana-datasources.yml...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/configs/grafana-datasources.yml  
  âœ“ Datasource: Prometheus (auto-configured)

â†’ Generating openclaw/config.json...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/configs/openclaw/config.json  
  âœ“ Model: ollama/mistral:latest  
  âœ“ Workspace: /workspace (mapped to /mnt/data/ai-platform/openclaw)  
  âœ“ Auto-save enabled

â†’ Generating API keys file...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/.secrets/api\_keys.enc  
  âœ“ Encrypted with GPG (passphrase protected)

â†’ Storing Google Drive token...  
  âœ“ $ROOT\_PATH (../scripts)/deployment/.secrets/gdrive\_token.json  
  âœ“ Encrypted with GPG

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
âœ… SYSTEM SETUP COMPLETE  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration Summary:  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
System:  
  âœ“ CPU: AMD Ryzen 9 5950X (32 threads)  
  âœ“ RAM: 64 GB  
  âœ“ GPU: NVIDIA GeForce RTX 3090 (24GB VRAM)  
  âœ“ Storage (Config): $ROOT\_PATH (../scripts) (387 GB free)  
  âœ“ Storage (Data): /mnt/data/ai-platform (1.2 TB free)

Network:  
  âœ“ Domain: ai.jglaine.com  
  âœ“ HTTPS: Enabled (Let's Encrypt)  
  âœ“ Reverse Proxy: Caddy  
  âœ“ Tailscale: Configured (OpenClaw access)

Services (17 total):  
  âœ“ Tailscale VPN  
  âœ“ Caddy (reverse proxy)  
  âœ“ PostgreSQL (database)  
  âœ“ Redis (cache)  
  âœ“ MinIO (object storage)  
  âœ“ Prometheus (metrics)  
  âœ“ Grafana (dashboards)  
  âœ“ Ollama (local LLM)  
  âœ“ LiteLLM (routing)  
  âœ“ OpenClaw (AI coding) \- Tailscale only  
  âœ“ Dify (LLM apps)  
  âœ“ Anything LLM (document chat)  
  âœ“ Open WebUI (Ollama interface)  
  âœ“ Qdrant (vector DB)  
  âœ“ n8n (workflows)  
  âœ“ Flowise (LLM flows)  
  âœ“ Signal-CLI (notifications) \- DISABLED

Models:  
  Local (Ollama):  
    âœ“ llama3.2:latest (8B, general)  
    âœ“ mistral:latest (7B, coding)  
    âœ“ qwen2.5:7b (7B, multilingual)  
    
  External APIs:  
    âœ“ OpenAI GPT-4o (complex queries)  
    âœ“ Groq llama-3.1-70b (fast inference)

Integrations:  
  âœ“ Google Drive sync: ENABLED (1 hour interval)  
  âœ“ Signal notifications: DISABLED

Access URLs (after deployment):  
  âœ“ Open WebUI:    https://ai.jglaine.com/webui  
  âœ“ Dify:          https://ai.jglaine.com/dify  
  âœ“ Anything LLM:  https://ai.jglaine.com/anything  
  âœ“ n8n:           https://ai.jglaine.com/n8n  
  âœ“ Flowise:       https://ai.jglaine.com/flowise  
  âœ“ Grafana:       https://ai.jglaine.com/grafana  
  âœ“ MinIO Console: https://ai.jglaine.com/minio  
  âœ“ OpenClaw:      http://\<tailscale-ip\>:18789 (not proxied)

Files Generated:  
  âœ“ deployment/.secrets/.env (87 variables, encrypted)  
  âœ“ deployment/.secrets/api\_keys.enc (encrypted)  
  âœ“ deployment/.secrets/gdrive\_token.json (encrypted)  
  âœ“ deployment/stack/docker-compose.yml (17 services)  
  âœ“ deployment/stack/docker-compose.override.yml (GPU config)  
  âœ“ deployment/configs/Caddyfile (14 routes)  
  âœ“ deployment/configs/litellm\_config.yaml  
  âœ“ deployment/configs/prometheus.yml  
  âœ“ deployment/configs/grafana-datasources.yml  
  âœ“ deployment/configs/openclaw/config.json

Security:  
  âœ“ All credentials encrypted with GPG  
  âœ“ .secrets/ directory: chmod 700  
  âœ“ Strong random passwords generated (32 characters)  
  âœ“ Tailscale VPN for OpenClaw (not exposed to public)

Next Steps:  
  1\. Review configuration: cat deployment/.secrets/.env  
  2\. Deploy services: bash 2-deploy-services.sh  
  3\. Monitor deployment progress (takes 5-15 minutes)  
  4\. After deployment, Tailscale will assign IP for OpenClaw

Disk Space Estimate:  
  Initial deployment: \~25 GB  
  After model downloads: \~40 GB  
  Expected growth (1 month): \~60-100 GB

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
âš ï¸  IMPORTANT NOTES  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1\. OpenClaw Access:  
   \- NOT accessible via domain (security by design)  
   \- ONLY via Tailscale VPN: http://\<tailscale-ip\>:18789  
   \- Tailscale IP will be shown in Script 2 output  
   \- Use: tailscale status (to find IP after deployment)

2\. Credentials:  
   \- All passwords stored in: deployment/.secrets/.env  
   \- File is GPG encrypted (passphrase required to decrypt)  
   \- BACKUP THIS FILE (and remember GPG passphrase)

3\. Google Drive Sync:  
   \- First sync will start 1 hour after deployment  
   \- Synced files: /mnt/data/ai-platform/gdrive/  
   \- Ingested by Anything LLM automatically

4\. GPU Configuration:  
   \- Ollama will use full GPU (24GB VRAM)  
   \- Parallel model loading: 3 models max  
   \- If GPU memory issues occur, reduce OLLAMA\_NUM\_PARALLEL in .env

5\. External API Costs:  
   \- OpenAI GPT-4o: \~$0.01/1K tokens (only for complex queries)  
   \- Groq: Free tier (limited rate limits)  
   \- LiteLLM will prefer local models to minimize costs

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

---

## **ðŸš€ SCRIPT 2: SERVICE DEPLOYMENT**

**Purpose**: Launch all Docker containers, download Ollama models, initialize databases, start Tailscale, and validate service health.

### **Expected Output**

$ bash 2-deploy-services.sh

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ¦ž AI PLATFORM \- SERVICE DEPLOYMENT  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
Version: v75.2.0  
Host: homelab.local (192.168.1.100)  
User: jglaine  
Date: 2025-02-07 15:45:18 UTC

This script will:  
  1\. Validate configuration from Script 1  
  2\. Pull Docker images  
  3\. Start infrastructure services (PostgreSQL, Redis, MinIO)  
  4\. Initialize Tailscale VPN  
  5\. Start Ollama and download models  
  6\. Deploy AI applications  
  7\. Start reverse proxy (Caddy)  
  8\. Run health checks on all services  
  9\. Display access URLs and Tailscale IP

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 1/10: PRE-DEPLOYMENT VALIDATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Checking prerequisites...  
  âœ“ Docker daemon running  
  âœ“ Docker Compose v2.29.1  
  âœ“ User 'jglaine' in 'docker' group  
  âœ“ GPU available (NVIDIA RTX 3090\)

â†’ Validating configuration files...  
  âœ“ deployment/.secrets/.env exists  
  âœ“ deployment/stack/docker-compose.yml exists  
  âœ“ deployment/stack/docker-compose.override.yml exists (GPU config)  
  âœ“ deployment/configs/Caddyfile exists  
  âœ“ deployment/configs/litellm\_config.yaml exists  
  âœ“ deployment/configs/openclaw/config.json exists

â†’ Decrypting .env file...  
  \[?\] Enter GPG passphrase: \*\*\*\*\*\*\*\*\*\*\*\*  
  âœ“ .env decrypted successfully

â†’ Loading environment variables...  
  âœ“ 87 variables loaded  
  âœ“ CONFIG\_ROOT=$ROOT\_PATH (../scripts)  
  âœ“ DATA\_ROOT=/mnt/data/ai-platform  
  âœ“ GPU\_AVAILABLE=true

â†’ Validating directory structure...  
  âœ“ CONFIG\_ROOT exists (20 MB used)  
  âœ“ DATA\_ROOT exists (1.2 TB free)  
  âœ“ All volume directories exist

â†’ Checking port availability...  
  âœ“ Port 80 (Caddy) \- available  
  âœ“ Port 443 (Caddy) \- available  
  âœ“ Port 5432 (PostgreSQL) \- available  
  âœ“ Port 6379 (Redis) \- available  
  âœ“ Port 11434 (Ollama) \- available  
  âœ“ Port 18789 (OpenClaw) \- available  
  âœ“ Port 3000 (Grafana) \- available  
  âœ“ Port 3001 (Dify) \- available  
  âœ“ Port 3002 (Anything LLM) \- available  
  âœ“ Port 3003 (Open WebUI) \- available  
  âœ“ Port 3004 (Flowise) \- available  
  âœ“ Port 5678 (n8n) \- available  
  âœ“ Port 6333 (Qdrant) \- available  
  âœ“ Port 9000 (MinIO API) \- available  
  âœ“ Port 9001 (MinIO Console) \- available

â†’ Disk space check...  
  âœ“ /mnt/data: 1.2 TB free (sufficient for deployment \+ growth)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 2/10: DOCKER IMAGE PULL  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Pulling Docker images (this may take 5-10 minutes)...

  \[1/17\] postgres:16-alpine  
    âœ“ Digest: sha256:abc123...  
    âœ“ Size: 238 MB  
    âœ“ Status: Downloaded newer image

  \[2/17\] redis:7-alpine  
    âœ“ Digest: sha256:def456...  
    âœ“ Size: 41 MB  
    âœ“ Status: Downloaded newer image

  \[3/17\] minio/minio:latest  
    âœ“ Digest: sha256:ghi789...  
    âœ“ Size: 267 MB  
    âœ“ Status: Downloaded newer image

  \[4/17\] ollama/ollama:latest  
    âœ“ Digest: sha256:jkl012...  
    âœ“ Size: 1.2 GB  
    âœ“ Status: Downloaded newer image

  \[5/17\] ghcr.io/berriai/litellm:latest  
    âœ“ Digest: sha256:mno345...  
    âœ“ Size: 523 MB  
    âœ“ Status: Downloaded newer image

  \[6/17\] tailscale/tailscale:latest  
    âœ“ Digest: sha256:pqr678...  
    âœ“ Size: 187 MB  
    âœ“ Status: Downloaded newer image

  \[7/17\] openclaw/openclaw:latest  
    âœ“ Digest: sha256:stu901...  
    âœ“ Size: 892 MB  
    âœ“ Status: Downloaded newer image

  \[8/17\] langgenius/dify-api:latest  
    âœ“ Digest: sha256:vwx234...  
    âœ“ Size: 1.1 GB  
    âœ“ Status: Downloaded newer image

  \[9/17\] langgenius/dify-web:latest  
    âœ“ Digest: sha256:yza567...  
    âœ“ Size: 456 MB  
    âœ“ Status: Downloaded newer image

  \[10/17\] anything-llm/anything-llm:latest  
    âœ“ Digest: sha256:bcd890...  
    âœ“ Size: 743 MB  
    âœ“ Status: Downloaded newer image

  \[11/17\] ghcr.io/open-webui/open-webui:main  
    âœ“ Digest: sha256:efg123...  
    âœ“ Size: 612 MB  
    âœ“ Status: Downloaded newer image

  \[12/17\] qdrant/qdrant:latest  
    âœ“ Digest: sha256:hij456...  
    âœ“ Size: 234 MB  
    âœ“ Status: Downloaded newer image

  \[13/17\] n8nio/n8n:latest  
    âœ“ Digest: sha256:klm789...  
    âœ“ Size: 567 MB  
    âœ“ Status: Downloaded newer image

  \[14/17\] flowiseai/flowise:latest  
    âœ“ Digest: sha256:nop012...  
    âœ“ Size: 489 MB  
    âœ“ Status: Downloaded newer image

  \[15/17\] caddy:2-alpine  
    âœ“ Digest: sha256:qrs345...  
    âœ“ Size: 47 MB  
    âœ“ Status: Downloaded newer image

  \[16/17\] prom/prometheus:latest  
    âœ“ Digest: sha256:tuv678...  
    âœ“ Size: 234 MB  
    âœ“ Status: Downloaded newer image

  \[17/17\] grafana/grafana:latest  
    âœ“ Digest: sha256:wxy901...  
    âœ“ Size: 389 MB  
    âœ“ Status: Downloaded newer image

â†’ Image pull summary:  
  âœ“ Total images: 17  
  âœ“ Total size: 8.2 GB  
  âœ“ All images pulled successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 3/10: INFRASTRUCTURE SERVICES STARTUP  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Starting PostgreSQL...  
  âœ“ Container: postgres-ai-platform  
  âœ“ Status: Started  
  âœ“ Health check: Waiting... (max 30s)  
  âœ“ Health check: Healthy (took 8s)  
  âœ“ Port 5432: Listening

â†’ Initializing PostgreSQL databases...  
  âœ“ Database: ai\_platform\_db (created)  
  âœ“ Database: dify\_db (created)  
  âœ“ Database: anything\_llm\_db (created)  
  âœ“ Database: n8n\_db (created)  
  âœ“ User: ai\_platform (granted all privileges)

â†’ Starting Redis...  
  âœ“ Container: redis-ai-platform  
  âœ“ Status: Started  
  âœ“ Health check: Healthy (took 3s)  
  âœ“ Port 6379: Listening  
  âœ“ Auth: Password protected

â†’ Starting MinIO...  
  âœ“ Container: minio-ai-platform  
  âœ“ Status: Started  
  âœ“ Health check: Healthy (took 5s)  
  âœ“ Port 9000 (API): Listening  
  âœ“ Port 9001 (Console): Listening

â†’ Initializing MinIO buckets...  
  âœ“ Bucket: dify-storage (created)  
  âœ“ Bucket: anything-llm-storage (created)  
  âœ“ Bucket: n8n-storage (created)  
  âœ“ Access policy: Public read

â†’ Starting Qdrant...  
  âœ“ Container: qdrant-ai-platform  
  âœ“ Status: Started  
  âœ“ Health check: Healthy (took 4s)  
  âœ“ Port 6333 (HTTP): Listening  
  âœ“ Port 6334 (gRPC): Listening

â†’ Starting Prometheus...  
  âœ“ Container: prometheus-ai-platform  
  âœ“ Status: Started  
  âœ“ Config: $ROOT\_PATH (../scripts)/deployment/configs/prometheus.yml  
  âœ“ Health check: Healthy (took 6s)  
  âœ“ Port 9090: Listening

â†’ Infrastructure services summary:  
  âœ“ PostgreSQL: Running (8s to healthy)  
  âœ“ Redis: Running (3s to healthy)  
  âœ“ MinIO: Running (5s to healthy)  
  âœ“ Qdrant: Running (4s to healthy)  
  âœ“ Prometheus: Running (6s to healthy)  
  âœ“ Total startup time: 26 seconds

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 4/10: TAILSCALE VPN INITIALIZATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Starting Tailscale container...  
  âœ“ Container: tailscale-ai-platform  
  âœ“ Status: Started  
  âœ“ Auth key: tskey-auth-xxxxxxxxxxxx-\*\*\*

â†’ Connecting to Tailscale network...  
  âœ“ Authenticating...  
  âœ“ Connected to network: homelab-network  
  âœ“ Device name: homelab-aiplatform  
  âœ“ Tailscale IP assigned: 100.64.0.5

â†’ Verifying Tailscale connectivity...  
  âœ“ Ping tailscale.com: OK  
  âœ“ Peer visibility: 3 devices visible

â†’ Tailscale configuration:  
  âœ“ Network: homelab-network  
  âœ“ Device IP: 100.64.0.5  
  âœ“ Device name: homelab-aiplatform  
  âœ“ OpenClaw will be accessible at: http://100.64.0.5:18789

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 5/10: OLLAMA STARTUP & MODEL DOWNLOAD  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Starting Ollama service...  
  âœ“ Container: ollama-ai-platform  
  âœ“ Status: Started  
  âœ“ GPU: NVIDIA RTX 3090 detected  
  âœ“ VRAM: 24 GB available  
  âœ“ GPU layers: Full offload (999 layers)  
  âœ“ Health check: Healthy (took 7s)  
  âœ“ Port 11434: Listening

â†’ Downloading Ollama models (this will take 10-20 minutes)...

  \[1/3\] llama3.2:latest  
    âœ“ Size: 4.7 GB  
    âœ“ Downloading... â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%  
    âœ“ Verifying checksum... OK  
    âœ“ Loading into VRAM... OK (uses 5.2 GB VRAM)  
    âœ“ Status: Ready  
    âœ“ Time: 4m 32s

  \[2/3\] mistral:latest  
    âœ“ Size: 4.1 GB  
    âœ“ Downloading... â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%  
    âœ“ Verifying checksum... OK  
    âœ“ Loading into VRAM... OK (uses 4.6 GB VRAM)  
    âœ“ Status: Ready  
    âœ“ Time: 3m 58s

  \[3/3\] qwen2.5:7b  
    âœ“ Size: 4.7 GB  
    âœ“ Downloading... â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%  
    âœ“ Verifying checksum... OK  
    âœ“ Loading into VRAM... OK (uses 5.1 GB VRAM)  
    âœ“ Status: Ready  
    âœ“ Time: 4m 21s

â†’ Testing Ollama inference...  
  âœ“ Model: llama3.2:latest  
  âœ“ Prompt: "Hello, world\!"  
  âœ“ Response: "Hello\! How can I assist you today?"  
  âœ“ Inference speed: 47 tokens/sec (GPU accelerated)  
  âœ“ Latency: 38ms (first token)

â†’ Ollama summary:  
  âœ“ Models downloaded: 3  
  âœ“ Total size: 13.5 GB  
  âœ“ VRAM usage: 14.9 GB / 24 GB (62%)  
  âœ“ Available for parallel: 3 models max  
  âœ“ Total download time: 12m 51s

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 6/10: LITELLM ROUTING LAYER  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Starting LiteLLM...  
  âœ“ Container: litellm-ai-platform  
  âœ“ Status: Started  
  âœ“ Config: $ROOT\_PATH (../scripts)/deployment/configs/litellm\_config.yaml  
  âœ“ Health check: Healthy (took 5s)  
  âœ“ Port 4000: Listening

â†’ Validating LiteLLM configuration...  
  âœ“ Local models (Ollama): 3 configured  
    \- llama3.2:latest  
    \- mistral:latest  
    \- qwen2.5:7b  
  âœ“ External APIs: 2 configured  
    \- OpenAI GPT-4o (complex queries)  
    \- Groq llama-3.1-70b (fallback)

â†’ Testing LiteLLM routing...  
    
  Test 1: Simple query (should route to local Ollama)  
    âœ“ Prompt: "What is 2+2?"  
    âœ“ Routed to: ollama/llama3.2:latest  
    âœ“ Response time: 156ms  
    âœ“ Status: PASS

  Test 2: Complex query (should route to OpenAI)  
    âœ“ Prompt: "Explain quantum computing in detail..."  
    âœ“ Routed to: openai/gpt-4o  
    âœ“ Response time: 2.3s  
    âœ“ Status: PASS

  Test 3: Fallback test (simulate OpenAI failure)  
    âœ“ Simulating OpenAI timeout...  
    âœ“ Fallback to: groq/llama-3.1-70b  
    âœ“ Response time: 890ms  
    âœ“ Status: PASS

â†’ LiteLLM routing summary:  
  âœ“ Routing logic: Working correctly  
  âœ“ Local models: Reachable  
  âœ“ External APIs: Reachable  
  âœ“ Fallback chain: Functional

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 7/10: AI APPLICATION SERVICES  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Starting Open WebUI...  
  âœ“ Container: open-webui-ai-platform  
  âœ“ Status: Started  
  âœ“ Connected to: Ollama (http://ollama:11434)  
  âœ“ Health check: Healthy (took 8s)  
  âœ“ Port 3003: Listening

â†’ Starting Dify...  
  âœ“ Container: dify-api-ai-platform  
  âœ“ Container: dify-web-ai-platform  
  âœ“ Status: Started (both)  
  âœ“ Database: dify\_db (connected)  
  âœ“ Vector DB: Qdrant (connected)  
  âœ“ Object storage: MinIO (connected)  
  âœ“ Health check: Healthy (took 12s)  
  âœ“ Port 3001: Listening

â†’ Initializing Dify database schema...  
  âœ“ Running migrations... (15 migrations applied)  
  âœ“ Creating default workspace...  
  âœ“ Admin user: admin@ai-platform.local (auto-generated)  
  âœ“ Admin password: \<shown in logs, please change on first login\>

â†’ Starting Anything LLM...  
  âœ“ Container: anything-llm-ai-platform  
  âœ“ Status: Started  
  âœ“ Database: anything\_llm\_db (connected)  
  âœ“ Vector DB: Qdrant (connected)  
  âœ“ LLM Provider: LiteLLM (connected)  
  âœ“ Health check: Healthy (took 9s)  
  âœ“ Port 3002: Listening

â†’ Starting OpenClaw (AI Coding Agent)...  
  âœ“ Container: openclaw-ai-platform  
  âœ“ Status: Started  
  âœ“ Workspace: /mnt/data/ai-platform/openclaw (mounted)  
  âœ“ Model: ollama/mistral:latest  
  âœ“ Ollama connection: OK  
  âœ“ Health check: Healthy (took 6s)  
  âœ“ Port 18789: Listening (Tailscale only)  
  âœ“ Access URL: http://100.64.0.5:18789

â†’ Starting n8n...  
  âœ“ Container: n8n-ai-platform  
  âœ“ Status: Started  
  âœ“ Database: n8n\_db (connected)  
  âœ“ Health check: Healthy (took 10s)  
  âœ“ Port 5678: Listening

â†’ Starting Flowise...  
  âœ“ Container: flowise-ai-platform  
  âœ“ Status: Started  
  âœ“ Connected to: LiteLLM  
  âœ“ Vector DB: Qdrant (connected)  
  âœ“ Health check: Healthy (took 7s)  
  âœ“ Port 3004: Listening

â†’ AI applications summary:  
  âœ“ Open WebUI: Running (port 3003\)  
  âœ“ Dify: Running (port 3001\)  
  âœ“ Anything LLM: Running (port 3002\)  
  âœ“ OpenClaw: Running (port 18789, Tailscale only)  
  âœ“ n8n: Running (port 5678\)  
  âœ“ Flowise: Running (port 3004\)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 8/10: MONITORING & VISUALIZATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Starting Grafana...  
  âœ“ Container: grafana-ai-platform  
  âœ“ Status: Started  
  âœ“ Datasource: Prometheus (auto-configured)  
  âœ“ Health check: Healthy (took 7s)  
  âœ“ Port 3000: Listening

â†’ Importing Grafana dashboards...  
  âœ“ Dashboard: Ollama Performance (ID: 1\)  
  âœ“ Dashboard: LiteLLM Routing (ID: 2\)  
  âœ“ Dashboard: System Resources (ID: 3\)  
  âœ“ Dashboard: Application Health (ID: 4\)  
  âœ“ Default home dashboard: Ollama Performance

â†’ Grafana configuration:  
  âœ“ Admin user: admin  
  âœ“ Admin password: \<stored in .env\>  
  âœ“ Anonymous access: Disabled  
  âœ“ Default org: AI Platform

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 9/10: REVERSE PROXY & SSL  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Starting Caddy...  
  âœ“ Container: caddy-ai-platform  
  âœ“ Status: Started  
  âœ“ Config: $ROOT\_PATH (../scripts)/deployment/configs/Caddyfile  
  âœ“ Domain: ai.jglaine.com  
  âœ“ Health check: Healthy (took 6s)  
  âœ“ Port 80 (HTTP): Listening  
  âœ“ Port 443 (HTTPS): Listening

â†’ Requesting Let's Encrypt certificate...  
  âœ“ Domain validation: ai.jglaine.com (DNS challenge)  
  âœ“ Certificate issued: Let's Encrypt Authority X3  
  âœ“ Valid until: 2025-05-08 15:52:34 UTC (90 days)  
  âœ“ Auto-renewal: Configured (every 30 days)

â†’ Testing reverse proxy routes...  
    
  \[1/14\] Open WebUI: https://ai.jglaine.com/webui  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 87ms  
    âœ“ Status: 200 OK

  \[2/14\] Dify: https://ai.jglaine.com/dify  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 102ms  
    âœ“ Status: 200 OK

  \[3/14\] Anything LLM: https://ai.jglaine.com/anything  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 76ms  
    âœ“ Status: 200 OK

  \[4/14\] n8n: https://ai.jglaine.com/n8n  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 94ms  
    âœ“ Status: 200 OK

  \[5/14\] Flowise: https://ai.jglaine.com/flowise  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 81ms  
    âœ“ Status: 200 OK

  \[6/14\] Grafana: https://ai.jglaine.com/grafana  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 68ms  
    âœ“ Status: 200 OK

  \[7/14\] MinIO Console: https://ai.jglaine.com/minio  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 73ms  
    âœ“ Status: 200 OK

  \[8/14\] Prometheus: https://ai.jglaine.com/prometheus  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 59ms  
    âœ“ Status: 200 OK

  \[9/14\] Qdrant Dashboard: https://ai.jglaine.com/qdrant  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 65ms  
    âœ“ Status: 200 OK

  \[10/14\] Ollama API: https://ai.jglaine.com/ollama  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 71ms  
    âœ“ Status: 200 OK

  \[11/14\] LiteLLM API: https://ai.jglaine.com/litellm  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response time: 62ms  
    âœ“ Status: 200 OK

  \[12/14\] Health Check Endpoint: https://ai.jglaine.com/health  
    âœ“ Proxy status: OK  
    âœ“ SSL/TLS: Valid certificate  
    âœ“ Response: {"status":"healthy","services":17}  
    âœ“ Status: 200 OK

  \[13/14\] HTTP â†’ HTTPS Redirect  
    âœ“ http://ai.jglaine.com â†’ https://ai.jglaine.com  
    âœ“ Status: 301 Moved Permanently  
    âœ“ Redirect working correctly

  \[14/14\] OpenClaw (should NOT be proxied)  
    âœ“ https://ai.jglaine.com/openclaw â†’ 404 Not Found (expected)  
    âœ“ http://100.64.0.5:18789 â†’ 200 OK (Tailscale only)  
    âœ“ Isolation confirmed: OpenClaw not exposed via domain

â†’ Reverse proxy summary:  
  âœ“ All routes functional (14/14)  
  âœ“ SSL/TLS enabled  
  âœ“ HTTP â†’ HTTPS redirect: Working  
  âœ“ OpenClaw isolation: Confirmed (Tailscale only)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 10/10: FINAL HEALTH CHECKS & INTEGRATION TESTS  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Running comprehensive health checks...

  Infrastructure Services:  
    âœ“ PostgreSQL: Healthy (query test: 3ms)  
    âœ“ Redis: Healthy (ping test: 1ms)  
    âœ“ MinIO: Healthy (bucket list: OK)  
    âœ“ Qdrant: Healthy (collection list: OK)  
    âœ“ Prometheus: Healthy (scraping 12 targets)

  AI Services:  
    âœ“ Ollama: Healthy (3 models loaded, 14.9GB VRAM used)  
    âœ“ LiteLLM: Healthy (routing functional)  
    âœ“ Open WebUI: Healthy (connected to Ollama)  
    âœ“ Dify: Healthy (all dependencies connected)  
    âœ“ Anything LLM: Healthy (vector DB connected)  
    âœ“ OpenClaw: Healthy (Tailscale accessible)  
    âœ“ n8n: Healthy (workflow engine ready)  
    âœ“ Flowise: Healthy (flow builder ready)

  Monitoring:  
    âœ“ Grafana: Healthy (dashboards loaded)  
    âœ“ Prometheus: Healthy (metrics collecting)

  Network:  
    âœ“ Caddy: Healthy (SSL active, 14 routes)  
    âœ“ Tailscale: Healthy (IP: 100.64.0.5)

â†’ Integration tests...

  Test 1: Ollama â†’ LiteLLM â†’ Open WebUI (end-to-end)  
    âœ“ Prompt sent via Open WebUI  
    âœ“ Routed through LiteLLM  
    âœ“ Processed by Ollama (llama3.2:latest)  
    âœ“ Response received: "Hello\! I'm ready to assist."  
    âœ“ Total latency: 342ms  
    âœ“ Status: PASS

  Test 2: Document ingestion (Anything LLM)  
    âœ“ Test document uploaded: test.pdf (1.2 MB)  
    âœ“ Chunked: 47 chunks  
    âœ“ Embedded via LiteLLM: OK  
    âœ“ Stored in Qdrant: OK  
    âœ“ Query test: "What is in the document?"  
    âœ“ Response: Relevant summary returned  
    âœ“ Status: PASS

  Test 3: External API fallback (LiteLLM)  
    âœ“ Complex query sent (\>8K tokens)  
    âœ“ LiteLLM routed to: OpenAI GPT-4o  
    âœ“ Response received: OK  
    âœ“ Status: PASS

  Test 4: OpenClaw code generation  
    âœ“ Accessed via Tailscale: http://100.64.0.5:18789  
    âœ“ Prompt: "Write a Python function to sort a list"  
    âœ“ Model: ollama/mistral:latest  
    âœ“ Code generated: Valid Python function  
    âœ“ Saved to: /mnt/data/ai-platform/openclaw/projects/test/  
    âœ“ Status: PASS

  Test 5: Grafana metrics visualization  
    âœ“ Ollama inference metrics: Visible  
    âœ“ LiteLLM routing stats: Visible  
    âœ“ System resources: Visible  
    âœ“ Status: PASS

  Test 6: n8n workflow execution  
    âœ“ Test workflow created: "Notify on model completion"  
    âœ“ Trigger: Webhook  
    âœ“ Action: Call LiteLLM API  
    âœ“ Workflow executed: OK  
    âœ“ Status: PASS

â†’ Google Drive sync status...  
  âœ“ Google Drive integration: ENABLED  
  âœ“ First sync scheduled: 2025-02-07 16:52:18 UTC (in 1 hour)  
  âœ“ Sync destination: /mnt/data/ai-platform/gdrive/  
  âœ“ Anything LLM auto-ingestion: ENABLED

â†’ Backup verification...  
  âœ“ Backup script deployed: $ROOT\_PATH (../scripts)/scripts/backup.sh  
  âœ“ Cron job created: Daily at 2:00 AM UTC  
  âœ“ Backup destination: /mnt/data/ai-platform/backups/  
  âœ“ Retention: 7 days

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
âœ… DEPLOYMENT COMPLETE  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Deployment Summary:  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Total Services: 17 (all healthy)  
  âœ“ Infrastructure: 5 services  
  âœ“ AI Applications: 6 services  
  âœ“ Monitoring: 2 services  
  âœ“ Networking: 2 services  
  âœ“ Databases: 2 services

Deployment Timeline:  
  Phase 1 (Validation): 1m 12s  
  Phase 2 (Image pull): 6m 34s  
  Phase 3 (Infrastructure): 26s  
  Phase 4 (Tailscale): 15s  
  Phase 5 (Ollama \+ models): 12m 51s  
  Phase 6 (LiteLLM): 32s  
  Phase 7 (AI apps): 2m 18s  
  Phase 8 (Monitoring): 47s  
  Phase 9 (Reverse proxy): 1m 03s  
  Phase 10 (Health checks): 1m 25s  
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
  Total: 26m 43s

Resource Usage:  
  CPU: 18% (avg across 32 threads)  
  RAM: 22.4 GB / 64 GB (35%)  
  GPU VRAM: 14.9 GB / 24 GB (62%)  
  Disk (DATA\_ROOT): 41.2 GB used, 1.16 TB free

Network Configuration:  
  Domain: ai.jglaine.com  
  HTTPS: Enabled (Let's Encrypt)  
  Certificate valid until: 2025-05-08  
  Tailscale IP: 100.64.0.5

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸŒ ACCESS URLS  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AI Applications (via domain, HTTPS enabled):  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
  Open WebUI      https://ai.jglaine.com/webui  
  Dify            https://ai.jglaine.com/dify  
  Anything LLM    https://ai.jglaine.com/anything  
  n8n             https://ai.jglaine.com/n8n  
  Flowise         https://ai.jglaine.com/flowise

OpenClaw (Tailscale VPN ONLY, NOT via domain):  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
  OpenClaw        http://100.64.0.5:18789

Monitoring & Management:  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
  Grafana         https://ai.jglaine.com/grafana  
  Prometheus      https://ai.jglaine.com/prometheus  
  MinIO Console   https://ai.jglaine.com/minio

APIs (programmatic access):  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
  Ollama API      https://ai.jglaine.com/ollama/api  
  LiteLLM API     https://ai.jglaine.com/litellm/v1  
  Qdrant API      https://ai.jglaine.com/qdrant/collections

Health Check:  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
  Platform Status https://ai.jglaine.com/health

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”‘ DEFAULT CREDENTIALS  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âš ï¸  IMPORTANT: Change all default passwords on first login\!

Grafana:  
  Username: admin  
  Password: (stored in .env: GRAFANA\_ADMIN\_PASSWORD)

Dify:  
  Username: admin@ai-platform.local  
  Password: (shown in logs during Phase 7, or reset via CLI)

MinIO Console:  
  Username: minioadmin  
  Password: (stored in .env: MINIO\_ROOT\_PASSWORD)

n8n:  
  Initial setup required (create account on first access)

Open WebUI:  
  Initial setup required (create account on first access)

Anything LLM:  
  Initial setup required (create account on first access)

Flowise:  
  Initial setup required (create account on first access)

OpenClaw:  
  No authentication (secured by Tailscale VPN)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ“Š QUICK START GUIDE  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1\. Access Open WebUI (easiest way to test):  
   â†’ Go to: https://ai.jglaine.com/webui  
   â†’ Create an account  
   â†’ Select model: llama3.2:latest  
   â†’ Start chatting\!

2\. Test OpenClaw (AI coding agent):  
   â†’ Ensure you're connected to Tailscale VPN  
   â†’ Go to: http://100.64.0.5:18789  
   â†’ Try: "Create a Python Flask API with 3 endpoints"

3\. Monitor system health:  
   â†’ Go to: https://ai.jglaine.com/grafana  
   â†’ Login with admin credentials  
   â†’ View: "Ollama Performance" dashboard

4\. Set up workflows (n8n):  
   â†’ Go to: https://ai.jglaine.com/n8n  
   â†’ Create account on first access  
   â†’ Example workflow: "Notify me when Ollama finishes a large task"

5\. Build LLM apps (Dify):  
   â†’ Go to: https://ai.jglaine.com/dify  
   â†’ Login: admin@ai-platform.local  
   â†’ Create your first LLM-powered application

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
âš ï¸  IMPORTANT NOTES  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1\. OpenClaw Access:  
   âœ“ Only accessible via Tailscale VPN  
   âœ“ NOT exposed to public internet (security by design)  
   âœ“ If Tailscale disconnects, run:  
     docker restart tailscale-ai-platform

2\. Google Drive Sync:  
   âœ“ First sync: 2025-02-07 16:52:18 UTC (in 1 hour)  
   âœ“ Interval: Every 1 hour  
   âœ“ Destination: /mnt/data/ai-platform/gdrive/  
   âœ“ Auto-ingested by Anything LLM

3\. SSL Certificate:  
   âœ“ Auto-renews every 30 days  
   âœ“ Next renewal: \~2025-03-09  
   âœ“ Caddy handles renewal automatically (no action required)

4\. Ollama Models:  
   âœ“ 3 models downloaded (13.5 GB)  
   âœ“ To add more models:  
     docker exec ollama-ai-platform ollama pull \<model-name\>  
   âœ“ Check available models:  
     docker exec ollama-ai-platform ollama list

5\. Backups:  
   âœ“ Automated daily backups at 2:00 AM UTC  
   âœ“ Backup location: /mnt/data/ai-platform/backups/  
   âœ“ Retention: 7 days  
   âœ“ Manual backup:  
     bash $ROOT\_PATH (../scripts)/scripts/backup.sh

6\. Resource Monitoring:  
   âœ“ Current VRAM usage: 62% (14.9 GB / 24 GB)  
   âœ“ Can load 3 models in parallel  
   âœ“ If VRAM errors occur, reduce OLLAMA\_NUM\_PARALLEL in .env

7\. External API Costs:  
   âœ“ OpenAI GPT-4o: Only used for complex queries  
   âœ“ Estimated monthly cost: $5-$20 (depends on usage)  
   âœ“ Monitor usage: https://platform.openai.com/usage

8\. Service Logs:  
   âœ“ View logs for any service:  
     docker logs \<container-name\> \-f  
   âœ“ Example:  
     docker logs ollama-ai-platform \-f

9\. Stopping Services:  
   âœ“ Stop all services:  
     cd $ROOT\_PATH (../scripts)/deployment/stack  
     docker-compose down  
   âœ“ Start all services:  
     docker-compose up \-d

10\. Updating Services:  
    âœ“ Pull latest images:  
      docker-compose pull  
    âœ“ Restart with new images:  
      docker-compose up \-d \--force-recreate

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ“ NEXT STEPS  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1\. âœ… Deployment complete (this script)  
2\. â†’ Configure services (run: bash 3-configure-services.sh)  
   \- Set up first-time passwords  
   \- Configure integrations  
   \- Test all workflows  
3\. â†’ Add optional services (run: bash 4-add-services.sh)  
   \- Install additional models  
   \- Enable more features  
   \- Customize deployments

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
âœ¨ Deployment completed successfully\!  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Logs saved to: /mnt/data/ai-platform/logs/deployment-20250207-155218.log

For support, visit: https://github.com/yourusername/ai-platform

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### **Script 2 Success Criteria**

* Exit code 0 (success)  
* All 17 containers running and healthy  
* PostgreSQL: Databases created (4 databases)  
* Redis: Connection successful  
* MinIO: Buckets created (3 buckets)  
* Qdrant: Running and accessible  
* Ollama: 3 models downloaded (13.5 GB)  
* Ollama: GPU active (VRAM usage visible)  
* LiteLLM: Routing functional (local \+ external APIs)  
* Tailscale: IP assigned (e.g., 100.64.0.5)  
* OpenClaw: Accessible via Tailscale IP only  
* Caddy: SSL certificate obtained (Let's Encrypt)  
* Caddy: All 14 routes functional  
* Caddy: OpenClaw NOT proxied (404 on domain, 200 on Tailscale IP)  
* Grafana: Dashboards loaded  
* Integration tests: All 6 tests PASS  
* Health check endpoint: Returns 200 OK  
* Backup cron job: Created  
* Total deployment time: \< 30 minutes

---

## **âš™ï¸ SCRIPT 3: SERVICE CONFIGURATION**

**Purpose**: Perform post-deployment configuration, set up integrations, test workflows, configure monitoring alerts, and verify end-to-end functionality.

### **Expected Output**

$ bash 3-configure-services.sh

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ¦ž AI PLATFORM \- SERVICE CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
Version: v75.2.0  
Host: homelab.local (192.168.1.100)  
User: jglaine  
Date: 2025-02-07 16:15:30 UTC

This script will:  
  1\. Verify all services are healthy  
  2\. Configure first-time setup for services requiring manual setup  
  3\. Set up monitoring alerts (Prometheus \+ Grafana)  
  4\. Test integrations (Google Drive, external APIs)  
  5\. Configure backup automation  
  6\. Run end-to-end workflow tests  
  7\. Generate usage documentation

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 1/8: SERVICE HEALTH VERIFICATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Checking all 17 services...  
  âœ“ postgres-ai-platform: Healthy  
  âœ“ redis-ai-platform: Healthy  
  âœ“ minio-ai-platform: Healthy  
  âœ“ qdrant-ai-platform: Healthy  
  âœ“ prometheus-ai-platform: Healthy  
  âœ“ grafana-ai-platform: Healthy  
  âœ“ ollama-ai-platform: Healthy  
  âœ“ litellm-ai-platform: Healthy  
  âœ“ tailscale-ai-platform: Healthy  
  âœ“ openclaw-ai-platform: Healthy  
  âœ“ open-webui-ai-platform: Healthy  
  âœ“ dify-api-ai-platform: Healthy  
  âœ“ dify-web-ai-platform: Healthy  
  âœ“ anything-llm-ai-platform: Healthy  
  âœ“ n8n-ai-platform: Healthy  
  âœ“ flowise-ai-platform: Healthy  
  âœ“ caddy-ai-platform: Healthy

â†’ All services healthy (17/17)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 2/8: FIRST-TIME SERVICE SETUP  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Configuring Open WebUI...  
  \[?\] Open WebUI requires initial account creation  
  \[?\] Visit: https://ai.jglaine.com/webui  
  \[?\] Create admin account now? \[Y/n\]: Y

â†’ Opening browser to: https://ai.jglaine.com/webui  
  âš ï¸  Please complete account creation in browser  
  \[?\] Press Enter when account is created...

â†’ Verifying Open WebUI configuration...  
  âœ“ Admin account created  
  âœ“ Connected to Ollama  
  âœ“ Models available: 3  
  âœ“ Status: Configured

â†’ Configuring Dify...  
  âœ“ Admin account already exists: admin@ai-platform.local  
  âœ“ Password: (stored in deployment logs from Script 2\)  
    
  \[?\] Reset admin password? \[y/N\]: N  
    
  âœ“ Default workspace created  
  âœ“ Ollama integration: Active  
  âœ“ Vector DB (Qdrant): Connected  
  âœ“ Status: Configured

â†’ Configuring Anything LLM...  
  \[?\] Anything LLM requires initial account creation  
  \[?\] Visit: https://ai.jglaine.com/anything  
  \[?\] Create admin account now? \[Y/n\]: Y

â†’ Opening browser to: https://ai.jglaine.com/anything  
  âš ï¸  Please complete account creation in browser  
  \[?\] Press Enter when account is created...

â†’ Verifying Anything LLM configuration...  
  âœ“ Admin account created  
  âœ“ LLM provider: LiteLLM (configured)  
  âœ“ Vector DB: Qdrant (connected)  
  âœ“ Google Drive sync: ENABLED  
  âœ“ Status: Configured

â†’ Configuring n8n...  
  \[?\] n8n requires initial account creation  
  \[?\] Visit: https://ai.jglaine.com/n8n  
  \[?\] Create admin account now? \[Y/n\]: Y

â†’ Opening browser to: https://ai.jglaine.com/n8n  
  âš ï¸  Please complete account creation in browser  
  \[?\] Press Enter when account is created...

â†’ Verifying n8n configuration...  
  âœ“ Admin account created  
  âœ“ Workflow engine: Ready  
  âœ“ Credentials vault: Active  
  âœ“ Status: Configured

â†’ Configuring Flowise...  
  \[?\] Flowise requires initial account creation  
  \[?\] Visit: https://ai.jglaine.com/flowise  
  \[?\] Create admin account now? \[Y/n\]: Y

â†’ Opening browser to: https://ai.jglaine.com/flowise  
  âš ï¸  Please complete account creation in browser  
  \[?\] Press Enter when account is created...

â†’ Verifying Flowise configuration...  
  âœ“ Admin account created  
  âœ“ LLM connections: LiteLLM \+ Ollama  
  âœ“ Vector DB: Qdrant (connected)  
  âœ“ Status: Configured

â†’ First-time setup summary:  
  âœ“ Open WebUI: Configured  
  âœ“ Dify: Configured (admin password unchanged)  
  âœ“ Anything LLM: Configured  
  âœ“ n8n: Configured  
  âœ“ Flowise: Configured  
  âœ“ OpenClaw: No setup required (Tailscale VPN)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 3/8: MONITORING & ALERTS CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Configuring Prometheus alert rules...  
  âœ“ Alert: OllamaDown (if Ollama unreachable for 5 minutes)  
  âœ“ Alert: HighVRAMUsage (if VRAM \> 90% for 10 minutes)  
  âœ“ Alert: PostgreSQLDown (if PostgreSQL unreachable)  
  âœ“ Alert: DiskSpaceLow (if DATA\_ROOT \< 50 GB free)  
  âœ“ Alert: HighCPUUsage (if CPU \> 80% for 15 minutes)  
  âœ“ Alert: ContainerDown (if any AI Platform container stops)  
  âœ“ Alert rules saved: $ROOT\_PATH (../scripts)/deployment/configs/prometheus-alerts.yml  
  âœ“ Prometheus reloaded configuration

â†’ Configuring Grafana alerting...  
  âœ“ Alert channel: Email (admin@jglaine.com)  
  âœ“ Alert channel: (Signal disabled, skipped)  
  âœ“ Notification policy: Send on state change  
 âœ“ Silence period: None (alert on every occurrence)  
  âœ“ Repeat interval: 4 hours

â†’ Testing Grafana alert delivery...  
  âœ“ Test alert sent to: admin@jglaine.com  
  âœ“ Email delivered successfully (check inbox)  
  âœ“ Alert configuration: Validated

â†’ Creating Grafana notification rules...  
  âœ“ Rule 1: Ollama performance degradation (trigger: inference \> 10s)  
  âœ“ Rule 2: LiteLLM routing failures (trigger: \>5% error rate)  
  âœ“ Rule 3: Disk space warning (trigger: \<100 GB free)  
  âœ“ Rule 4: GPU temperature alert (trigger: \>85Â°C)  
  âœ“ Rule 5: Service health check failure  
  âœ“ All rules: Active

â†’ Monitoring & alerts summary:  
  âœ“ Prometheus alerts: 6 rules configured  
  âœ“ Grafana notifications: 5 rules configured  
  âœ“ Alert channels: Email (active)  
  âœ“ Test alert: Delivered successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 4/8: INTEGRATION TESTING  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Testing Google Drive integration...  
  âœ“ Token file exists: $ROOT\_PATH (../scripts)/deployment/.secrets/gdrive\_token.json  
  âœ“ Token decrypted successfully  
  âœ“ Token valid until: 2025-02-08 15:10:42 UTC  
    
  â†’ Triggering manual sync (test)...  
    âœ“ Connecting to Google Drive API...  
    âœ“ Authenticated as: user@jglaine.com  
    âœ“ Root folder accessible  
    âœ“ Files found: 47 documents  
    âœ“ Downloading to: /mnt/data/ai-platform/gdrive/  
      
    Download progress:  
      \[1/47\] Project\_Proposal.docx (2.3 MB) âœ“  
      \[2/47\] Meeting\_Notes\_2024.pdf (1.1 MB) âœ“  
      \[3/47\] Budget\_Sheet.xlsx (0.8 MB) âœ“  
      ...  
      \[47/47\] Research\_Paper.pdf (3.4 MB) âœ“  
      
    âœ“ Total downloaded: 47 files (127.8 MB)  
    âœ“ Sync duration: 43 seconds  
    
  â†’ Testing Anything LLM auto-ingestion...  
    âœ“ Monitoring directory: /mnt/data/ai-platform/gdrive/  
    âœ“ Processing files... (47 documents)  
      
    Ingestion progress:  
      âœ“ Chunking documents: 47/47  
      âœ“ Total chunks created: 1,847  
      âœ“ Embedding via LiteLLM: 1,847/1,847  
      âœ“ Storing in Qdrant: 1,847/1,847  
      âœ“ Indexing complete  
      
    âœ“ Ingestion duration: 4m 12s  
    
  â†’ Testing document search...  
    Query: "What are the budget allocations for Q1?"  
    âœ“ Search executed in Qdrant  
    âœ“ Retrieved: 8 relevant chunks  
    âœ“ Context assembled  
    âœ“ LLM response generated  
    âœ“ Response: "Based on the budget sheet, Q1 allocations are..."  
    âœ“ Search latency: 1.2s  
    âœ“ Status: PASS  
    
  âœ“ Google Drive integration: Fully functional

â†’ Testing external API routing (LiteLLM)...  
    
  Test 1: Simple query â†’ Local Ollama  
    âœ“ Query: "What is Python?"  
    âœ“ Routed to: ollama/llama3.2:latest  
    âœ“ Response time: 187ms  
    âœ“ Status: PASS  
    
  Test 2: Complex query â†’ OpenAI GPT-4o  
    âœ“ Query: "Write a detailed technical specification for a microservices architecture..."  
    âœ“ Detected: Complex (\>500 tokens expected)  
    âœ“ Routed to: openai/gpt-4o  
    âœ“ Response time: 3.4s  
    âœ“ Status: PASS  
    
  Test 3: OpenAI unavailable â†’ Groq fallback  
    âœ“ Simulating OpenAI timeout...  
    âœ“ Fallback triggered  
    âœ“ Routed to: groq/llama-3.1-70b-versatile  
    âœ“ Response time: 1.1s  
    âœ“ Status: PASS  
    
  Test 4: All external APIs down â†’ Local fallback  
    âœ“ Simulating all external API failures...  
    âœ“ Final fallback triggered  
    âœ“ Routed to: ollama/mistral:latest  
    âœ“ Response time: 234ms  
    âœ“ Status: PASS  
    
  âœ“ LiteLLM routing: Fully functional

â†’ Testing Ollama multi-model inference...  
    
  Test 1: Load 3 models in parallel  
    âœ“ Loading llama3.2:latest (VRAM: 5.2 GB)  
    âœ“ Loading mistral:latest (VRAM: 4.6 GB)  
    âœ“ Loading qwen2.5:7b (VRAM: 5.1 GB)  
    âœ“ Total VRAM: 14.9 GB / 24 GB (62%)  
    âœ“ All models loaded successfully  
    
  Test 2: Concurrent inference (3 requests)  
    âœ“ Request 1 â†’ llama3.2:latest: "Translate to French..."  
    âœ“ Request 2 â†’ mistral:latest: "Write a Python function..."  
    âœ“ Request 3 â†’ qwen2.5:7b: "ç”¨ä¸­æ–‡å›žç­”..."  
    âœ“ All responses received  
    âœ“ Avg latency: 312ms  
    âœ“ No VRAM errors  
    âœ“ Status: PASS  
    
  âœ“ Ollama multi-model: Fully functional

â†’ Testing OpenClaw (AI coding agent)...  
  âœ“ Access via Tailscale: http://100.64.0.5:18789  
  âœ“ Connection: Established  
    
  Test task: "Create a REST API with 3 endpoints"  
  âœ“ Model: ollama/mistral:latest  
  âœ“ Prompt sent  
  âœ“ Code generation started...  
  âœ“ Files created:  
    \- /mnt/data/ai-platform/openclaw/projects/rest\_api/app.py  
    \- /mnt/data/ai-platform/openclaw/projects/rest\_api/requirements.txt  
    \- /mnt/data/ai-platform/openclaw/projects/rest\_api/README.md  
  âœ“ Code review: Syntax valid (Python 3.11)  
  âœ“ Auto-saved: Enabled  
  âœ“ Conversation history: Saved  
  âœ“ Status: PASS  
    
  âœ“ OpenClaw: Fully functional

â†’ Integration testing summary:  
  âœ“ Google Drive sync: PASS  
  âœ“ Anything LLM ingestion: PASS  
  âœ“ Document search: PASS  
  âœ“ LiteLLM routing: PASS (4/4 tests)  
  âœ“ Ollama multi-model: PASS (2/2 tests)  
  âœ“ OpenClaw coding: PASS

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 5/8: WORKFLOW AUTOMATION SETUP  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Creating sample n8n workflows...

  Workflow 1: "Daily Summary Report"  
    Purpose: Generate daily summary of AI platform usage  
    Trigger: Cron (daily at 9:00 AM)  
    Steps:  
      1\. Query Prometheus for metrics (last 24h)  
      2\. Aggregate: Total queries, VRAM usage, response times  
      3\. Call LiteLLM to generate summary  
      4\. Send email with report  
    âœ“ Workflow created and saved  
    âœ“ Status: Active (next run: 2025-02-08 09:00:00 UTC)

  Workflow 2: "Model Performance Alert"  
    Purpose: Notify when Ollama response time \> 5s  
    Trigger: Webhook (from Prometheus)  
    Steps:  
      1\. Receive alert from Prometheus  
      2\. Query Ollama for model status  
      3\. Generate diagnostic report  
      4\. Send email notification  
    âœ“ Workflow created and saved  
    âœ“ Status: Active  
    âœ“ Webhook URL: https://ai.jglaine.com/n8n/webhook/model-alert

  Workflow 3: "Auto-Document Ingestion"  
    Purpose: Automatically ingest new Google Drive files  
    Trigger: Google Drive webhook (on new file)  
    Steps:  
      1\. Detect new file in Google Drive  
      2\. Download to /mnt/data/ai-platform/gdrive/  
      3\. Trigger Anything LLM ingestion API  
      4\. Send confirmation notification  
    âœ“ Workflow created and saved  
    âœ“ Status: Active  
    âœ“ Google Drive webhook: Configured

â†’ Creating sample Flowise flows...

  Flow 1: "Conversational RAG"  
    Purpose: Chat with your documents (Google Drive)  
    Components:  
      \- Vector store: Qdrant  
      \- Embeddings: LiteLLM (via Ollama)  
      \- LLM: LiteLLM (with fallback)  
      \- Memory: Conversation buffer  
    âœ“ Flow created and saved  
    âœ“ Status: Ready  
    âœ“ Chat endpoint: https://ai.jglaine.com/flowise/api/v1/prediction/conv-rag

  Flow 2: "Multi-Agent Code Review"  
    Purpose: Review code using multiple AI agents  
    Agents:  
      \- Agent 1: Syntax checker (mistral)  
      \- Agent 2: Security auditor (llama3.2)  
      \- Agent 3: Performance optimizer (qwen2.5)  
      \- Orchestrator: Aggregates feedback  
    âœ“ Flow created and saved  
    âœ“ Status: Ready  
    âœ“ API endpoint: https://ai.jglaine.com/flowise/api/v1/prediction/code-review

  Flow 3: "Smart Document Summarizer"  
    Purpose: Summarize long documents intelligently  
    Logic:  
      \- Short docs (\<5 pages) â†’ Local Ollama  
      \- Long docs (\>5 pages) â†’ OpenAI GPT-4o (better for long context)  
    âœ“ Flow created and saved  
    âœ“ Status: Ready  
    âœ“ API endpoint: https://ai.jglaine.com/flowise/api/v1/prediction/summarizer

â†’ Testing sample workflows...  
  âœ“ n8n Workflow 1: Test execution successful (dry run)  
  âœ“ n8n Workflow 2: Webhook responding (200 OK)  
  âœ“ n8n Workflow 3: Google Drive webhook verified  
  âœ“ Flowise Flow 1: Chat test successful  
  âœ“ Flowise Flow 2: Code review test successful  
  âœ“ Flowise Flow 3: Summarization test successful

â†’ Workflow automation summary:  
  âœ“ n8n workflows: 3 created (all active)  
  âœ“ Flowise flows: 3 created (all ready)  
  âœ“ All workflows tested: PASS

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 6/8: BACKUP AUTOMATION CONFIGURATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Configuring automated backups...

  Backup strategy:  
    \- PostgreSQL: Daily pg\_dump (all databases)  
    \- Qdrant: Daily snapshot export  
    \- Configuration: .env \+ docker-compose files  
    \- Secrets: Encrypted credentials backup  
    \- Retention: 7 days (auto-delete old backups)  
    
  âœ“ Backup script: $ROOT\_PATH (../scripts)/scripts/backup.sh  
  âœ“ Backup destination: /mnt/data/ai-platform/backups/  
  âœ“ Cron job: Daily at 2:00 AM UTC  
  âœ“ Cron entry: 0 2 \* \* \* $ROOT\_PATH (../scripts)/scripts/backup.sh

â†’ Running test backup (manual)...  
    
  \[1/5\] PostgreSQL backup...  
    âœ“ Backing up: ai\_platform\_db  
    âœ“ Backing up: dify\_db  
    âœ“ Backing up: anything\_llm\_db  
    âœ“ Backing up: n8n\_db  
    âœ“ Total size: 47.2 MB  
    âœ“ Compressed: 12.8 MB (73% reduction)  
    âœ“ Saved: /mnt/data/ai-platform/backups/postgres-20250207.sql.gz  
    
  \[2/5\] Qdrant snapshot...  
    âœ“ Creating snapshot via Qdrant API  
    âœ“ Snapshot ID: snapshot-20250207-161842  
    âœ“ Size: 127.8 MB (1,847 vectors)  
    âœ“ Compressed: 43.1 MB  
    âœ“ Saved: /mnt/data/ai-platform/backups/qdrant-20250207.tar.gz  
    
  \[3/5\] Configuration backup...  
    âœ“ Copying: deployment/.secrets/.env (encrypted)  
    âœ“ Copying: deployment/stack/docker-compose.yml  
    âœ“ Copying: deployment/configs/\* (all config files)  
    âœ“ Total: 14 files  
    âœ“ Archived: /mnt/data/ai-platform/backups/config-20250207.tar.gz (2.3 MB)  
    
  \[4/5\] Secrets backup...  
    âœ“ Copying: .secrets/api\_keys.enc  
    âœ“ Copying: .secrets/gdrive\_token.json  
    âœ“ All secrets encrypted with GPG  
    âœ“ Archived: /mnt/data/ai-platform/backups/secrets-20250207.tar.gz.gpg (1.1 MB)  
    
  \[5/5\] OpenClaw workspace backup...  
    âœ“ Backing up: /mnt/data/ai-platform/openclaw/projects/  
    âœ“ Files: 23 (code artifacts)  
    âœ“ Archived: /mnt/data/ai-platform/backups/openclaw-20250207.tar.gz (4.7 MB)

â†’ Test backup summary:  
  âœ“ PostgreSQL: 12.8 MB (4 databases)  
  âœ“ Qdrant: 43.1 MB (1,847 vectors)  
  âœ“ Configuration: 2.3 MB (14 files)  
  âœ“ Secrets: 1.1 MB (encrypted)  
  âœ“ OpenClaw: 4.7 MB (23 files)  
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
  âœ“ Total backup size: 63.9 MB  
  âœ“ Backup duration: 1m 34s  
  âœ“ All backups verified: Checksums valid

â†’ Setting up backup monitoring...  
  âœ“ Grafana alert: Backup failure notification  
  âœ“ Prometheus metric: backup\_last\_success\_timestamp  
  âœ“ Email on failure: admin@jglaine.com  
  âœ“ Next scheduled backup: 2025-02-08 02:00:00 UTC

â†’ Backup automation summary:  
  âœ“ Automated backups: ENABLED  
  âœ“ Schedule: Daily at 2:00 AM UTC  
  âœ“ Retention: 7 days  
  âœ“ Test backup: Successful (63.9 MB)  
  âœ“ Monitoring: Configured

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 7/8: END-TO-END WORKFLOW TESTS  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Running comprehensive end-to-end tests...

  Test 1: Complete RAG Pipeline  
    Scenario: User asks question about documents via Open WebUI  
    Steps:  
      1\. User query: "What was discussed in the last meeting?"  
      2\. Open WebUI â†’ LiteLLM â†’ Anything LLM (RAG)  
      3\. Anything LLM searches Qdrant (Google Drive docs)  
      4\. Retrieves relevant chunks  
      5\. LiteLLM generates response (using context)  
      6\. Response returned to user  
      
    âœ“ Query sent via Open WebUI  
    âœ“ Routed through LiteLLM  
    âœ“ RAG search in Qdrant: 6 relevant chunks retrieved  
    âœ“ Context assembled (847 tokens)  
    âœ“ LLM response generated (ollama/llama3.2)  
    âœ“ Response: "The last meeting discussed budget allocations..."  
    âœ“ End-to-end latency: 2.8s  
    âœ“ Status: PASS

  Test 2: Multi-Service Code Generation  
    Scenario: Generate code with OpenClaw, review with Flowise  
    Steps:  
      1\. OpenClaw generates Python Flask API  
      2\. Code saved to workspace  
      3\. Flowise multi-agent review triggered  
      4\. Review results returned  
      5\. Code improvements suggested  
      
    âœ“ OpenClaw prompt: "Create a Flask API for user management"  
    âœ“ Code generated: app.py, models.py, routes.py  
    âœ“ Saved to: /mnt/data/ai-platform/openclaw/projects/user\_api/  
    âœ“ Flowise review triggered (3 agents)  
    âœ“ Agent 1 (Syntax): No errors found  
    âœ“ Agent 2 (Security): Suggested input validation improvements  
    âœ“ Agent 3 (Performance): Suggested database indexing  
    âœ“ Aggregated feedback generated  
    âœ“ Status: PASS

  Test 3: Automated Workflow Execution  
    Scenario: New file in Google Drive â†’ Auto-ingestion â†’ Notification  
    Steps:  
      1\. Simulate new file upload to Google Drive  
      2\. n8n workflow detects change (webhook)  
      3\. File downloaded to /mnt/data/ai-platform/gdrive/  
      4\. Anything LLM ingestion triggered  
      5\. Email notification sent  
      
    âœ“ Test file created: test\_document\_new.pdf  
    âœ“ Google Drive webhook triggered  
    âœ“ n8n workflow activated  
    âœ“ File downloaded: test\_document\_new.pdf (1.2 MB)  
    âœ“ Anything LLM ingestion: 12 chunks created  
    âœ“ Embedded and stored in Qdrant  
    âœ“ Email notification sent to: admin@jglaine.com  
    âœ“ Total workflow time: 47s  
    âœ“ Status: PASS

  Test 4: External API Fallback Chain  
    Scenario: Test full fallback hierarchy  
    Steps:  
      1\. Send complex query (forces external API)  
      2\. Simulate OpenAI failure  
      3\. Fallback to Groq  
      4\. Simulate Groq failure  
      5\. Final fallback to local Ollama  
      
    âœ“ Query: "Explain distributed systems architecture in detail..."  
    âœ“ Detected: Complex query (\>1000 tokens expected)  
    âœ“ Primary route: openai/gpt-4o (simulated failure)  
    âœ“ Fallback 1: groq/llama-3.1-70b (simulated failure)  
    âœ“ Fallback 2: ollama/mistral:latest (success)  
    âœ“ Response generated locally  
    âœ“ Latency: 3.2s (acceptable for complex query)  
    âœ“ Status: PASS

  Test 5: Multi-Model Parallel Inference  
    Scenario: 3 simultaneous requests to different models  
    Steps:  
      1\. Request A â†’ llama3.2 (translation)  
      2\. Request B â†’ mistral (code generation)  
      3\. Request C â†’ qwen2.5 (Chinese language task)  
      4\. All processed in parallel  
      
    âœ“ Request A sent: "Translate to Spanish: Hello world"  
    âœ“ Request B sent: "Write a Python decorator for logging"  
    âœ“ Request C sent: "ç”¨ä¸­æ–‡è§£é‡Šæœºå™¨å­¦ä¹ "  
    âœ“ All models active simultaneously  
    âœ“ VRAM usage: 14.9 GB (stable, no OOM)  
    âœ“ Response A received: 412ms  
    âœ“ Response B received: 738ms  
    âœ“ Response C received: 521ms  
    âœ“ All responses valid  
    âœ“ Status: PASS

  Test 6: Monitoring & Alerting  
    Scenario: Trigger alert condition, verify notification  
    Steps:  
      1\. Simulate high VRAM usage (\>90%)  
      2\. Prometheus detects condition  
      3\. Alert rule triggered  
      4\. Grafana sends notification  
      
    âœ“ Simulating VRAM spike: 22.1 GB / 24 GB (92%)  
    âœ“ Prometheus alert rule evaluated  
    âœ“ Alert state: FIRING  
    âœ“ Grafana notification triggered  
    âœ“ Email sent to: admin@jglaine.com  
    âœ“ Email subject: "\[ALERT\] High VRAM Usage on AI Platform"  
    âœ“ Email delivered: Confirmed  
    âœ“ Alert cleared (VRAM returned to normal)  
    âœ“ Status: PASS

â†’ End-to-end test summary:  
  âœ“ Test 1 (RAG Pipeline): PASS  
  âœ“ Test 2 (Code Generation): PASS  
  âœ“ Test 3 (Automated Workflow): PASS  
  âœ“ Test 4 (Fallback Chain): PASS  
  âœ“ Test 5 (Parallel Inference): PASS  
  âœ“ Test 6 (Monitoring): PASS  
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
  âœ“ All tests passed: 6/6 (100%)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 8/8: DOCUMENTATION GENERATION  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â†’ Generating usage documentation...

  \[1/5\] Quick Start Guide  
    âœ“ File: $ROOT\_PATH (../scripts)/docs/QUICKSTART.md  
    âœ“ Contents: Basic usage, common tasks, first steps  
    âœ“ Size: 12.4 KB

  \[2/5\] API Reference  
    âœ“ File: $ROOT\_PATH (../scripts)/docs/API\_REFERENCE.md  
    âœ“ Contents: All API endpoints, examples, authentication  
    âœ“ Services documented:  
      \- Ollama API (3 endpoints)  
      \- LiteLLM API (5 endpoints)  
      \- Flowise API (3 flows)  
      \- n8n Webhooks (3 workflows)  
      \- Qdrant API (vector operations)  
    âœ“ Size: 28.7 KB

  \[3/5\] Troubleshooting Guide  
    âœ“ File: $ROOT\_PATH (../scripts)/docs/TROUBLESHOOTING.md  
    âœ“ Contents: Common issues, solutions, debugging tips  
    âœ“ Covers:  
      \- Service startup issues  
      \- VRAM/OOM errors  
      \- Network connectivity  
      \- SSL certificate problems  
      \- Backup/restore procedures  
    âœ“ Size: 19.3 KB

  \[4/5\] Advanced Configuration  
    âœ“ File: $ROOT\_PATH (../scripts)/docs/ADVANCED.md  
    âœ“ Contents: Custom configurations, fine-tuning, optimization  
    âœ“ Topics:  
      \- Adding new Ollama models  
      \- LiteLLM routing customization  
      \- Grafana dashboard creation  
      \- n8n workflow examples  
      \- Performance tuning  
    âœ“ Size: 34.1 KB

  \[5/5\] Security Best Practices  
    âœ“ File: $ROOT\_PATH (../scripts)/docs/SECURITY.md  
    âœ“ Contents: Security hardening, credential management, compliance  
    âœ“ Topics:  
      \- Tailscale VPN configuration  
      \- OpenClaw isolation rationale  
      \- Credential encryption (GPG)  
      \- SSL/TLS best practices  
      \- Backup encryption  
    âœ“ Size: 16.8 KB

â†’ Generating system inventory...  
  âœ“ File: $ROOT\_PATH (../scripts)/docs/SYSTEM\_INVENTORY.json  
  âœ“ Contents: Complete system state snapshot  
    \- All service versions  
    \- Container IDs and statuses  
    \- Resource usage metrics  
    \- Network configuration  
    \- Storage locations  
    \- API endpoints  
  âœ“ Format: JSON (machine-readable)  
  âœ“ Size: 8.2 KB

â†’ Generating usage examples...  
  âœ“ Directory: $ROOT\_PATH (../scripts)/examples/  
  âœ“ Example 1: curl\_examples.sh (API testing)  
  âœ“ Example 2: python\_client.py (Python SDK usage)  
  âœ“ Example 3: n8n\_workflows.json (workflow templates)  
  âœ“ Example 4: flowise\_flows.json (flow templates)  
  âœ“ Total: 4 example files

â†’ Documentation summary:  
  âœ“ Core docs: 5 files (111.3 KB total)  
  âœ“ System inventory: 1 file (JSON)  
  âœ“ Examples: 4 files  
  âœ“ All docs saved to: $ROOT\_PATH (../scripts)/docs/

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
âœ… SERVICE CONFIGURATION COMPLETE  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration Summary:  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Services Configured:  
  âœ“ Open WebUI: Admin account created  
  âœ“ Dify: Admin password unchanged (use existing)  
  âœ“ Anything LLM: Admin account created, Google Drive sync active  
  âœ“ n8n: Admin account created, 3 workflows active  
  âœ“ Flowise: Admin account created, 3 flows ready  
  âœ“ Grafana: Alerts configured, email channel active  
  âœ“ Prometheus: 6 alert rules active  
  âœ“ OpenClaw: No setup required (Tailscale VPN)

Integrations Tested:  
  âœ“ Google Drive sync: Functional (47 files synced)  
  âœ“ Anything LLM ingestion: Functional (1,847 vectors)  
  âœ“ LiteLLM routing: Functional (local \+ external APIs)  
  âœ“ Ollama multi-model: Functional (3 models parallel)  
  âœ“ OpenClaw coding: Functional (code generation tested)

Workflows Created:  
  n8n:  
    âœ“ Daily Summary Report (active, next run: 09:00 AM UTC)  
    âœ“ Model Performance Alert (active, webhook ready)  
    âœ“ Auto-Document Ingestion (active, Google Drive webhook)  
    
  Flowise:  
    âœ“ Conversational RAG (API: /api/v1/prediction/conv-rag)  
    âœ“ Multi-Agent Code Review (API: /api/v1/prediction/code-review)  
    âœ“ Smart Document Summarizer (API: /api/v1/prediction/summarizer)

Backups Configured:  
  âœ“ Automated daily backups: ENABLED  
  âœ“ Schedule: 2:00 AM UTC  
  âœ“ Retention: 7 days  
  âœ“ Last test backup: 63.9 MB (successful)  
  âœ“ Next backup: 2025-02-08 02:00:00 UTC

End-to-End Tests:  
  âœ“ All 6 tests PASSED (100%)  
  âœ“ RAG pipeline: Functional  
  âœ“ Code generation \+ review: Functional  
  âœ“ Automated workflows: Functional  
  âœ“ API fallback chain: Functional  
  âœ“ Parallel inference: Functional  
  âœ“ Monitoring & alerts: Functional

Documentation Generated:  
  âœ“ Quick Start Guide (12.4 KB)  
  âœ“ API Reference (28.7 KB)  
  âœ“ Troubleshooting Guide (19.3 KB)  
  âœ“ Advanced Configuration (34.1 KB)  
  âœ“ Security Best Practices (16.8 KB)  
  âœ“ System Inventory (JSON, 8.2 KB)  
  âœ“ Usage Examples (4 files)

System Health:  
  âœ“ All services: Healthy (17/17)  
  âœ“ CPU usage: 21% (avg)  
  âœ“ RAM usage: 23.7 GB / 64 GB (37%)  
  âœ“ GPU VRAM: 14.9 GB / 24 GB (62%)  
  âœ“ Disk (DATA\_ROOT): 42.1 GB used, 1.15 TB free  
  âœ“ Network: All routes responsive  
  âœ“ SSL certificate: Valid (expires 2025-05-08)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ“ NEXT STEPS  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1\. âœ… System setup complete (Script 1\)  
2\. âœ… Services deployed (Script 2\)  
3\. âœ… Configuration complete (this script)  
4\. â†’ Add optional services (run: bash 4-add-services.sh)  
   \- Install additional Ollama models  
   \- Enable more integrations  
   \- Customize advanced features

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ“š DOCUMENTATION QUICK LINKS  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Getting Started:  
  $ROOT\_PATH (../scripts)/docs/QUICKSTART.md

API Documentation:  
  $ROOT\_PATH (../scripts)/docs/API\_REFERENCE.md

Troubleshooting:  
  $ROOT\_PATH (../scripts)/docs/TROUBLESHOOTING.md

Advanced Topics:  
  $ROOT\_PATH (../scripts)/docs/ADVANCED.md

Security:  
  $ROOT\_PATH (../scripts)/docs/SECURITY.md

Examples:  
  $ROOT\_PATH (../scripts)/examples/

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
âœ¨ Configuration completed successfully\!  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Logs saved to: /mnt/data/ai-platform/logs/configuration-20250207-161530.log

Your AI platform is fully configured and ready for production use\! ðŸš€

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

---

## **ðŸŽ¯ Script 3 Success Criteria**

* Exit code 0 (success)  
* All 17 services remain healthy  
* Open WebUI: Admin account created  
* Dify: Admin account verified or password reset  
* Anything LLM: Admin account created, Google Drive sync confirmed  
* n8n: Admin account created, 3 workflows active  
* Flowise: Admin account created, 3 flows ready  
* Prometheus: 6 alert rules configured and active  
* Grafana: 5 notification rules configured, email channel tested  
* Google Drive integration: Test sync successful (files downloaded and ingested)  
* LiteLLM routing: All 4 tests PASS  
* OpenClaw: Code generation test PASS  
* Backup automation: Test backup successful (\~64 MB), cron job created  
* n8n workflows: 3 created and tested  
* Flowise flows: 3 created and tested  
* End-to-end tests: All 6 tests PASS (100%)  
* Documentation: 5 core docs \+ 1 inventory \+ 4 examples generated  
* Total configuration time: \< 15 minutes (excluding manual account creation pauses)

## **ðŸ”§ SCRIPT 4: ADD OPTIONAL SERVICES**

**Purpose**: Install additional Ollama models, enable advanced integrations, and customize features  
**Prerequisites**: Scripts 1-3 completed successfully  
**Estimated time**: 15-45 minutes (depending on model sizes)

### **Script 4 Expected Output**

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸš€ AI PLATFORM \- SCRIPT 4: ADD OPTIONAL SERVICES  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Script: 4-add-services.sh  
Version: v75.2.0  
Started: 2025-02-07 16:20:15 UTC  
Host: jglaine-ai-server  
User: jglaine

Prerequisites Check:  
  âœ“ Script 1 (setup): Completed  
  âœ“ Script 2 (deployment): Completed  
  âœ“ Script 3 (configuration): Completed  
  âœ“ All services: Healthy (17/17)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸŽ¯ OPTIONAL SERVICES MENU  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Available Options:

\[1\] Install Additional Ollama Models  
    \- Download and configure more LLMs for local inference  
    \- Estimated time: 5-30 minutes per model (depending on size)  
    \- VRAM impact: Variable (2-40 GB per model)

\[2\] Enable Advanced Vector Database Features  
    \- Multi-collection support in Qdrant  
    \- Hybrid search (vector \+ keyword)  
    \- Collection snapshots and backups

\[3\] Add Custom LiteLLM Routes  
    \- Configure custom routing logic  
    \- Add new external API providers  
    \- Set up per-user/per-team quotas

\[4\] Install Additional n8n Nodes  
    \- Community nodes for extended functionality  
    \- Custom integrations (Slack, Discord, etc.)  
    \- Advanced workflow templates

\[5\] Enable Model Fine-Tuning Capabilities  
    \- LoRA adapter support in Ollama  
    \- Training data pipeline setup  
    \- Fine-tuning workflow automation

\[6\] Add Observability Stack Extensions  
    \- Jaeger for distributed tracing  
    \- Elasticsearch for log aggregation  
    \- Custom Grafana dashboards

\[7\] Enable Multi-User Authentication  
    \- Keycloak integration for SSO  
    \- LDAP/Active Directory support  
    \- Role-based access control (RBAC)

\[8\] Install Development Tools  
    \- Jupyter Lab for experimentation  
    \- Code-Server (VS Code in browser)  
    \- Model testing playground

\[9\] Configure Advanced Backup Strategies  
    \- Off-site backup to S3-compatible storage  
    \- Incremental backups  
    \- Automated disaster recovery

\[0\] Exit (no additional services)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

\[?\] Select options (comma-separated, e.g., 1,2,4): 1,3,8

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 1/3: INSTALL ADDITIONAL OLLAMA MODELS  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Current installed models:  
  âœ“ llama3.2:latest (5.2 GB VRAM)  
  âœ“ mistral:latest (4.6 GB VRAM)  
  âœ“ qwen2.5:7b (5.1 GB VRAM)  
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
  Total: 14.9 GB / 24 GB VRAM used (62%)

Available models for installation:

Code Generation Models:  
  \[1\] codellama:7b (Code Llama \- 3.8 GB)  
  \[2\] codellama:13b (Code Llama Large \- 7.3 GB)  
  \[3\] deepseek-coder:6.7b (DeepSeek Coder \- 3.7 GB)  
  \[4\] starcoder2:7b (StarCoder 2 \- 4.0 GB)

Reasoning & Analysis Models:  
  \[5\] llama3.1:8b (Llama 3.1 \- 4.7 GB)  
  \[6\] llama3.1:70b-q4 (Llama 3.1 70B Quantized \- 39 GB) âš ï¸ High VRAM  
  \[7\] qwen2.5:14b (Qwen 2.5 14B \- 8.5 GB)  
  \[8\] phi3:medium (Phi-3 Medium \- 7.9 GB)

Specialized Models:  
  \[9\] llava:13b (Vision model \- 8.0 GB)  
  \[10\] nomic-embed-text (Embeddings only \- 0.5 GB)  
  \[11\] aya:8b (Multilingual \- 4.7 GB)  
  \[12\] gemma2:9b (Gemma 2 \- 5.5 GB)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

\[?\] Select models to install (comma-separated): 1,3,10

â†’ Selected models:  
  âœ“ codellama:7b (3.8 GB)  
  âœ“ deepseek-coder:6.7b (3.7 GB)  
  âœ“ nomic-embed-text (0.5 GB)  
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
  Total to download: 8.0 GB  
  Estimated VRAM after install: 22.9 GB / 24 GB (95%) âš ï¸ High usage

\[?\] Continue with installation? \[Y/n\]: Y

â†’ Installing models...

\[1/3\] Installing codellama:7b...  
  âœ“ Pulling from Ollama registry  
  Download progress:  
    \[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\] 100% (3.8 GB)  
  âœ“ Download complete: 2m 14s  
  âœ“ Extracting layers...  
  âœ“ Creating model...  
  âœ“ Model loaded in Ollama  
    
  Testing inference:  
    Prompt: "Write a Python function to reverse a string"  
    âœ“ Response generated in 412ms  
    âœ“ Code quality: Valid Python syntax  
    
  âœ“ codellama:7b installed successfully

\[2/3\] Installing deepseek-coder:6.7b...  
  âœ“ Pulling from Ollama registry  
  Download progress:  
    \[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\] 100% (3.7 GB)  
  âœ“ Download complete: 2m 04s  
  âœ“ Extracting layers...  
  âœ“ Creating model...  
  âœ“ Model loaded in Ollama  
    
  Testing inference:  
    Prompt: "Explain this code: def factorial(n): return 1 if n \== 0 else n \* factorial(n-1)"  
    âœ“ Response generated in 387ms  
    âœ“ Explanation quality: Accurate and detailed  
    
  âœ“ deepseek-coder:6.7b installed successfully

\[3/3\] Installing nomic-embed-text...  
  âœ“ Pulling from Ollama registry  
  Download progress:  
    \[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\] 100% (0.5 GB)  
  âœ“ Download complete: 18s  
  âœ“ Extracting layers...  
  âœ“ Creating model...  
  âœ“ Model loaded in Ollama  
    
  Testing embeddings:  
    Text: "Machine learning is a subset of artificial intelligence"  
    âœ“ Embedding generated: 768-dimensional vector  
    âœ“ Latency: 23ms  
    
  âœ“ nomic-embed-text installed successfully

â†’ Updating LiteLLM configuration...  
  âœ“ Added codellama:7b to routing pool  
  âœ“ Added deepseek-coder:6.7b to routing pool  
  âœ“ Added nomic-embed-text for embeddings  
  âœ“ Routing rules updated:  
    \- Code generation queries â†’ codellama:7b or deepseek-coder (prefer deepseek)  
    \- Embeddings â†’ nomic-embed-text (faster than OpenAI)  
  âœ“ LiteLLM config reloaded

â†’ Model installation summary:  
  âœ“ Models installed: 3/3  
  âœ“ Total download size: 8.0 GB  
  âœ“ Total time: 4m 36s  
  âœ“ Current VRAM usage: 22.9 GB / 24 GB (95%)  
  âš ï¸ VRAM near capacity \- monitor performance

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 2/3: ADD CUSTOM LITELLM ROUTES  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Current LiteLLM routing configuration:  
  âœ“ Local Ollama models: 6 models  
  âœ“ External APIs: OpenAI, Anthropic, Groq, DeepSeek  
  âœ“ Fallback chain: Configured (3 levels)  
  âœ“ Rate limiting: Disabled (can enable per-route)

Custom routing options:

\[1\] Add new external API provider  
\[2\] Configure per-user quotas  
\[3\] Add team-based routing (different models per team)  
\[4\] Enable cost tracking and budget alerts  
\[5\] Configure advanced caching strategies  
\[6\] Add custom routing rules (e.g., time-based, workload-based)

\[?\] Select options (comma-separated): 1,4

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Option 1: Add New External API Provider  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Available providers:  
  \[1\] Mistral AI  
  \[2\] Cohere  
  \[3\] Together AI  
  \[4\] Perplexity AI  
  \[5\] Custom OpenAI-compatible endpoint

\[?\] Select provider: 1

â†’ Configuring Mistral AI...

\[?\] Mistral AI API key: \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*

â†’ Validating API key...  
  âœ“ API key valid  
  âœ“ Account tier: Standard  
  âœ“ Available models:  
    \- mistral-small-latest  
    \- mistral-medium-latest  
    \- mistral-large-latest  
    \- codestral-latest

\[?\] Which models to enable? \[1,2,3,4\]: 3,4

â†’ Adding Mistral models to LiteLLM...  
  âœ“ mistral-large-latest added (for complex reasoning)  
  âœ“ codestral-latest added (for advanced code generation)  
    
â†’ Configuring routing preferences...  
  \[?\] When to use Mistral Large over OpenAI GPT-4o?  
      1\. Always prefer Mistral (lower cost)  
      2\. Use as fallback if OpenAI fails  
      3\. Load balance 50/50  
      4\. Never use (manual trigger only)  
    
  Selection: 3

  âœ“ Load balancing configured: 50% Mistral, 50% OpenAI  
  âœ“ Codestral set as primary for code generation (local models as fallback)

â†’ Updating secrets...  
  âœ“ MISTRAL\_API\_KEY stored in .secrets/api\_keys.enc (encrypted)  
  âœ“ LiteLLM config updated: $ROOT\_PATH (../scripts)/deployment/configs/litellm\_config.yml

â†’ Testing Mistral integration...  
  Test 1: Simple query to mistral-large-latest  
    âœ“ Query: "Explain quantum computing in simple terms"  
    âœ“ Response received in 1.8s  
    âœ“ Status: PASS  
    
  Test 2: Code generation with codestral-latest  
    âœ“ Query: "Write a TypeScript React component for a login form"  
    âœ“ Response received in 2.3s  
    âœ“ Code quality: Valid TypeScript  
    âœ“ Status: PASS

  âœ“ Mistral AI integration: Fully functional

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Option 4: Enable Cost Tracking and Budget Alerts  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â†’ Configuring cost tracking...

\[?\] Monthly budget limit (USD): 100

â†’ Cost tracking configuration:  
  âœ“ Budget limit: $100.00/month  
  âœ“ Tracking enabled for:  
    \- OpenAI API calls  
    \- Anthropic API calls  
    \- Groq API calls (free tier, tracked for visibility)  
    \- DeepSeek API calls  
    \- Mistral AI calls  
  âœ“ Local Ollama calls: Free (not tracked)

â†’ Setting up budget alerts...  
  âœ“ Alert at 50% budget ($50): Email notification  
  âœ“ Alert at 75% budget ($75): Email \+ reduce external API usage  
  âœ“ Alert at 90% budget ($90): Email \+ pause non-critical external API calls  
  âœ“ Alert at 100% budget ($100): Email \+ block all external API calls (local only)

â†’ Configuring cost tracking dashboard...  
  âœ“ Grafana dashboard created: "API Cost Tracking"  
  âœ“ Panels added:  
    \- Current month spending (by provider)  
    \- Cost per model  
    \- Requests vs. cost comparison  
    \- Budget utilization gauge  
    \- Cost trend forecast  
  âœ“ Dashboard URL: https://ai.jglaine.com/grafana/d/cost-tracking

â†’ Creating cost tracking Prometheus metrics...  
  âœ“ Metric: litellm\_api\_cost\_total (cumulative cost)  
  âœ“ Metric: litellm\_api\_cost\_by\_provider (per provider)  
  âœ“ Metric: litellm\_budget\_utilization\_percent  
  âœ“ All metrics: Exported and scraped by Prometheus

â†’ Testing cost tracking...  
  âœ“ Simulating $5 of API calls  
  âœ“ Costs recorded in Prometheus  
  âœ“ Grafana dashboard updated  
  âœ“ Budget utilization: 5% ($5.00 / $100.00)  
  âœ“ Status: Functional

â†’ Custom LiteLLM routes summary:  
  âœ“ New provider added: Mistral AI (2 models)  
  âœ“ Cost tracking: ENABLED  
  âœ“ Budget limit: $100/month  
  âœ“ Budget alerts: Configured (4 levels)  
  âœ“ Grafana dashboard: Created

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ”§ PHASE 3/3: INSTALL DEVELOPMENT TOOLS  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Development tools to install:

\[1\] Jupyter Lab \- Interactive Python notebooks (âœ“ selected)  
\[2\] Code-Server \- VS Code in browser (âœ“ selected)  
\[3\] Model Testing Playground \- Custom UI for model comparison (âœ“ selected)

â†’ Installing development tools...

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Tool 1/3: Jupyter Lab  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â†’ Creating Jupyter Lab service...  
  âœ“ Docker image: jupyter/scipy-notebook:latest  
  âœ“ Port: 8888  
  âœ“ Volume: /mnt/data/ai-platform/jupyter (notebooks storage)  
  âœ“ Network: ai-platform (access to all services)  
    
â†’ Installing Python packages...  
  âœ“ openai (for OpenAI API)  
  âœ“ anthropic (for Claude API)  
  âœ“ litellm (for LiteLLM client)  
  âœ“ qdrant-client (for Qdrant access)  
  âœ“ pandas, numpy, matplotlib (data science)  
  âœ“ langchain (for LLM orchestration)  
  âœ“ transformers (for model experimentation)

â†’ Starting Jupyter Lab...  
  âœ“ Container ID: 9f8e7d6c5b4a  
  âœ“ Status: Running  
  âœ“ Health check: Passed  
    
â†’ Configuring Caddy reverse proxy...  
  âœ“ Route added: https://ai.jglaine.com/jupyter  
  âœ“ Authentication: Caddy basic auth (username: jglaine)  
  âœ“ SSL: Let's Encrypt certificate

â†’ Generating sample notebooks...  
  âœ“ Notebook 1: Getting Started with LiteLLM.ipynb  
  âœ“ Notebook 2: RAG Pipeline with Qdrant.ipynb  
  âœ“ Notebook 3: Model Comparison and Benchmarking.ipynb  
  âœ“ Notebook 4: Fine-tuning Ollama Models.ipynb  
  âœ“ Notebook 5: Cost Analysis and Optimization.ipynb  
  âœ“ All notebooks saved to: /mnt/data/ai-platform/jupyter/

â†’ Jupyter Lab summary:  
  âœ“ URL: https://ai.jglaine.com/jupyter  
  âœ“ Status: Running  
  âœ“ Sample notebooks: 5 created  
  âœ“ Python packages: 15+ installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Tool 2/3: Code-Server (VS Code)  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â†’ Creating Code-Server service...  
  âœ“ Docker image: codercom/code-server:latest  
  âœ“ Port: 8443  
  âœ“ Volume: /mnt/data/ai-platform/code-server (workspace)  
  âœ“ Network: ai-platform  
    
â†’ Installing VS Code extensions...  
  âœ“ Python (ms-python.python)  
  âœ“ Jupyter (ms-toolsai.jupyter)  
  âœ“ Docker (ms-azuretools.vscode-docker)  
  âœ“ YAML (redhat.vscode-yaml)  
  âœ“ GitLens (eamodio.gitlens)  
  âœ“ Prettier (esbenp.prettier-vscode)  
  âœ“ Remote \- SSH (for OpenClaw access)

â†’ Starting Code-Server...  
  âœ“ Container ID: 7a6b5c4d3e2f  
  âœ“ Status: Running  
  âœ“ Health check: Passed  
    
â†’ Configuring Caddy reverse proxy...  
  âœ“ Route added: https://ai.jglaine.com/code  
  âœ“ Authentication: Caddy basic auth (username: jglaine)  
  âœ“ SSL: Let's Encrypt certificate

â†’ Creating workspace folders...  
  âœ“ /mnt/data/ai-platform/code-server/projects/  
  âœ“ /mnt/data/ai-platform/code-server/scripts/  
  âœ“ /mnt/data/ai-platform/code-server/notebooks/  
  âœ“ Symlink to OpenClaw workspace: /mnt/data/ai-platform/openclaw/ â†’ workspace/openclaw/

â†’ Generating sample projects...  
  âœ“ Project 1: litellm-api-client/ (Python API client)  
  âœ“ Project 2: qdrant-rag-demo/ (RAG implementation)  
  âœ“ Project 3: n8n-custom-node/ (n8n node template)  
  âœ“ Project 4: grafana-custom-dashboard/ (dashboard JSON)  
  âœ“ All projects saved to: /mnt/data/ai-platform/code-server/projects/

â†’ Code-Server summary:  
  âœ“ URL: https://ai.jglaine.com/code  
  âœ“ Status: Running  
  âœ“ VS Code extensions: 7 installed  
  âœ“ Sample projects: 4 created  
  âœ“ OpenClaw integration: Symlinked workspace

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Tool 3/3: Model Testing Playground  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â†’ Creating Model Testing Playground...  
  âœ“ Custom React app: ai-model-playground  
  âœ“ Docker image: node:20-alpine (build from source)  
  âœ“ Port: 3005  
  âœ“ Network: ai-platform

â†’ Building playground UI...  
  âœ“ Cloning repository: https://github.com/ai-platform/model-playground  
  âœ“ Installing dependencies (npm install)...  
  âœ“ Building React app (npm run build)...  
  âœ“ Build complete: 2m 18s  
  âœ“ Serving with nginx

â†’ Starting Model Testing Playground...  
  âœ“ Container ID: 3e4f5g6h7i8j  
  âœ“ Status: Running  
  âœ“ Health check: Passed

â†’ Configuring Caddy reverse proxy...  
  âœ“ Route added: https://ai.jglaine.com/playground  
  âœ“ Authentication: None (same auth as main platform)  
  âœ“ SSL: Let's Encrypt certificate

â†’ Configuring playground settings...  
  âœ“ Connected LLM providers:  
    \- Ollama (6 local models)  
    \- LiteLLM (all configured routes)  
    \- OpenAI (direct access)  
    \- Anthropic (direct access)  
    \- Mistral AI (direct access)  
  âœ“ Features enabled:  
    \- Side-by-side model comparison  
    \- Latency benchmarking  
    \- Token usage tracking  
    \- Cost estimation  
    \- Response quality voting  
    \- Export results to CSV

â†’ Model Testing Playground summary:  
  âœ“ URL: https://ai.jglaine.com/playground  
  âœ“ Status: Running  
  âœ“ Connected providers: 5 (18 total models)  
  âœ“ Features: 6 enabled

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
âœ… OPTIONAL SERVICES INSTALLATION COMPLETE  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Installation Summary:  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
New Ollama Models:  
  âœ“ codellama:7b (3.8 GB) \- Code generation  
  âœ“ deepseek-coder:6.7b (3.7 GB) \- Code analysis  
  âœ“ nomic-embed-text (0.5 GB) \- Fast embeddings  
  Total models: 6 (was 3\)  
  Total VRAM: 22.9 GB / 24 GB (95%)

Custom LiteLLM Routes:  
  âœ“ Mistral AI integration (2 models)  
  âœ“ Cost tracking enabled ($100/month budget)  
  âœ“ Budget alerts configured (4 levels)  
  âœ“ Grafana cost dashboard created

Development Tools:  
  âœ“ Jupyter Lab: https://ai.jglaine.com/jupyter  
  âœ“ Code-Server (VS Code): https://ai.jglaine.com/code  
  âœ“ Model Testing Playground: https://ai.jglaine.com/playground  
  Total new services: 3

System Status After Installation:  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
Services Running:  
  Core services: 17/17 âœ“  
  Optional services: 3/3 âœ“  
  Total: 20/20 âœ“

Resource Usage:  
  CPU: 27% (avg)  
  RAM: 31.4 GB / 64 GB (49%)  
  GPU VRAM: 22.9 GB / 24 GB (95%) âš ï¸  
  Disk (DATA\_ROOT): 58.7 GB used, 1.13 TB free

Network:  
  All routes responding: âœ“  
  SSL certificates valid: âœ“  
  Tailscale VPN active: âœ“

Performance:  
  Ollama avg response time: 287ms (slightly slower due to high VRAM)  
  LiteLLM routing latency: 12ms  
  All services responding: \<500ms

Recommendations:  
  âš ï¸ VRAM usage at 95% \- consider:  
    1\. Unloading unused models (ollama unload \<model\>)  
    2\. Using smaller quantized models (q4 or q5)  
    3\. Upgrading GPU (48 GB VRAM recommended for 6+ models)  
  âœ“ All other metrics: Healthy

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
ðŸ“ NEXT STEPS  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Your AI platform is fully configured with optional services\!

Access your new tools:  
  â€¢ Jupyter Lab: https://ai.jglaine.com/jupyter  
  â€¢ VS Code: https://ai.jglaine.com/code  
  â€¢ Model Playground: https://ai.jglaine.com/playground

Documentation:  
  â€¢ Review: $ROOT\_PATH (../scripts)/docs/ADVANCED.md  
  â€¢ Cost tracking: https://ai.jglaine.com/grafana/d/cost-tracking  
  â€¢ Sample notebooks: /mnt/data/ai-platform/jupyter/

Monitor VRAM usage:  
  â€¢ Grafana: https://ai.jglaine.com/grafana  
  â€¢ CLI: nvidia-smi \-l 5  
  â€¢ Unload models: docker exec ollama ollama unload \<model\>

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  
âœ¨ Optional services installation completed successfully\!  
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Logs saved to: /mnt/data/ai-platform/logs/add-services-20250207-162015.log

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

---

## **ðŸ“Š TROUBLESHOOTING GUIDE**

### **Common Issues and Solutions**

#### **1\. Service Won't Start**

**Symptom**: Container shows "Unhealthy" or "Restarting" status

\# Check logs  
docker logs \<container-name\>

\# Common causes and fixes:

\# A. Port already in use  
sudo netstat \-tulpn | grep \<port\>  
\# Solution: Change port in docker-compose.yml or kill conflicting process

\# B. Volume permission errors  
sudo chown \-R $USER:$USER /mnt/data/ai-platform/volumes/\<service\>  
docker restart \<container-name\>

\# C. Insufficient memory  
docker stats  
\# Solution: Increase Docker memory limit or reduce number of services

\# D. Configuration file error  
docker exec \<container-name\> cat /path/to/config.yml  
\# Validate YAML syntax, fix errors, restart container

#### **2\. Ollama Out of Memory (OOM)**

**Symptom**: Ollama crashes, CUDA out of memory errors

\# Check current VRAM usage  
nvidia-smi

\# Solution 1: Unload unused models  
docker exec ollama ollama list  
docker exec ollama ollama unload mistral:latest

\# Solution 2: Use smaller quantized models  
docker exec ollama ollama pull llama3.2:7b-q4\_K\_M  \# Q4 quantization  
docker exec ollama ollama rm llama3.2:latest      \# Remove unquantized

\# Solution 3: Limit concurrent model loading  
\# Edit deployment/configs/ollama.env:  
OLLAMA\_MAX\_LOADED\_MODELS=2  
docker restart ollama

\# Solution 4: Reduce model context size  
\# When calling Ollama API:  
curl http://localhost:11434/api/generate \-d '{  
  "model": "llama3.2",  
  "prompt": "Hello",  
  "options": {  
    "num\_ctx": 2048  \# Reduce from 4096  
  }  
}'

#### **3\. LiteLLM Routing Failures**

**Symptom**: Queries fail, "No available models" error

\# Check LiteLLM logs  
docker logs litellm

\# Verify routing configuration  
docker exec litellm cat /app/config.yml

\# Test specific provider  
docker exec \-it litellm bash  
curl \-X POST http://localhost:8000/chat/completions \\  
  \-H "Content-Type: application/json" \\  
  \-d '{  
    "model": "ollama/llama3.2",  
    "messages": \[{"role": "user", "content": "test"}\]  
  }'

\# Common fixes:

\# A. External API key invalid  
\# Re-enter API key in Script 1 or edit .secrets/api\_keys.enc

\# B. Ollama unreachable from LiteLLM  
docker exec litellm ping ollama  \# Should succeed  
\# Fix: Ensure both on same Docker network

\# C. Model not registered in LiteLLM  
\# Edit deployment/configs/litellm\_config.yml, add model, restart

\# D. Rate limit hit on external API  
\# Check cost tracking dashboard, wait for rate limit reset  
\# Or enable fallback to local models

#### **4\. Google Drive Sync Not Working**

**Symptom**: No files downloading, "Invalid credentials" error

\# Check token file exists  
ls \-lh $ROOT\_PATH (../scripts)/deployment/.secrets/gdrive\_token.json

\# Verify token is valid  
docker exec anything-llm cat /app/storage/.secrets/gdrive\_token.json

\# Re-authenticate (if token expired)  
\# Re-run Script 3, Phase 9 (Google Drive Configuration)

\# Manual sync test  
docker exec anything-llm python3 /app/scripts/gdrive\_sync.py \--test

\# Check sync logs  
docker logs anything-llm | grep gdrive

\# Common fixes:

\# A. Token expired (tokens expire after 7 days of inactivity)  
\# Solution: Re-run OAuth flow in Script 3

\# B. Drive API not enabled  
\# Solution: Visit https://console.cloud.google.com/apis/library/drive.googleapis.com  
\# Click "Enable API"

\# C. Insufficient permissions  
\# Solution: Ensure OAuth consent screen includes drive.readonly scope

\# D. Firewall blocking outbound HTTPS  
\# Solution: Allow outbound HTTPS (port 443\) to \*.googleapis.com

#### **5\. SSL Certificate Issues**

**Symptom**: "Certificate not trusted" warnings, HTTPS errors

\# Check Caddy status  
docker exec caddy caddy version  
docker logs caddy | grep "certificate"

\# Verify DNS propagation  
dig ai.jglaine.com \+short  
\# Should return your server's public IP

\# Force certificate renewal  
docker exec caddy caddy reload \--config /etc/caddy/Caddyfile

\# Check Let's Encrypt rate limits  
\# Visit: https://letsencrypt.org/docs/rate-limits/  
\# Max 5 certificates per week per domain

\# Common fixes:

\# A. Domain not resolving  
\# Solution: Update DNS A record, wait for propagation (up to 48h)

\# B. Port 80/443 blocked  
sudo netstat \-tulpn | grep :80  
sudo netstat \-tulpn | grep :443  
\# Solution: Open ports in firewall

\# C. Rate limit hit  
\# Solution: Wait 7 days, or use staging environment:  
\# Edit deployment/stack/docker-compose.yml:  
\# caddy:  
\#   command: caddy run \--config /etc/caddy/Caddyfile \--adapter caddyfile \--debug

\# D. Certificate files corrupted  
\# Solution: Remove and regenerate  
docker exec caddy rm \-rf /data/caddy/certificates  
docker restart caddy

#### **6\. High CPU/Memory Usage**

**Symptom**: System sluggish, services slow to respond

\# Identify resource-hungry containers  
docker stats \--no-stream

\# Check system resources  
htop  
free \-h  
df \-h

\# Common culprits and fixes:

\# A. Too many Ollama models loaded  
docker exec ollama ollama list  
docker exec ollama ollama unload \<unused-model\>

\# B. n8n workflows running too frequently  
\# Access n8n UI, edit workflow cron schedules

\# C. Prometheus storing too much data  
\# Reduce retention period:  
\# Edit deployment/stack/docker-compose.yml:  
\# prometheus:  
\#   command:  
\#     \- '--storage.tsdb.retention.time=7d'  \# Change from 15d

\# D. Vector database indexing  
\# Check Qdrant logs  
docker logs qdrant  
\# If indexing, wait for completion (can take 10-30 minutes for large datasets)

\# E. Log files filling disk  
du \-sh /mnt/data/ai-platform/logs/\*  
\# Rotate logs:  
find /mnt/data/ai-platform/logs/ \-name "\*.log" \-mtime \+7 \-delete

\# F. Docker image cache  
docker system df  
docker system prune \-a \--volumes  \# WARNING: Removes unused images/volumes

#### **7\. Anything LLM Document Ingestion Failing**

**Symptom**: Documents not searchable, embedding errors

\# Check Anything LLM logs  
docker logs anything-llm | grep \-i error

\# Verify Qdrant connection  
docker exec anything-llm curl http://qdrant:6333/health  
\# Should return: {"status":"ok"}

\# Check collection exists  
docker exec qdrant curl http://localhost:6333/collections  
\# Should list: anything\_llm\_collection

\# Manual ingestion test  
docker exec \-it anything-llm bash  
curl http://localhost:3001/api/v1/admin/system/sync-documents \\  
  \-H "Authorization: Bearer \<admin-token\>"

\# Common fixes:

\# A. Qdrant collection not created  
docker exec qdrant curl \-X PUT http://localhost:6333/collections/anything\_llm\_collection \\  
  \-H "Content-Type: application/json" \\  
  \-d '{  
    "vectors": {  
      "size": 768,  
      "distance": "Cosine"  
    }  
  }'

\# B. Embedding model not loaded  
docker exec ollama ollama pull nomic-embed-text  
docker restart anything-llm

\# C. File format not supported  
\# Anything LLM supports: PDF, DOCX, TXT, MD, HTML, CSV  
\# Convert unsupported files or add custom parser

\# D. Document too large  
\# Split large documents:  
split \-b 10M large\_doc.pdf large\_doc\_part\_  
\# Ingest parts separately

\# E. Memory insufficient for embedding  
\# Reduce batch size:  
\# Edit deployment/configs/anything\_llm.env:  
EMBEDDING\_BATCH\_SIZE=5  \# Reduce from 10

#### **8\. OpenClaw Not Accessible**

**Symptom**: Cannot reach [http://100.x.x.x:18789](http://100.x.x.x:18789)

\# Verify Tailscale is running  
sudo tailscale status

\# Check Tailscale IP assignment  
ip addr show tailscale0  
\# Should show: inet 100.x.x.x/32

\# Verify OpenClaw container running  
docker ps | grep openclaw

\# Test from Tailscale device  
curl http://\<tailscale-ip\>:18789/health  
\# Should return: {"status":"ok"}

\# Common fixes:

\# A. Tailscale not authenticated  
sudo tailscale up \--authkey=\<your-auth-key\>

\# B. OpenClaw container not exposed on Tailscale interface  
\# Edit deployment/stack/docker-compose.yml:  
\# openclaw:  
\#   ports:  
\#     \- "100.x.x.x:18789:18789"  \# Replace with your Tailscale IP  
docker-compose up \-d openclaw

\# C. Firewall blocking Tailscale  
sudo ufw allow in on tailscale0  
sudo ufw reload

\# D. OpenClaw crashed  
docker logs openclaw  
docker restart openclaw

#### **9\. Backup Script Failing**

**Symptom**: Cron job not running, backups not created

\# Check cron job exists  
crontab \-l | grep backup

\# Check backup script permissions  
ls \-lh $ROOT\_PATH (../scripts)/scripts/backup.sh  
chmod \+x $ROOT\_PATH (../scripts)/scripts/backup.sh

\# Run manually to see errors  
bash $ROOT\_PATH (../scripts)/scripts/backup.sh

\# Check backup logs  
cat /mnt/data/ai-platform/logs/backup-\*.log

\# Common fixes:

\# A. PostgreSQL dump failing  
docker exec postgres pg\_dumpall \-U postgres  
\# If error, check PostgreSQL logs:  
docker logs postgres

\# B. Disk space insufficient  
df \-h /mnt/data  
\# Free up space or increase disk size

\# C. GPG key not configured (for encrypted backups)  
gpg \--list-keys  
\# If empty, generate key:  
gpg \--gen-key

\# D. Permissions issue  
sudo chown \-R $USER:$USER /mnt/data/ai-platform/backups/  
sudo chmod 700 /mnt/data/ai-platform/backups/

#### **10\. Performance Degradation Over Time**

**Symptom**: Queries getting slower, system less responsive

\# Check Grafana performance dashboard  
\# Visit: https://ai.jglaine.com/grafana

\# Identify bottlenecks:

\# A. Database bloat  
docker exec postgres psql \-U postgres \-c "SELECT pg\_size\_pretty(pg\_database\_size('ai\_platform\_db'));"  
\# If \>5GB, consider vacuuming:  
docker exec postgres psql \-U postgres \-d ai\_platform\_db \-c "VACUUM FULL;"

\# B. Vector database needs optimization  
docker exec qdrant curl \-X POST http://localhost:6333/collections/anything\_llm\_collection/optimize

\# C. Redis memory full  
docker exec redis redis-cli INFO memory  
\# If used\_memory \> 80%, clear old caches:  
docker exec redis redis-cli FLUSHDB

\# D. Too many Docker containers  
docker ps \-a | wc \-l  
\# Remove stopped containers:  
docker container prune

\# E. Log files growing  
du \-sh /mnt/data/ai-platform/logs/  
\# Implement log rotation (see backup script)

\# F. Model context caching  
\# Ollama caches prompts for performance  
\# If cache corrupted, clear:  
docker exec ollama rm \-rf /root/.ollama/cache/  
docker restart ollama

---

## **ðŸ”§ MAINTENANCE & OPERATIONS**

### **Daily Operations**

#### **1\. System Health Check**

\# Quick health check script  
cat \> $ROOT\_PATH (../scripts)/scripts/health-check.sh \<\< 'EOF'  
\#\!/bin/bash  
echo "=== AI Platform Health Check \==="  
echo "Date: $(date)"  
echo ""

\# Service status  
echo "\[1/5\] Service Status"  
docker ps \--format "table {{.Names}}\\t{{.Status}}" | grep \-E "ollama|litellm|caddy|qdrant"

\# Resource usage  
echo ""  
echo "\[2/5\] Resource Usage"  
echo "CPU: $(top \-bn1 | grep "Cpu(s)" | awk '{print $2}' | cut \-d'%' \-f1)%"  
echo "RAM: $(free \-h | grep Mem | awk '{print $3 "/" $2}')"  
echo "GPU VRAM: $(nvidia-smi \--query-gpu=memory.used,memory.total \--format=csv,noheader,nounits | awk '{print $1 " GB / " $3 " GB"}')"

\# Disk space  
echo ""  
echo "\[3/5\] Disk Space"  
df \-h /mnt/data | tail \-1 | awk '{print $3 " used / " $2 " total (" $5 " utilization)"}'

\# Recent errors  
echo ""  
echo "\[4/5\] Recent Errors (last hour)"  
find /mnt/data/ai-platform/logs/ \-name "\*.log" \-mmin \-60 \-exec grep \-i "error" {} \\; | wc \-l

\# Backup status  
echo ""  
echo "\[5/5\] Backup Status"  
ls \-lh /mnt/data/ai-platform/backups/ | tail \-3

echo ""  
echo "=== Health Check Complete \==="  
EOF

chmod \+x $ROOT\_PATH (../scripts)/scripts/health-check.sh

\# Run daily check  
bash $ROOT\_PATH (../scripts)/scripts/health-check.sh

#### **2\. Monitor API Costs**

\# Daily cost check  
curl \-s https://ai.jglaine.com/grafana/api/dashboards/uid/cost-tracking \\  
  \-H "Authorization: Bearer \<grafana-api-key\>" \\  
  | jq '.dashboard.panels\[\] | select(.title \== "Current Month Spending") | .targets\[0\].expr'

\# Or visit Grafana directly:  
\# https://ai.jglaine.com/grafana/d/cost-tracking

#### **3\. Review Query Logs**

\# Analyze query patterns  
docker logs litellm \--since 24h | grep "POST /chat/completions" | wc \-l

\# Most used models  
docker logs litellm \--since 24h | grep "model:" | sort | uniq \-c | sort \-rn | head \-10

\# Average response time  
docker logs ollama \--since 24h | grep "response\_time" | awk '{sum+=$NF; count++} END {print sum/count "ms"}'

### **Weekly Maintenance**

#### **1\. Update Docker Images**

\# Check for updates  
docker images | grep "jan/\\|bitnami/\\|grafana/\\|prom/"

\# Update script  
cat \> $ROOT\_PATH (../scripts)/scripts/update-images.sh \<\< 'EOF'  
\#\!/bin/bash  
set \-e

echo "=== Updating Docker Images \==="

\# Pull latest images  
docker-compose \-f $ROOT\_PATH (../scripts)/deployment/stack/docker-compose.yml pull

\# Recreate containers with new images  
docker-compose \-f $ROOT\_PATH (../scripts)/deployment/stack/docker-compose.yml up \-d

\# Remove old images  
docker image prune \-a \-f

echo "=== Update Complete \==="  
docker ps \--format "table {{.Names}}\\t{{.Image}}\\t{{.Status}}"  
EOF

chmod \+x $ROOT\_PATH (../scripts)/scripts/update-images.sh

\# Run weekly (Sundays at 3 AM)  
crontab \-l | grep \-v "update-images" | crontab \-  
(crontab \-l 2\>/dev/null; echo "0 3 \* \* 0 $ROOT\_PATH (../scripts)/scripts/update-images.sh") | crontab \-

#### **2\. Optimize Databases**

\# PostgreSQL optimization  
docker exec postgres psql \-U postgres \<\< 'EOF'  
VACUUM ANALYZE;  
REINDEX DATABASE ai\_platform\_db;  
REINDEX DATABASE dify\_db;  
REINDEX DATABASE anything\_llm\_db;  
REINDEX DATABASE n8n\_db;  
EOF

\# Qdrant optimization  
docker exec qdrant curl \-X POST http://localhost:6333/collections/anything\_llm\_collection/optimize

\# Redis cleanup  
docker exec redis redis-cli \<\< 'EOF'  
MEMORY PURGE  
BGSAVE  
EOF

#### **3\. Review Security Logs**

\# Check failed authentication attempts  
docker logs caddy \--since 168h | grep "401\\|403" | wc \-l

\# Review Tailscale connection logs  
sudo tailscale status \--json | jq '.Peer\[\] | {name: .HostName, lastSeen: .LastSeen}'

\# Check for suspicious API usage patterns  
docker logs litellm \--since 168h | grep \-E "429|rate\_limit" | wc \-l

#### **4\. Test Backups**

\# Test restore procedure (monthly recommended)  
cat \> $ROOT\_PATH (../scripts)/scripts/test-restore.sh \<\< 'EOF'  
\#\!/bin/bash  
set \-e

echo "=== Testing Backup Restore \==="

\# Find latest backup  
LATEST\_BACKUP=$(ls \-t /mnt/data/ai-platform/backups/postgres-\*.sql.gz | head \-1)  
echo "Testing restore of: $LATEST\_BACKUP"

\# Create test database  
docker exec postgres psql \-U postgres \-c "DROP DATABASE IF EXISTS test\_restore\_db;"  
docker exec postgres psql \-U postgres \-c "CREATE DATABASE test\_restore\_db;"

\# Restore to test database  
gunzip \-c $LATEST\_BACKUP | docker exec \-i postgres psql \-U postgres \-d test\_restore\_db

\# Verify restoration  
TABLES=$(docker exec postgres psql \-U postgres \-d test\_restore\_db \-t \-c "SELECT COUNT(\*) FROM information\_schema.tables WHERE table\_schema='public';")

if \[ "$TABLES" \-gt 0 \]; then  
  echo "âœ“ Restore successful: $TABLES tables found"  
else  
  echo "âœ— Restore failed: No tables found"  
  exit 1  
fi

\# Cleanup  
docker exec postgres psql \-U postgres \-c "DROP DATABASE test\_restore\_db;"

echo "=== Test Complete \==="  
EOF

chmod \+x $ROOT\_PATH (../scripts)/scripts/test-restore.sh

### **Monthly Maintenance**

#### **1\. Security Updates**

\# System updates  
sudo apt update && sudo apt upgrade \-y

\# Restart services if kernel updated  
sudo needrestart \-r a

\# Update Docker Engine  
sudo apt install docker-ce docker-ce-cli containerd.io

\# Update Docker Compose  
sudo curl \-L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname \-s)-$(uname \-m)" \-o /usr/local/bin/docker-compose  
sudo chmod \+x /usr/local/bin/docker-compose

#### **2\. Certificate Renewal**

\# Let's Encrypt certificates auto-renew via Caddy  
\# Verify renewal working:  
docker logs caddy | grep "certificate renewed"

\# Manual renewal if needed:  
docker exec caddy caddy reload \--config /etc/caddy/Caddyfile

#### **3\. Audit API Keys**

\# List all API keys (redacted)  
cat $ROOT\_PATH (../scripts)/deployment/.secrets/.env | grep API\_KEY | sed 's/=.\*/=\*\*\*REDACTED\*\*\*/'

\# Rotate keys quarterly:  
\# 1\. Generate new keys in respective provider dashboards  
\# 2\. Update .secrets/.env  
\# 3\. Restart affected services  
\# 4\. Test connectivity  
\# 5\. Revoke old keys

#### **4\. Capacity Planning**

\# Generate capacity report  
cat \> $ROOT\_PATH (../scripts)/scripts/capacity-report.sh \<\< 'EOF'  
\#\!/bin/bash  
echo "=== Capacity Planning Report \==="  
echo "Generated: $(date)"  
echo ""

\# Disk growth rate (GB per month)  
echo "\[1\] Disk Usage Trend"  
CURRENT\_SIZE=$(du \-sb /mnt/data/ai-platform | awk '{print $1}')  
BACKUP\_SIZE=$(du \-sb /mnt/data/ai-platform/backups | awk '{print $1}')  
echo "Current usage: $(numfmt \--to=iec-i \--suffix=B $CURRENT\_SIZE)"  
echo "Backup size: $(numfmt \--to=iec-i \--suffix=B $BACKUP\_SIZE)"  
echo "Growth rate: \~$(echo "scale=2; $CURRENT\_SIZE / 30 / 1024 / 1024 / 1024" | bc) GB/day"

\# Request volume trend  
echo ""  
echo "\[2\] Request Volume"  
docker logs litellm \--since 720h 2\>/dev/null | grep "POST /chat/completions" | wc \-l | awk '{print "Last 30 days: " $1 " requests"}'

\# VRAM usage trend  
echo ""  
echo "\[3\] Average VRAM Usage"  
echo "Current: $(nvidia-smi \--query-gpu=memory.used \--format=csv,noheader,nounits) GB"  
echo "Peak: Check Grafana for historical data"

\# Database size trend  
echo ""  
echo "\[4\] Database Growth"  
docker exec postgres psql \-U postgres \-t \-c "SELECT pg\_database.datname, pg\_size\_pretty(pg\_database\_size(pg\_database.datname)) FROM pg\_database ORDER BY pg\_database\_size(pg\_database.datname) DESC;"

echo ""  
echo "=== Report Complete \==="  
EOF

chmod \+x $ROOT\_PATH (../scripts)/scripts/capacity-report.sh

### **Quarterly Maintenance**

#### **1\. Disaster Recovery Drill**

\# Simulate complete system failure and restore  
\# Document each step and time taken  
\# Update disaster recovery plan based on findings

#### **2\. Performance Baseline Update**

\# Run comprehensive benchmarks  
\# Compare against previous quarter  
\# Identify performance regressions  
\# Update capacity plan

#### **3\. Security Audit**

\# Review access logs  
\# Update passwords and API keys  
\# Review firewall rules  
\# Scan for vulnerabilities  
\# Update security documentation

---

## **ðŸ”’ SECURITY CONSIDERATIONS**

### **Network Security**

#### **1\. Firewall Configuration**

\# UFW (Uncomplicated Firewall) setup  
sudo ufw default deny incoming  
sudo ufw default allow outgoing

\# Allow SSH (change 22 to your custom port)  
sudo ufw allow 22/tcp

\# Allow HTTP/HTTPS (for Caddy)  
sudo ufw allow 80/tcp  
sudo ufw allow 443/tcp

\# Allow Tailscale  
sudo ufw allow in on tailscale0  
sudo ufw allow 41641/udp  \# Tailscale control plane

\# Deny direct access to internal services  
\# (Only accessible via Caddy reverse proxy or Tailscale)  
sudo ufw deny 11434/tcp  \# Ollama  
sudo ufw deny 8000/tcp   \# LiteLLM  
sudo ufw deny 6333/tcp   \# Qdrant  
sudo ufw deny 5432/tcp   \# PostgreSQL  
sudo ufw deny 6379/tcp   \# Redis

\# Enable firewall  
sudo ufw enable  
sudo ufw status verbose

#### **2\. TLS/SSL Best Practices**

\# Caddy automatically:  
\# \- Obtains certificates from Let's Encrypt  
\# \- Enables HTTPS by default  
\# \- Enforces TLS 1.2+  
\# \- Uses secure cipher suites

\# Verify SSL configuration  
curl \-I https://ai.jglaine.com | grep \-i "strict-transport-security"  
\# Should show: Strict-Transport-Security: max-age=31536000

\# Test SSL rating  
\# Visit: https://www.ssllabs.com/ssltest/analyze.html?d=ai.jglaine.com  
\# Target: A or A+ rating

#### **3\. Tailscale VPN Hardening**

\# Enable key expiry (force re-authentication every 90 days)  
sudo tailscale up \--ssh \--force-reauth \--auth-key-expiry=90d

\# Enable MagicDNS for easy service discovery  
sudo tailscale up \--accept-dns=true

\# Disable Tailscale SSH if not needed  
sudo tailscale up \--ssh=false

\# Restrict access to specific users/devices  
\# In Tailscale admin console:  
\# \- Enable ACLs (Access Control Lists)  
\# \- Define rules like:  
{  
  "acls": \[  
    {  
      "action": "accept",  
      "src": \["user@example.com"\],  
      "dst": \["tag:ai-platform:\*"\]  
    }  
  \]  
}

### **Authentication & Authorization**

#### **1\. API Key Management**

\# Store API keys encrypted  
cat \> $ROOT\_PATH (../scripts)/scripts/manage-secrets.sh \<\< 'EOF'  
\#\!/bin/bash

encrypt\_secret() {  
  echo "$1" | gpg \--encrypt \--recipient admin@jglaine.com \--armor  
}

decrypt\_secret() {  
  echo "$1" | gpg \--decrypt \--armor  
}

rotate\_key() {  
  KEY\_NAME=$1  
  NEW\_VALUE=$2  
    
  \# Backup old value  
  OLD\_VALUE=$(grep "$KEY\_NAME=" .secrets/.env | cut \-d'=' \-f2)  
  echo "$KEY\_NAME=$OLD\_VALUE" \>\> .secrets/.env.backup.$(date \+%Y%m%d)  
    
  \# Update with new value  
  sed \-i "s|$KEY\_NAME=.\*|$KEY\_NAME=$NEW\_VALUE|" .secrets/.env  
    
  echo "âœ“ Rotated $KEY\_NAME"  
}

case "$1" in  
  encrypt)  
    encrypt\_secret "$2"  
    ;;  
  decrypt)  
    decrypt\_secret "$2"  
    ;;  
  rotate)  
    rotate\_key "$2" "$3"  
    ;;  
  \*)  
    echo "Usage: $0 {encrypt|decrypt|rotate} \<args\>"  
    exit 1  
    ;;  
esac  
EOF

chmod \+x $ROOT\_PATH (../scripts)/scripts/manage-secrets.sh

#### **2\. Service Authentication**

\# Open WebUI: Password-based (change default)  
\# Dify: Password-based (change default)  
\# Anything LLM: Password-based with API tokens  
\# n8n: Password-based with API keys for webhooks  
\# Flowise: API key authentication  
\# Grafana: Admin password \+ viewer role

\# Enforce strong passwords policy:  
\# \- Minimum 16 characters  
\# \- Mix of uppercase, lowercase, numbers, symbols  
\# \- No dictionary words  
\# \- Use password manager (e.g., Bitwarden)

#### **3\. Multi-Factor Authentication (MFA)**

\# Enable MFA for critical services:

\# Grafana MFA (TOTP)  
\# 1\. Login to Grafana as admin  
\# 2\. User Settings â†’ Authentication â†’ Setup MFA  
\# 3\. Scan QR code with authenticator app

\# n8n MFA (optional, via reverse proxy)  
\# Add OAuth2 proxy in Caddy:  
ai.jglaine.com {  
  route /n8n/\* {  
    forward\_auth oauth2-proxy:4180 {  
      uri /oauth2/auth  
      copy\_headers X-Auth-Request-User X-Auth-Request-Email  
    }  
    reverse\_proxy n8n:5678  
  }  
}

### **Data Security**

#### **1\. Encryption at Rest**

\# Encrypt sensitive volumes  
\# Option A: LUKS full-disk encryption (recommended for /mnt/data)  
sudo cryptsetup luksFormat /dev/sdb  
sudo cryptsetup luksOpen /dev/sdb encrypted\_data  
sudo mkfs.ext4 /dev/mapper/encrypted\_data  
sudo mount /dev/mapper/encrypted\_data /mnt/data

\# Option B: Encrypted container for secrets  
\# Already implemented in deployment scripts (GPG)

#### **2\. Encryption in Transit**

\# All external traffic: HTTPS (via Caddy)  
\# Internal service-to-service: Docker network (isolated)  
\# Tailscale: WireGuard encryption (automatically enabled)

\# Verify internal traffic not exposed:  
sudo netstat \-tulpn | grep \-E "11434|8000|6333|5432|6379"  
\# Should only show 127.0.0.1 or Docker bridge IP

#### **3\. Backup Encryption**

\# Backups encrypted with GPG (already configured)  
\# Verify encryption:  
file /mnt/data/ai-platform/backups/secrets-\*.tar.gz.gpg  
\# Should show: GPG encrypted data

\# Test decryption:  
gpg \--decrypt /mnt/data/ai-platform/backups/secrets-20250207.tar.gz.gpg | tar \-tzf \- | head  
\# Should list files after entering passphrase

### **Compliance & Privacy**

#### **1\. GDPR Compliance**

\# Data retention policy  
\# \- User data: Configurable in each service  
\# \- Logs: 30 days (automated rotation)  
\# \- Backups: 7 days (automated deletion)  
\# \- Vector database: Manual deletion on request

\# Data deletion script  
cat \> $ROOT\_PATH (../scripts)/scripts/delete-user-data.sh \<\< 'EOF'  
\#\!/bin/bash  
USER\_EMAIL=$1

echo "=== Deleting data for $USER\_EMAIL \==="

\# Delete from PostgreSQL  
docker exec postgres psql \-U postgres \-d ai\_platform\_db \<\< SQL  
DELETE FROM users WHERE email='$USER\_EMAIL';  
DELETE FROM conversations WHERE user\_email='$USER\_EMAIL';  
SQL

\# Delete from Qdrant (if user-specific collections exist)  
docker exec qdrant curl \-X DELETE http://localhost:6333/collections/user\_${USER\_EMAIL//\[@.\]/\_}

\# Delete from logs  
find /mnt/data/ai-platform/logs/ \-type f \-exec sed \-i "/$USER\_EMAIL/d" {} \\;

echo "=== Deletion complete \==="  
echo "âš ï¸ Manual verification required in:"  
echo "  \- Dify UI"  
echo "  \- Anything LLM UI"  
echo "  \- n8n workflows"  
EOF

chmod \+x $ROOT\_PATH (../scripts)/scripts/delete-user-data.sh

#### **2\. Data Privacy**

\# Local-first approach: All sensitive data stays on your infrastructure  
\# External API calls: Only for complex queries (can be disabled)  
\# Logging: Scrub sensitive data

\# Configure log scrubbing in LiteLLM:  
\# Edit deployment/configs/litellm\_config.yml:  
litellm\_settings:  
  drop\_params: true  \# Don't log request content  
  success\_callback: null  \# Disable external logging  
  failure\_callback: null

#### **3\. Audit Logging**

\# Enable comprehensive audit logs  
\# Track: Who accessed what, when, from where

\# PostgreSQL audit logging  
docker exec postgres psql \-U postgres \<\< 'EOF'  
ALTER SYSTEM SET log\_statement \= 'all';  
ALTER SYSTEM SET log\_connections \= 'on';  
ALTER SYSTEM SET log\_disconnections \= 'on';  
SELECT pg\_reload\_conf();  
EOF

\# Service-specific audit logs  
\# \- Grafana: Already enabled (check Settings â†’ Users â†’ Activity Log)  
\# \- n8n: Execution data stored in PostgreSQL  
\# \- Dify: Activity logs in UI

### **Security Monitoring**

#### **1\. Intrusion Detection**

\# Install fail2ban for SSH brute force protection  
sudo apt install fail2ban \-y

\# Configure fail2ban  
sudo cat \> /etc/fail2ban/jail.local \<\< 'EOF'  
\[DEFAULT\]  
bantime \= 3600  
findtime \= 600  
maxretry \= 5

\[sshd\]  
enabled \= true  
port \= 22  
logpath \= /var/log/auth.log  
EOF

sudo systemctl restart fail2ban

#### **2\. Vulnerability Scanning**

\# Scan Docker images for vulnerabilities  
docker run \--rm \-v /var/run/docker.sock:/var/run/docker.sock \\  
  aquasec/trivy:latest image ollama/ollama:latest

\# Schedule weekly scans  
crontab \-l | grep \-v "trivy" | crontab \-  
(crontab \-l 2\>/dev/null; echo "0 3 \* \* 0 $ROOT\_PATH (../scripts)/scripts/security-scan.sh") | crontab \-

#### **3\. Security Alerts**

\# Already configured in Grafana (Script 3\)  
\# Additional alerts:

\# A. Unusual API access patterns  
\# \- Spike in failed authentication attempts  
\# \- Requests from unexpected IPs  
\# \- High volume of requests from single user

\# B. System security events  
\# \- Unauthorized SSH attempts  
\# \- File integrity changes in /etc or config directories  
\# \- New Docker containers started outside deployment scripts

\# C. Data exfiltration attempts  
\# \- Unusually large data transfers  
\# \- Bulk database queries  
\# \- Multiple failed access attempts to sensitive endpoints

---

## **ðŸ“ CHANGELOG**

### **Version 75.2.0 (2025-02-07)**

**Major Update: Complete Deployment Guide**

#### **Added**

* **Script 0**: Pre-flight checks and cleanup  
* **Script 1**: System setup and configuration  
* **Script 2**: Core service deployment  
* **Script 3**: Service configuration and integration testing  
* **Script 4**: Optional services (models, tools, monitoring)  
* **Comprehensive troubleshooting guide** (10 common issues)  
* **Maintenance operations guide** (daily, weekly, monthly, quarterly)  
* **Security considerations** (network, auth, data, compliance)  
* **Architecture diagrams** and **service inventory**

#### **Enhanced**

* **Google Drive integration** with automatic document ingestion  
* **LiteLLM routing** with fallback chains and load balancing  
* **Cost tracking** with budget alerts and Grafana dashboard  
* **Backup automation** with encryption and off-site options  
* **Monitoring stack** with Prometheus \+ Grafana \+ custom dashboards  
* **Development tools** (Jupyter Lab, VS Code, model playground)

#### **Fixed**

* **VRAM management** for multi-model loading  
* **SSL certificate** auto-renewal via Caddy  
* **Tailscale networking** for secure OpenClaw access  
* **Database optimization** procedures

#### **Security**

* **Encrypted API keys** (GPG)  
* **Encrypted backups** (GPG)  
* **Firewall rules** (UFW)  
* **TLS 1.2+ enforcement** (Caddy)  
* **Fail2ban** for SSH protection  
* **Vulnerability scanning** (Trivy)

#### **Documentation**

* **5 core docs** (111 KB total)  
* **System inventory** (JSON)  
* **4 usage examples**  
* **Complete API reference**  
* **Security best practices**

---

## **ðŸŽ‰ DEPLOYMENT GUIDE COMPLETE**

### **What You've Built**

A **production-ready, enterprise-grade AI platform** with:

âœ… **17+ services** running in Docker  
âœ… **6 Ollama models** for local inference  
âœ… **5 AI applications** (Open WebUI, Dify, Anything LLM, n8n, Flowise)  
âœ… **Smart routing** (local/cloud hybrid with fallbacks)  
âœ… **Google Drive integration** (automatic document sync)  
âœ… **Vector database** (Qdrant with 1,847+ embeddings)  
âœ… **Monitoring stack** (Prometheus \+ Grafana with alerts)  
âœ… **Secure networking** (Tailscale VPN \+ SSL via Caddy)  
âœ… **Automated backups** (daily, encrypted, 7-day retention)  
âœ… **Development tools** (Jupyter Lab, VS Code, model playground)  
âœ… **Cost tracking** ($100/month budget with alerts)  
âœ… **Comprehensive documentation** (120+ KB)

### **Total Setup Time**

* **Script 0** (Cleanup): \~2 minutes  
* **Script 1** (Setup): \~15 minutes  
* **Script 2** (Deployment): \~12 minutes (45 minutes with model downloads)  
* **Script 3** (Configuration): \~15 minutes  
* **Script 4** (Optional): \~30 minutes (varies by selections)

**Total: \~1-2 hours** (including user inputs and initial model downloads)
